
================================================================================
File: /Users/geoffpike/Code/lobe-chat/scripts/readmeWorkflow/const.ts
================================================================================

import { resolve } from 'node:path';

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';

export const root = resolve(__dirname, '../..');

export interface DataItem {
  author: string;
  createdAt: string;
  homepage: string;
  identifier: string;
  meta: { avatar: string; description: string; tags: string[]; title: string };
}

export const AGENT_URL = 'https://chat-agents.lobehub.com/index.json';
export const AGENT_I18N_URL = (lang: string) =>
  `https://chat-agents.lobehub.com/index.${lang}.json`;
export const PLUGIN_URL = 'https://chat-plugins.lobehub.com/index.json';
export const PLUGIN_I18N_URL = (lang: string) =>
  `https://chat-plugins.lobehub.com/index.${lang}.json`;

export const AGENT_SPLIT = '<!-- AGENT LIST -->';
export const PLUGIN_SPLIT = '<!-- PLUGIN LIST -->';
export const PROVIDER_SPLIT = '<!-- PROVIDER LIST -->';

export const PROVIDER_LIST = DEFAULT_MODEL_PROVIDER_LIST.filter(
  (item) => item.chatModels.length > 0 && item.id !== 'lobehub',
).map((item) => {
  return {
    id: item.id,
    name: item.name,
  };
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/scripts/cdnWorkflow/uploader.ts
================================================================================

import { consola } from 'consola';
import dotenv from 'dotenv';

import s3 from './s3';
import type { ImgInfo, S3UserConfig, UploadResult } from './s3/types';
import { formatPath } from './s3/utils';

dotenv.config();

if (!process.env.DOC_S3_ACCESS_KEY_ID) {
  consola.error('请配置 Doc S3 存储的环境变量: DOC_S3_ACCESS_KEY_ID');
  // eslint-disable-next-line unicorn/no-process-exit
  process.exit(1);
}

if (!process.env.DOC_S3_SECRET_ACCESS_KEY) {
  consola.error('请配置 Doc S3 存储的环境变量: DOC_S3_SECRET_ACCESS_KEY');
  // eslint-disable-next-line unicorn/no-process-exit
  process.exit(1);
}

if (!process.env.DOC_S3_PUBLIC_DOMAIN) {
  consola.error('请配置 Doc S3 存储的环境变量: DOC_S3_PUBLIC_DOMAIN');
  // eslint-disable-next-line unicorn/no-process-exit
  process.exit(1);
}

export const BASE_PATH = 'blog/assets';

export const uploader = async (
  file: File,
  filename: string,
  basePath: string = BASE_PATH,
  uploadPath?: string,
) => {
  const item: ImgInfo = {
    buffer: Buffer.from(await file.arrayBuffer()),
    extname: file.name.split('.').pop() as string,
    fileName: file.name,
    mimeType: file.type,
  };

  const userConfig: S3UserConfig = {
    accessKeyId: process.env.DOC_S3_ACCESS_KEY_ID || '',
    bucketName: 'hub-apac-1',
    endpoint: 'https://d35842305b91be4b48e06ff9a9ad83f5.r2.cloudflarestorage.com',
    pathPrefix: process.env.DOC_S3_PUBLIC_DOMAIN || '',
    pathStyleAccess: true,
    region: 'auto',
    secretAccessKey: process.env.DOC_S3_SECRET_ACCESS_KEY || '',
    uploadPath: uploadPath || `${basePath}${filename}.{extName}`,
  };

  const client = s3.createS3Client(userConfig);

  let results: UploadResult;

  try {
    results = await s3.createUploadTask({
      acl: 'public-read',
      bucketName: userConfig.bucketName,
      client,
      item: item,
      path: formatPath(item, userConfig.uploadPath),
      urlPrefix: userConfig.pathPrefix,
    });

    return results.url;
  } catch (error) {
    consola.error('上传到 S3 存储发生错误，请检查网络连接和配置是否正确');
    consola.error(error);
  }
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/file.ts
================================================================================

import { count, sum } from 'drizzle-orm';
import { and, asc, desc, eq, ilike, inArray, like, notExists, or } from 'drizzle-orm/expressions';
import type { PgTransaction } from 'drizzle-orm/pg-core';

import { LobeChatDatabase } from '@/database/type';
import { FilesTabs, QueryFileListParams, SortType } from '@/types/files';

import {
  FileItem,
  NewFile,
  NewGlobalFile,
  chunks,
  embeddings,
  fileChunks,
  files,
  globalFiles,
  knowledgeBaseFiles,
} from '../../schemas';

export class FileModel {
  private readonly userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  create = async (
    params: Omit<NewFile, 'id' | 'userId'> & { knowledgeBaseId?: string },
    insertToGlobalFiles?: boolean,
  ) => {
    const result = await this.db.transaction(async (trx) => {
      if (insertToGlobalFiles) {
        await trx.insert(globalFiles).values({
          fileType: params.fileType,
          hashId: params.fileHash!,
          metadata: params.metadata,
          size: params.size,
          url: params.url,
        });
      }

      const result = await trx
        .insert(files)
        .values({ ...params, userId: this.userId })
        .returning();

      const item = result[0];

      if (params.knowledgeBaseId) {
        await trx
          .insert(knowledgeBaseFiles)
          .values({ fileId: item.id, knowledgeBaseId: params.knowledgeBaseId });
      }

      return item;
    });

    return { id: result.id };
  };

  createGlobalFile = async (file: Omit<NewGlobalFile, 'id' | 'userId'>) => {
    return this.db.insert(globalFiles).values(file).returning();
  };

  checkHash = async (hash: string) => {
    const item = await this.db.query.globalFiles.findFirst({
      where: eq(globalFiles.hashId, hash),
    });
    if (!item) return { isExist: false };

    return {
      fileType: item.fileType,
      isExist: true,
      metadata: item.metadata,
      size: item.size,
      url: item.url,
    };
  };

  delete = async (id: string, removeGlobalFile: boolean = true) => {
    const file = await this.findById(id);
    if (!file) return;

    const fileHash = file.fileHash!;

    return await this.db.transaction(async (trx) => {
      // 1. 删除相关的 chunks
      await this.deleteFileChunks(trx as any, [id]);

      // 2. 删除文件记录
      await trx.delete(files).where(and(eq(files.id, id), eq(files.userId, this.userId)));

      const result = await trx
        .select({ count: count() })
        .from(files)
        .where(and(eq(files.fileHash, fileHash)));

      const fileCount = result[0].count;

      // delete the file from global file if it is not used by other files
      // if `DISABLE_REMOVE_GLOBAL_FILE` is true, we will not remove the global file
      if (fileCount === 0 && removeGlobalFile) {
        await trx.delete(globalFiles).where(eq(globalFiles.hashId, fileHash));

        return file;
      }
    });
  };

  deleteGlobalFile = async (hashId: string) => {
    return this.db.delete(globalFiles).where(eq(globalFiles.hashId, hashId));
  };

  countUsage = async () => {
    const result = await this.db
      .select({
        totalSize: sum(files.size),
      })
      .from(files)
      .where(eq(files.userId, this.userId));

    return parseInt(result[0].totalSize!) || 0;
  };

  deleteMany = async (ids: string[], removeGlobalFile: boolean = true) => {
    const fileList = await this.findByIds(ids);
    const hashList = fileList.map((file) => file.fileHash!);

    return await this.db.transaction(async (trx) => {
      // 1. 删除相关的 chunks
      await this.deleteFileChunks(trx as any, ids);

      // delete the files
      await trx.delete(files).where(and(inArray(files.id, ids), eq(files.userId, this.userId)));

      // count the files by hash
      const result = await trx
        .select({
          count: count(),
          hashId: files.fileHash,
        })
        .from(files)
        .where(inArray(files.fileHash, hashList))
        .groupBy(files.fileHash);

      // Create a Map to store the query result
      const countMap = new Map(result.map((item) => [item.hashId, item.count]));

      // Ensure that all incoming hashes have a result, even if it is 0
      const fileHashCounts = hashList.map((hashId) => ({
        count: countMap.get(hashId) || 0,
        hashId: hashId,
      }));

      const needToDeleteList = fileHashCounts.filter((item) => item.count === 0);

      if (needToDeleteList.length === 0 || !removeGlobalFile) return;

      // delete the file from global file if it is not used by other files
      await trx.delete(globalFiles).where(
        inArray(
          globalFiles.hashId,
          needToDeleteList.map((item) => item.hashId!),
        ),
      );

      return fileList.filter((file) =>
        needToDeleteList.some((item) => item.hashId === file.fileHash),
      );
    });
  };

  clear = async () => {
    return this.db.delete(files).where(eq(files.userId, this.userId));
  };

  query = async ({
    category,
    q,
    sortType,
    sorter,
    knowledgeBaseId,
    showFilesInKnowledgeBase,
  }: QueryFileListParams = {}) => {
    // 1. query where
    let whereClause = and(
      q ? ilike(files.name, `%${q}%`) : undefined,
      eq(files.userId, this.userId),
    );
    if (category && category !== FilesTabs.All) {
      const fileTypePrefix = this.getFileTypePrefix(category as FilesTabs);
      whereClause = and(whereClause, ilike(files.fileType, `${fileTypePrefix}%`));
    }

    // 2. order part

    let orderByClause = desc(files.createdAt);
    // create a map for sortable fields
    const sortableFields = {
      createdAt: files.createdAt,
      name: files.name,
      size: files.size,
      updatedAt: files.updatedAt,
    } as const;
    type SortableField = keyof typeof sortableFields;

    if (sorter && sortType && sorter in sortableFields) {
      const sortFunction = sortType.toLowerCase() === SortType.Asc ? asc : desc;
      orderByClause = sortFunction(sortableFields[sorter as SortableField]);
    }

    // 3. build query
    let query = this.db
      .select({
        chunkTaskId: files.chunkTaskId,
        createdAt: files.createdAt,
        embeddingTaskId: files.embeddingTaskId,
        fileType: files.fileType,
        id: files.id,
        name: files.name,
        size: files.size,
        updatedAt: files.updatedAt,
        url: files.url,
      })
      .from(files);

    // 4. add knowledge base query
    if (knowledgeBaseId) {
      // if knowledgeBaseId is provided, it means we are querying files in a knowledge-base

      // @ts-ignore
      query = query.innerJoin(
        knowledgeBaseFiles,
        and(
          eq(files.id, knowledgeBaseFiles.fileId),
          eq(knowledgeBaseFiles.knowledgeBaseId, knowledgeBaseId),
        ),
      );
    }
    // 5.if we don't show files in knowledge base, we need exclude files in knowledge base
    else if (!showFilesInKnowledgeBase) {
      whereClause = and(
        whereClause,
        notExists(
          this.db.select().from(knowledgeBaseFiles).where(eq(knowledgeBaseFiles.fileId, files.id)),
        ),
      );
    }

    // or we are just filter in the global files
    return query.where(whereClause).orderBy(orderByClause);
  };

  findByIds = async (ids: string[]) => {
    return this.db.query.files.findMany({
      where: and(inArray(files.id, ids), eq(files.userId, this.userId)),
    });
  };

  findById = async (id: string) => {
    return this.db.query.files.findFirst({
      where: and(eq(files.id, id), eq(files.userId, this.userId)),
    });
  };

  countFilesByHash = async (hash: string) => {
    const result = await this.db
      .select({
        count: count(),
      })
      .from(files)
      .where(and(eq(files.fileHash, hash)));

    return result[0].count;
  };

  update = async (id: string, value: Partial<FileItem>) =>
    this.db
      .update(files)
      .set({ ...value, updatedAt: new Date() })
      .where(and(eq(files.id, id), eq(files.userId, this.userId)));

  /**
   * get the corresponding file type prefix according to FilesTabs
   */
  private getFileTypePrefix = (category: FilesTabs): string => {
    switch (category) {
      case FilesTabs.Audios: {
        return 'audio';
      }
      case FilesTabs.Documents: {
        return 'application';
      }
      case FilesTabs.Images: {
        return 'image';
      }
      case FilesTabs.Videos: {
        return 'video';
      }
      default: {
        return '';
      }
    }
  };

  findByNames = async (fileNames: string[]) =>
    this.db.query.files.findMany({
      where: and(
        or(...fileNames.map((name) => like(files.name, `${name}%`))),
        eq(files.userId, this.userId),
      ),
    });

  // 抽象出通用的删除 chunks 方法
  private deleteFileChunks = async (trx: PgTransaction<any>, fileIds: string[]) => {
    const BATCH_SIZE = 1000; // 每批处理的数量

    // 1. 获取所有关联的 chunk IDs
    const relatedChunks = await trx
      .select({ chunkId: fileChunks.chunkId })
      .from(fileChunks)
      .where(inArray(fileChunks.fileId, fileIds));

    const chunkIds = relatedChunks.map((c) => c.chunkId).filter(Boolean) as string[];

    if (chunkIds.length === 0) return;

    // 2. 分批处理删除
    for (let i = 0; i < chunkIds.length; i += BATCH_SIZE) {
      const batchChunkIds = chunkIds.slice(i, i + BATCH_SIZE);

      await trx.delete(embeddings).where(inArray(embeddings.chunkId, batchChunkIds));

      await trx.delete(chunks).where(inArray(chunks.id, batchChunkIds));
    }

    return chunkIds;
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/asyncTask.ts
================================================================================

import { and, eq, inArray, lt } from 'drizzle-orm/expressions';

import { LobeChatDatabase } from '@/database/type';
import {
  AsyncTaskError,
  AsyncTaskErrorType,
  AsyncTaskStatus,
  AsyncTaskType,
} from '@/types/asyncTask';

import { AsyncTaskSelectItem, NewAsyncTaskItem, asyncTasks } from '../../schemas';

// set timeout to about 5 minutes, and give 2s padding time
export const ASYNC_TASK_TIMEOUT = 298 * 1000;

export class AsyncTaskModel {
  private userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  create = async (params: Pick<NewAsyncTaskItem, 'type' | 'status'>): Promise<string> => {
    const data = await this.db
      .insert(asyncTasks)
      .values({ ...params, userId: this.userId })
      .returning();

    return data[0].id;
  };

  delete = async (id: string) => {
    return this.db
      .delete(asyncTasks)
      .where(and(eq(asyncTasks.id, id), eq(asyncTasks.userId, this.userId)));
  };

  findById = async (id: string) => {
    return this.db.query.asyncTasks.findFirst({ where: and(eq(asyncTasks.id, id)) });
  };

  update(taskId: string, value: Partial<AsyncTaskSelectItem>) {
    return this.db
      .update(asyncTasks)
      .set({ ...value, updatedAt: new Date() })
      .where(and(eq(asyncTasks.id, taskId)));
  }

  findByIds = async (taskIds: string[], type: AsyncTaskType): Promise<AsyncTaskSelectItem[]> => {
    let chunkTasks: AsyncTaskSelectItem[] = [];

    if (taskIds.length > 0) {
      await this.checkTimeoutTasks(taskIds);
      chunkTasks = await this.db.query.asyncTasks.findMany({
        where: and(inArray(asyncTasks.id, taskIds), eq(asyncTasks.type, type)),
      });
    }

    return chunkTasks;
  };

  /**
   * make the task status to be `error` if the task is not finished in 20 seconds
   */
  checkTimeoutTasks = async (ids: string[]) => {
    const tasks = await this.db
      .select({ id: asyncTasks.id })
      .from(asyncTasks)
      .where(
        and(
          inArray(asyncTasks.id, ids),
          eq(asyncTasks.status, AsyncTaskStatus.Processing),
          lt(asyncTasks.createdAt, new Date(Date.now() - ASYNC_TASK_TIMEOUT)),
        ),
      );

    if (tasks.length > 0) {
      await this.db
        .update(asyncTasks)
        .set({
          error: new AsyncTaskError(
            AsyncTaskErrorType.Timeout,
            'chunking task is timeout, please try again',
          ),
          status: AsyncTaskStatus.Error,
        })
        .where(
          inArray(
            asyncTasks.id,
            tasks.map((item) => item.id),
          ),
        );
    }
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/chunk.ts
================================================================================

import { cosineDistance, count, sql } from 'drizzle-orm';
import { and, asc, desc, eq, inArray, isNull } from 'drizzle-orm/expressions';
import { chunk } from 'lodash-es';

import { LobeChatDatabase } from '@/database/type';
import { ChunkMetadata, FileChunk } from '@/types/chunk';

import {
  NewChunkItem,
  NewUnstructuredChunkItem,
  chunks,
  embeddings,
  fileChunks,
  files,
  unstructuredChunks,
} from '../../schemas';

export class ChunkModel {
  private userId: string;

  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  bulkCreate = async (params: NewChunkItem[], fileId: string) => {
    return this.db.transaction(async (trx) => {
      if (params.length === 0) return [];

      const result = await trx.insert(chunks).values(params).returning();

      const fileChunksData = result.map((chunk) => ({ chunkId: chunk.id, fileId }));

      if (fileChunksData.length > 0) {
        await trx.insert(fileChunks).values(fileChunksData);
      }

      return result;
    });
  };

  bulkCreateUnstructuredChunks = async (params: NewUnstructuredChunkItem[]) => {
    return this.db.insert(unstructuredChunks).values(params);
  };

  delete = async (id: string) => {
    return this.db.delete(chunks).where(and(eq(chunks.id, id), eq(chunks.userId, this.userId)));
  };

  deleteOrphanChunks = async () => {
    const orphanedChunks = await this.db
      .select({ chunkId: chunks.id })
      .from(chunks)
      .leftJoin(fileChunks, eq(chunks.id, fileChunks.chunkId))
      .where(isNull(fileChunks.fileId));

    const ids = orphanedChunks.map((chunk) => chunk.chunkId);
    if (ids.length === 0) return;

    const list = chunk(ids, 500);

    await this.db.transaction(async (trx) => {
      await Promise.all(
        list.map(async (chunkIds) => {
          await trx.delete(chunks).where(inArray(chunks.id, chunkIds));
        }),
      );
    });
  };

  findById = async (id: string) => {
    return this.db.query.chunks.findFirst({
      where: and(eq(chunks.id, id)),
    });
  };

  findByFileId = async (id: string, page = 0) => {
    const data = await this.db
      .select({
        abstract: chunks.abstract,
        createdAt: chunks.createdAt,
        id: chunks.id,
        index: chunks.index,
        metadata: chunks.metadata,
        text: chunks.text,
        type: chunks.type,
        updatedAt: chunks.updatedAt,
      })
      .from(chunks)
      .innerJoin(fileChunks, eq(chunks.id, fileChunks.chunkId))
      .where(and(eq(fileChunks.fileId, id), eq(chunks.userId, this.userId)))
      .limit(20)
      .offset(page * 20)
      .orderBy(asc(chunks.index));

    return data.map((item) => {
      const metadata = item.metadata as ChunkMetadata;

      return { ...item, metadata, pageNumber: metadata?.pageNumber } as FileChunk;
    });
  };

  getChunksTextByFileId = async (id: string): Promise<{ id: string; text: string }[]> => {
    const data = await this.db
      .select()
      .from(chunks)
      .innerJoin(fileChunks, eq(chunks.id, fileChunks.chunkId))
      .where(eq(fileChunks.fileId, id));

    return data
      .map((item) => item.chunks)
      .map((chunk) => ({ id: chunk.id, text: this.mapChunkText(chunk) }))
      .filter((chunk) => chunk.text) as { id: string; text: string }[];
  };

  countByFileIds = async (ids: string[]) => {
    if (ids.length === 0) return [];

    return this.db
      .select({
        count: count(fileChunks.chunkId),
        id: fileChunks.fileId,
      })
      .from(fileChunks)
      .where(inArray(fileChunks.fileId, ids))
      .groupBy(fileChunks.fileId);
  };

  countByFileId = async (ids: string) => {
    const data = await this.db
      .select({
        count: count(fileChunks.chunkId),
        id: fileChunks.fileId,
      })
      .from(fileChunks)
      .where(eq(fileChunks.fileId, ids))
      .groupBy(fileChunks.fileId);

    return data[0]?.count ?? 0;
  };

  semanticSearch = async ({
    embedding,
    fileIds,
  }: {
    embedding: number[];
    fileIds: string[] | undefined;
    query: string;
  }) => {
    const similarity = sql<number>`1 - (${cosineDistance(embeddings.embeddings, embedding)})`;

    const data = await this.db
      .select({
        fileId: fileChunks.fileId,
        fileName: files.name,
        id: chunks.id,
        index: chunks.index,
        metadata: chunks.metadata,
        similarity,
        text: chunks.text,
        type: chunks.type,
      })
      .from(chunks)
      .leftJoin(embeddings, eq(chunks.id, embeddings.chunkId))
      .leftJoin(fileChunks, eq(chunks.id, fileChunks.chunkId))
      .leftJoin(files, eq(fileChunks.fileId, files.id))
      .where(fileIds ? inArray(fileChunks.fileId, fileIds) : undefined)
      .orderBy((t) => desc(t.similarity))
      .limit(30);

    return data.map((item) => ({
      ...item,
      metadata: item.metadata as ChunkMetadata,
    }));
  };

  semanticSearchForChat = async ({
    embedding,
    fileIds,
  }: {
    embedding: number[];
    fileIds: string[] | undefined;
    query: string;
  }) => {
    const similarity = sql<number>`1 - (${cosineDistance(embeddings.embeddings, embedding)})`;

    const hasFiles = fileIds && fileIds.length > 0;

    if (!hasFiles) return [];

    const result = await this.db
      .select({
        fileId: files.id,
        fileName: files.name,
        id: chunks.id,
        index: chunks.index,
        metadata: chunks.metadata,
        similarity,
        text: chunks.text,
        type: chunks.type,
      })
      .from(chunks)
      .leftJoin(embeddings, eq(chunks.id, embeddings.chunkId))
      .leftJoin(fileChunks, eq(chunks.id, fileChunks.chunkId))
      .leftJoin(files, eq(files.id, fileChunks.fileId))
      .where(inArray(fileChunks.fileId, fileIds))
      .orderBy((t) => desc(t.similarity))
      .limit(5);

    return result.map((item) => {
      return {
        fileId: item.fileId,
        fileName: item.fileName,
        id: item.id,
        index: item.index,
        similarity: item.similarity,
        text: this.mapChunkText(item),
      };
    });
  };

  private mapChunkText = (chunk: { metadata: any; text: string | null; type: string | null }) => {
    let text = chunk.text;

    if (chunk.type === 'Table') {
      text = `${chunk.text}

content in Table html is below:
${(chunk.metadata as ChunkMetadata).text_as_html}
`;
    }

    return text;
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/message.ts
================================================================================

import { count } from 'drizzle-orm';
import { and, asc, desc, eq, gte, inArray, isNull, like, lt } from 'drizzle-orm/expressions';

import { LobeChatDatabase } from '@/database/type';
import { idGenerator } from '@/database/utils/idGenerator';
import {
  ChatFileItem,
  ChatImageItem,
  ChatTTS,
  ChatToolPayload,
  CreateMessageParams,
} from '@/types/message';
import { merge } from '@/utils/merge';

import {
  MessageItem,
  MessagePluginItem,
  NewMessageQuery,
  chunks,
  embeddings,
  fileChunks,
  files,
  messagePlugins,
  messageQueries,
  messageQueryChunks,
  messageTTS,
  messageTranslates,
  messages,
  messagesFiles,
} from '../../schemas';

export interface QueryMessageParams {
  current?: number;
  pageSize?: number;
  sessionId?: string | null;
  topicId?: string | null;
}

export class MessageModel {
  private userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  // **************** Query *************** //
  query = async (
    { current = 0, pageSize = 1000, sessionId, topicId }: QueryMessageParams = {},
    options: {
      postProcessUrl?: (path: string | null, file: { fileType: string }) => Promise<string>;
    } = {},
  ): Promise<MessageItem[]> => {
    const offset = current * pageSize;

    // 1. get basic messages
    const result = await this.db
      .select({
        /* eslint-disable sort-keys-fix/sort-keys-fix*/
        id: messages.id,
        role: messages.role,
        content: messages.content,
        error: messages.error,

        model: messages.model,
        provider: messages.provider,

        createdAt: messages.createdAt,
        updatedAt: messages.updatedAt,

        parentId: messages.parentId,
        threadId: messages.threadId,

        tools: messages.tools,
        tool_call_id: messagePlugins.toolCallId,

        plugin: {
          apiName: messagePlugins.apiName,
          arguments: messagePlugins.arguments,
          identifier: messagePlugins.identifier,
          type: messagePlugins.type,
        },
        pluginError: messagePlugins.error,
        pluginState: messagePlugins.state,

        translate: {
          content: messageTranslates.content,
          from: messageTranslates.from,
          to: messageTranslates.to,
        },

        ttsId: messageTTS.id,
        ttsContentMd5: messageTTS.contentMd5,
        ttsFile: messageTTS.fileId,
        ttsVoice: messageTTS.voice,
        /* eslint-enable */
      })
      .from(messages)
      .where(
        and(
          eq(messages.userId, this.userId),
          this.matchSession(sessionId),
          this.matchTopic(topicId),
        ),
      )
      .leftJoin(messagePlugins, eq(messagePlugins.id, messages.id))
      .leftJoin(messageTranslates, eq(messageTranslates.id, messages.id))
      .leftJoin(messageTTS, eq(messageTTS.id, messages.id))
      .orderBy(asc(messages.createdAt))
      .limit(pageSize)
      .offset(offset);

    const messageIds = result.map((message) => message.id as string);

    if (messageIds.length === 0) return [];

    // 2. get relative files
    const rawRelatedFileList = await this.db
      .select({
        fileType: files.fileType,
        id: messagesFiles.fileId,
        messageId: messagesFiles.messageId,
        name: files.name,
        size: files.size,
        url: files.url,
      })
      .from(messagesFiles)
      .leftJoin(files, eq(files.id, messagesFiles.fileId))
      .where(inArray(messagesFiles.messageId, messageIds));

    const relatedFileList = await Promise.all(
      rawRelatedFileList.map(async (file) => ({
        ...file,
        url: options.postProcessUrl
          ? await options.postProcessUrl(file.url, file as any)
          : (file.url as string),
      })),
    );

    const imageList = relatedFileList.filter((i) => (i.fileType || '').startsWith('image'));
    const fileList = relatedFileList.filter((i) => !(i.fileType || '').startsWith('image'));

    // 3. get relative file chunks
    const chunksList = await this.db
      .select({
        fileId: files.id,
        fileType: files.fileType,
        fileUrl: files.url,
        filename: files.name,
        id: chunks.id,
        messageId: messageQueryChunks.messageId,
        similarity: messageQueryChunks.similarity,
        text: chunks.text,
      })
      .from(messageQueryChunks)
      .leftJoin(chunks, eq(chunks.id, messageQueryChunks.chunkId))
      .leftJoin(fileChunks, eq(fileChunks.chunkId, chunks.id))
      .innerJoin(files, eq(fileChunks.fileId, files.id))
      .where(inArray(messageQueryChunks.messageId, messageIds));

    // 3. get relative message query
    const messageQueriesList = await this.db
      .select({
        id: messageQueries.id,
        messageId: messageQueries.messageId,
        rewriteQuery: messageQueries.rewriteQuery,
        userQuery: messageQueries.userQuery,
      })
      .from(messageQueries)
      .where(inArray(messageQueries.messageId, messageIds));

    return result.map(
      ({ model, provider, translate, ttsId, ttsFile, ttsContentMd5, ttsVoice, ...item }) => {
        const messageQuery = messageQueriesList.find((relation) => relation.messageId === item.id);
        return {
          ...item,
          chunksList: chunksList
            .filter((relation) => relation.messageId === item.id)
            .map((c) => ({
              ...c,
              similarity: Number(c.similarity) ?? undefined,
            })),

          extra: {
            fromModel: model,
            fromProvider: provider,
            translate,
            tts: ttsId
              ? {
                  contentMd5: ttsContentMd5,
                  file: ttsFile,
                  voice: ttsVoice,
                }
              : undefined,
          },
          fileList: fileList
            .filter((relation) => relation.messageId === item.id)
            // eslint-disable-next-line @typescript-eslint/no-unused-vars
            .map<ChatFileItem>(({ id, url, size, fileType, name }) => ({
              fileType: fileType!,
              id,
              name: name!,
              size: size!,
              url,
            })),

          imageList: imageList
            .filter((relation) => relation.messageId === item.id)
            // eslint-disable-next-line @typescript-eslint/no-unused-vars
            .map<ChatImageItem>(({ id, url, name }) => ({ alt: name!, id, url })),

          ragQuery: messageQuery?.rewriteQuery,
          ragQueryId: messageQuery?.id,
          ragRawQuery: messageQuery?.userQuery,
        };
      },
    );
  };

  findById = async (id: string) => {
    return this.db.query.messages.findFirst({
      where: and(eq(messages.id, id), eq(messages.userId, this.userId)),
    });
  };

  findMessageQueriesById = async (messageId: string) => {
    const result = await this.db
      .select({
        embeddings: embeddings.embeddings,
        id: messageQueries.id,
        query: messageQueries.rewriteQuery,
        rewriteQuery: messageQueries.rewriteQuery,
        userQuery: messageQueries.userQuery,
      })
      .from(messageQueries)
      .where(and(eq(messageQueries.messageId, messageId)))
      .leftJoin(embeddings, eq(embeddings.id, messageQueries.embeddingsId));

    if (result.length === 0) return undefined;

    return result[0];
  };

  queryAll = async (): Promise<MessageItem[]> => {
    return this.db
      .select()
      .from(messages)
      .orderBy(messages.createdAt)
      .where(eq(messages.userId, this.userId));
  };

  queryBySessionId = async (sessionId?: string | null): Promise<MessageItem[]> => {
    return this.db.query.messages.findMany({
      orderBy: [asc(messages.createdAt)],
      where: and(eq(messages.userId, this.userId), this.matchSession(sessionId)),
    });
  };

  queryByKeyword = async (keyword: string): Promise<MessageItem[]> => {
    if (!keyword) return [];
    return this.db.query.messages.findMany({
      orderBy: [desc(messages.createdAt)],
      where: and(eq(messages.userId, this.userId), like(messages.content, `%${keyword}%`)),
    });
  };

  count = async (): Promise<number> => {
    const result = await this.db
      .select({
        count: count(messages.id),
      })
      .from(messages)
      .where(eq(messages.userId, this.userId));

    return result[0].count;
  };

  countToday = async (): Promise<number> => {
    const today = new Date();
    today.setHours(0, 0, 0, 0);
    const tomorrow = new Date(today);
    tomorrow.setDate(tomorrow.getDate() + 1);

    const result = await this.db
      .select({
        count: count(messages.id),
      })
      .from(messages)
      .where(
        and(
          eq(messages.userId, this.userId),
          gte(messages.createdAt, today),
          lt(messages.createdAt, tomorrow),
        ),
      );

    return result[0].count;
  };

  hasMoreThanN = async (n: number): Promise<boolean> => {
    const result = await this.db
      .select({ id: messages.id })
      .from(messages)
      .where(eq(messages.userId, this.userId))
      .limit(n + 1);

    return result.length > n;
  };

  // **************** Create *************** //

  create = async (
    {
      fromModel,
      fromProvider,
      files,
      plugin,
      pluginState,
      fileChunks,
      ragQueryId,
      ...message
    }: CreateMessageParams,
    id: string = this.genId(),
  ): Promise<MessageItem> => {
    return this.db.transaction(async (trx) => {
      const [item] = (await trx
        .insert(messages)
        .values({
          ...message,
          id,
          model: fromModel,
          provider: fromProvider,
          userId: this.userId,
        })
        .returning()) as MessageItem[];

      // Insert the plugin data if the message is a tool
      if (message.role === 'tool') {
        await trx.insert(messagePlugins).values({
          apiName: plugin?.apiName,
          arguments: plugin?.arguments,
          id,
          identifier: plugin?.identifier,
          state: pluginState,
          toolCallId: message.tool_call_id,
          type: plugin?.type,
        });
      }

      if (files && files.length > 0) {
        await trx
          .insert(messagesFiles)
          .values(files.map((file) => ({ fileId: file, messageId: id })));
      }

      if (fileChunks && fileChunks.length > 0 && ragQueryId) {
        await trx.insert(messageQueryChunks).values(
          fileChunks.map((chunk) => ({
            chunkId: chunk.id,
            messageId: id,
            queryId: ragQueryId,
            similarity: chunk.similarity?.toString(),
          })),
        );
      }

      return item;
    });
  };

  batchCreate = async (newMessages: MessageItem[]) => {
    const messagesToInsert = newMessages.map((m) => {
      return { ...m, userId: this.userId };
    });

    return this.db.insert(messages).values(messagesToInsert);
  };

  createMessageQuery = async (params: NewMessageQuery) => {
    const result = await this.db.insert(messageQueries).values(params).returning();

    return result[0];
  };
  // **************** Update *************** //

  update = async (id: string, message: Partial<MessageItem>) => {
    return this.db
      .update(messages)
      .set(message)
      .where(and(eq(messages.id, id), eq(messages.userId, this.userId)));
  };

  updatePluginState = async (id: string, state: Record<string, any>) => {
    const item = await this.db.query.messagePlugins.findFirst({
      where: eq(messagePlugins.id, id),
    });
    if (!item) throw new Error('Plugin not found');

    return this.db
      .update(messagePlugins)
      .set({ state: merge(item.state || {}, state) })
      .where(eq(messagePlugins.id, id));
  };

  updateMessagePlugin = async (id: string, value: Partial<MessagePluginItem>) => {
    const item = await this.db.query.messagePlugins.findFirst({
      where: eq(messagePlugins.id, id),
    });
    if (!item) throw new Error('Plugin not found');

    return this.db.update(messagePlugins).set(value).where(eq(messagePlugins.id, id));
  };

  updateTranslate = async (id: string, translate: Partial<MessageItem>) => {
    const result = await this.db.query.messageTranslates.findFirst({
      where: and(eq(messageTranslates.id, id)),
    });

    // If the message does not exist in the translate table, insert it
    if (!result) {
      return this.db.insert(messageTranslates).values({ ...translate, id });
    }

    // or just update the existing one
    return this.db.update(messageTranslates).set(translate).where(eq(messageTranslates.id, id));
  };

  updateTTS = async (id: string, tts: Partial<ChatTTS>) => {
    const result = await this.db.query.messageTTS.findFirst({
      where: and(eq(messageTTS.id, id)),
    });

    // If the message does not exist in the translate table, insert it
    if (!result) {
      return this.db
        .insert(messageTTS)
        .values({ contentMd5: tts.contentMd5, fileId: tts.file, id, voice: tts.voice });
    }

    // or just update the existing one
    return this.db
      .update(messageTTS)
      .set({ contentMd5: tts.contentMd5, fileId: tts.file, voice: tts.voice })
      .where(eq(messageTTS.id, id));
  };

  // **************** Delete *************** //

  deleteMessage = async (id: string) => {
    return this.db.transaction(async (tx) => {
      // 1. 查询要删除的 message 的完整信息
      const message = await tx
        .select()
        .from(messages)
        .where(and(eq(messages.id, id), eq(messages.userId, this.userId)))
        .limit(1);

      // 如果找不到要删除的 message,直接返回
      if (message.length === 0) return;

      // 2. 检查 message 是否包含 tools
      const toolCallIds = message[0].tools?.map((tool: ChatToolPayload) => tool.id).filter(Boolean);

      let relatedMessageIds: string[] = [];

      if (toolCallIds?.length > 0) {
        // 3. 如果 message 包含 tools,查询出所有相关联的 message id
        const res = await tx
          .select({ id: messagePlugins.id })
          .from(messagePlugins)
          .where(inArray(messagePlugins.toolCallId, toolCallIds));

        relatedMessageIds = res.map((row) => row.id);
      }

      // 4. 合并要删除的 message id 列表
      const messageIdsToDelete = [id, ...relatedMessageIds];

      // 5. 删除所有相关的 message
      await tx.delete(messages).where(inArray(messages.id, messageIdsToDelete));
    });
  };

  deleteMessages = async (ids: string[]) =>
    this.db
      .delete(messages)
      .where(and(eq(messages.userId, this.userId), inArray(messages.id, ids)));

  deleteMessageTranslate = async (id: string) =>
    this.db.delete(messageTranslates).where(and(eq(messageTranslates.id, id)));

  deleteMessageTTS = async (id: string) =>
    this.db.delete(messageTTS).where(and(eq(messageTTS.id, id)));

  deleteMessageQuery = async (id: string) =>
    this.db.delete(messageQueries).where(and(eq(messageQueries.id, id)));

  deleteMessagesBySession = async (sessionId?: string | null, topicId?: string | null) =>
    this.db
      .delete(messages)
      .where(
        and(
          eq(messages.userId, this.userId),
          this.matchSession(sessionId),
          this.matchTopic(topicId),
        ),
      );

  deleteAllMessages = async () => {
    return this.db.delete(messages).where(eq(messages.userId, this.userId));
  };

  // **************** Helper *************** //

  private genId = () => idGenerator('messages', 14);

  private matchSession = (sessionId?: string | null) =>
    sessionId ? eq(messages.sessionId, sessionId) : isNull(messages.sessionId);

  private matchTopic = (topicId?: string | null) =>
    topicId ? eq(messages.topicId, topicId) : isNull(messages.topicId);
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/session.ts
================================================================================

import { Column, count, sql } from 'drizzle-orm';
import { and, asc, desc, eq, inArray, like, not, or } from 'drizzle-orm/expressions';

import { appEnv } from '@/config/app';
import { INBOX_SESSION_ID } from '@/const/session';
import { DEFAULT_AGENT_CONFIG } from '@/const/settings';
import { LobeChatDatabase } from '@/database/type';
import { idGenerator } from '@/database/utils/idGenerator';
import { parseAgentConfig } from '@/server/globalConfig/parseDefaultAgent';
import { ChatSessionList, LobeAgentSession } from '@/types/session';
import { merge } from '@/utils/merge';

import {
  AgentItem,
  NewAgent,
  NewSession,
  SessionItem,
  agents,
  agentsToSessions,
  sessionGroups,
  sessions,
} from '../../schemas';

export class SessionModel {
  private userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }
  // **************** Query *************** //

  query = async ({ current = 0, pageSize = 9999 } = {}) => {
    const offset = current * pageSize;

    return this.db.query.sessions.findMany({
      limit: pageSize,
      offset,
      orderBy: [desc(sessions.updatedAt)],
      where: and(eq(sessions.userId, this.userId), not(eq(sessions.slug, INBOX_SESSION_ID))),
      with: { agentsToSessions: { columns: {}, with: { agent: true } }, group: true },
    });
  };

  queryWithGroups = async (): Promise<ChatSessionList> => {
    // 查询所有会话
    const result = await this.query();

    const groups = await this.db.query.sessionGroups.findMany({
      orderBy: [asc(sessionGroups.sort), desc(sessionGroups.createdAt)],
      where: eq(sessions.userId, this.userId),
    });

    return {
      sessionGroups: groups as unknown as ChatSessionList['sessionGroups'],
      sessions: result.map((item) => this.mapSessionItem(item as any)),
    };
  };

  queryByKeyword = async (keyword: string) => {
    if (!keyword) return [];

    const keywordLowerCase = keyword.toLowerCase();

    const data = await this.findSessionsByKeywords({ keyword: keywordLowerCase });

    return data.map((item) => this.mapSessionItem(item as any));
  };

  findByIdOrSlug = async (
    idOrSlug: string,
  ): Promise<(SessionItem & { agent: AgentItem }) | undefined> => {
    const result = await this.db.query.sessions.findFirst({
      where: and(
        or(eq(sessions.id, idOrSlug), eq(sessions.slug, idOrSlug)),
        eq(sessions.userId, this.userId),
      ),
      with: { agentsToSessions: { columns: {}, with: { agent: true } }, group: true },
    });

    if (!result) return;

    return { ...result, agent: (result?.agentsToSessions?.[0] as any)?.agent } as any;
  };

  count = async (): Promise<number> => {
    const result = await this.db
      .select({
        count: count(sessions.id),
      })
      .from(sessions)
      .where(eq(sessions.userId, this.userId));

    return result[0].count;
  };

  hasMoreThanN = async (n: number): Promise<boolean> => {
    const result = await this.db
      .select({ id: sessions.id })
      .from(sessions)
      .where(eq(sessions.userId, this.userId))
      .limit(n + 1);

    return result.length > n;
  };

  // **************** Create *************** //

  create = async ({
    id = idGenerator('sessions'),
    type = 'agent',
    session = {},
    config = {},
    slug,
  }: {
    config?: Partial<NewAgent>;
    id?: string;
    session?: Partial<NewSession>;
    slug?: string;
    type: 'agent' | 'group';
  }): Promise<SessionItem> => {
    return this.db.transaction(async (trx) => {
      const newAgents = await trx
        .insert(agents)
        .values({
          ...config,
          createdAt: new Date(),
          id: idGenerator('agents'),
          updatedAt: new Date(),
          userId: this.userId,
        })
        .returning();

      const result = await trx
        .insert(sessions)
        .values({
          ...session,
          createdAt: new Date(),
          id,
          slug,
          type,
          updatedAt: new Date(),
          userId: this.userId,
        })
        .returning();

      await trx.insert(agentsToSessions).values({
        agentId: newAgents[0].id,
        sessionId: id,
      });

      return result[0];
    });
  };

  createInbox = async () => {
    const item = await this.db.query.sessions.findFirst({
      where: and(eq(sessions.userId, this.userId), eq(sessions.slug, INBOX_SESSION_ID)),
    });
    if (item) return;

    const serverAgentConfig = parseAgentConfig(appEnv.DEFAULT_AGENT_CONFIG) || {};

    return await this.create({
      config: merge(DEFAULT_AGENT_CONFIG, serverAgentConfig),
      slug: INBOX_SESSION_ID,
      type: 'agent',
    });
  };

  batchCreate = async (newSessions: NewSession[]) => {
    const sessionsToInsert = newSessions.map((s) => {
      return {
        ...s,
        id: this.genId(),
        userId: this.userId,
      };
    });

    return this.db.insert(sessions).values(sessionsToInsert);
  };

  duplicate = async (id: string, newTitle?: string) => {
    const result = await this.findByIdOrSlug(id);

    if (!result) return;

    // eslint-disable-next-line @typescript-eslint/no-unused-vars,unused-imports/no-unused-vars
    const { agent, clientId, ...session } = result;
    const sessionId = this.genId();

    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    const { id: _, slug: __, ...config } = agent;

    return this.create({
      config: config,
      id: sessionId,
      session: {
        ...session,
        title: newTitle || session.title,
      },
      type: 'agent',
    });
  };

  // **************** Delete *************** //

  /**
   * Delete a session, also delete all messages and topics associated with it.
   */
  delete = async (id: string) => {
    return this.db
      .delete(sessions)
      .where(and(eq(sessions.id, id), eq(sessions.userId, this.userId)));
  };

  /**
   * Batch delete sessions, also delete all messages and topics associated with them.
   */
  batchDelete = async (ids: string[]) => {
    return this.db
      .delete(sessions)
      .where(and(inArray(sessions.id, ids), eq(sessions.userId, this.userId)));
  };

  deleteAll = async () => {
    return this.db.delete(sessions).where(eq(sessions.userId, this.userId));
  };
  // **************** Update *************** //

  update = async (id: string, data: Partial<SessionItem>) => {
    return this.db
      .update(sessions)
      .set(data)
      .where(and(eq(sessions.id, id), eq(sessions.userId, this.userId)))
      .returning();
  };

  updateConfig = async (id: string, data: Partial<AgentItem>) => {
    if (Object.keys(data).length === 0) return;

    return this.db
      .update(agents)
      .set(data)
      .where(and(eq(agents.id, id), eq(agents.userId, this.userId)));
  };

  // **************** Helper *************** //

  private genId = () => idGenerator('sessions');

  private mapSessionItem = ({
    agentsToSessions,
    title,
    backgroundColor,
    description,
    avatar,
    groupId,
    ...res
  }: SessionItem & { agentsToSessions?: { agent: AgentItem }[] }): LobeAgentSession => {
    // TODO: 未来这里需要更好的实现方案，目前只取第一个
    const agent = agentsToSessions?.[0]?.agent;
    return {
      ...res,
      group: groupId,
      meta: {
        avatar: agent?.avatar ?? avatar ?? undefined,
        backgroundColor: agent?.backgroundColor ?? backgroundColor ?? undefined,
        description: agent?.description ?? description ?? undefined,
        tags: agent?.tags ?? undefined,
        title: agent?.title ?? title ?? undefined,
      },
      model: agent?.model,
    } as any;
  };

  findSessionsByKeywords = async (params: {
    current?: number;
    keyword: string;
    pageSize?: number;
  }) => {
    const { keyword, pageSize = 9999, current = 0 } = params;
    const offset = current * pageSize;
    const results = await this.db.query.agents.findMany({
      limit: pageSize,
      offset,
      orderBy: [desc(agents.updatedAt)],
      where: and(
        eq(agents.userId, this.userId),
        or(
          like(sql`lower(${agents.title})` as unknown as Column, `%${keyword.toLowerCase()}%`),
          like(
            sql`lower(${agents.description})` as unknown as Column,
            `%${keyword.toLowerCase()}%`,
          ),
        ),
      ),
      with: { agentsToSessions: { columns: {}, with: { session: true } } },
    });
    try {
      // @ts-expect-error
      return results.map((item) => item.agentsToSessions[0].session);
    } catch (e) {
      console.error('findSessionsByKeywords error:', e);
    }
    return [];
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/embedding.ts
================================================================================

import { count } from 'drizzle-orm';
import { and, eq } from 'drizzle-orm/expressions';

import { LobeChatDatabase } from '@/database/type';

import { NewEmbeddingsItem, embeddings } from '../../schemas';

export class EmbeddingModel {
  private userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  create = async (value: Omit<NewEmbeddingsItem, 'userId'>) => {
    const [item] = await this.db
      .insert(embeddings)
      .values({ ...value, userId: this.userId })
      .returning();

    return item.id as string;
  };

  bulkCreate = async (values: Omit<NewEmbeddingsItem, 'userId'>[]) => {
    return this.db
      .insert(embeddings)
      .values(values.map((item) => ({ ...item, userId: this.userId })))
      .onConflictDoNothing({
        target: [embeddings.chunkId],
      });
  };

  delete = async (id: string) => {
    return this.db
      .delete(embeddings)
      .where(and(eq(embeddings.id, id), eq(embeddings.userId, this.userId)));
  };

  query = async () => {
    return this.db.query.embeddings.findMany({
      where: eq(embeddings.userId, this.userId),
    });
  };

  findById = async (id: string) => {
    return this.db.query.embeddings.findFirst({
      where: and(eq(embeddings.id, id), eq(embeddings.userId, this.userId)),
    });
  };

  countUsage = async (): Promise<number> => {
    const result = await this.db
      .select({
        count: count(),
      })
      .from(embeddings)
      .where(eq(embeddings.userId, this.userId));

    return result[0].count;
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/user.ts
================================================================================

import { TRPCError } from '@trpc/server';
import { eq } from 'drizzle-orm/expressions';
import { DeepPartial } from 'utility-types';

import { LobeChatDatabase } from '@/database/type';
import { UserGuide, UserPreference } from '@/types/user';
import { UserKeyVaults, UserSettings } from '@/types/user/settings';
import { merge } from '@/utils/merge';

import { NewUser, UserItem, UserSettingsItem, userSettings, users } from '../../schemas';
import { SessionModel } from './session';

type DecryptUserKeyVaults = (
  encryptKeyVaultsStr: string | null,
  userId?: string,
) => Promise<UserKeyVaults>;

export class UserNotFoundError extends TRPCError {
  constructor() {
    super({ code: 'UNAUTHORIZED', message: 'user not found' });
  }
}

export class UserModel {
  private userId: string;
  private db: LobeChatDatabase;

  constructor(db: LobeChatDatabase, userId: string) {
    this.userId = userId;
    this.db = db;
  }

  getUserState = async (decryptor: DecryptUserKeyVaults) => {
    const result = await this.db
      .select({
        isOnboarded: users.isOnboarded,
        preference: users.preference,

        settingsDefaultAgent: userSettings.defaultAgent,
        settingsGeneral: userSettings.general,
        settingsKeyVaults: userSettings.keyVaults,
        settingsLanguageModel: userSettings.languageModel,
        settingsSystemAgent: userSettings.systemAgent,
        settingsTTS: userSettings.tts,
        settingsTool: userSettings.tool,
      })
      .from(users)
      .where(eq(users.id, this.userId))
      .leftJoin(userSettings, eq(users.id, userSettings.id));

    if (!result || !result[0]) {
      throw new UserNotFoundError();
    }

    const state = result[0];

    // Decrypt keyVaults
    const decryptKeyVaults = await decryptor(state.settingsKeyVaults, this.userId);

    const settings: DeepPartial<UserSettings> = {
      defaultAgent: state.settingsDefaultAgent || {},
      general: state.settingsGeneral || {},
      keyVaults: decryptKeyVaults,
      languageModel: state.settingsLanguageModel || {},
      systemAgent: state.settingsSystemAgent || {},
      tool: state.settingsTool || {},
      tts: state.settingsTTS || {},
    };

    return {
      isOnboarded: state.isOnboarded,
      preference: state.preference as UserPreference,
      settings,
      userId: this.userId,
    };
  };

  updateUser = async (value: Partial<UserItem>) => {
    return this.db
      .update(users)
      .set({ ...value, updatedAt: new Date() })
      .where(eq(users.id, this.userId));
  };

  deleteSetting = async () => {
    return this.db.delete(userSettings).where(eq(userSettings.id, this.userId));
  };

  updateSetting = async (value: Partial<UserSettingsItem>) => {
    return this.db
      .insert(userSettings)
      .values({
        id: this.userId,
        ...value,
      })
      .onConflictDoUpdate({
        set: value,
        target: userSettings.id,
      });
  };

  updatePreference = async (value: Partial<UserPreference>) => {
    const user = await this.db.query.users.findFirst({ where: eq(users.id, this.userId) });
    if (!user) return;

    return this.db
      .update(users)
      .set({ preference: merge(user.preference, value) })
      .where(eq(users.id, this.userId));
  };

  updateGuide = async (value: Partial<UserGuide>) => {
    const user = await this.db.query.users.findFirst({ where: eq(users.id, this.userId) });
    if (!user) return;

    const prevPreference = (user.preference || {}) as UserPreference;
    return this.db
      .update(users)
      .set({ preference: { ...prevPreference, guide: merge(prevPreference.guide || {}, value) } })
      .where(eq(users.id, this.userId));
  };

  // Static method

  static createUser = async (db: LobeChatDatabase, params: NewUser) => {
    // if user already exists, skip creation
    if (params.id) {
      const user = await db.query.users.findFirst({ where: eq(users.id, params.id) });
      if (!!user) return;
    }

    const [user] = await db
      .insert(users)
      .values({ ...params })
      .returning();

    // Create an inbox session for the user
    const model = new SessionModel(db, user.id);

    await model.createInbox();
  };

  static deleteUser = async (db: LobeChatDatabase, id: string) => {
    return db.delete(users).where(eq(users.id, id));
  };

  static findById = async (db: LobeChatDatabase, id: string) => {
    return db.query.users.findFirst({ where: eq(users.id, id) });
  };

  static findByEmail = async (db: LobeChatDatabase, email: string) => {
    return db.query.users.findFirst({ where: eq(users.email, email) });
  };

  static getUserApiKeys = async (
    db: LobeChatDatabase,
    id: string,
    decryptor: DecryptUserKeyVaults,
  ) => {
    const result = await db
      .select({
        settingsKeyVaults: userSettings.keyVaults,
      })
      .from(userSettings)
      .where(eq(userSettings.id, id));

    if (!result || !result[0]) {
      throw new UserNotFoundError();
    }

    const state = result[0];

    // Decrypt keyVaults
    return await decryptor(state.settingsKeyVaults, id);
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/__tests__/fixtures/embedding.ts
================================================================================

/* eslint-disable unicorn/numeric-separators-style */

// query: 设计思维的困境是什么 / text-embedding-3-small / 1024d
export const designThinkingQuery = [
  0.07259023189544678, -0.034810926765203476, 0.0220064427703619, 0.013883893378078938,
  0.03181556984782219, -0.02168262004852295, -0.010402800515294075, 0.023085853084921837,
  -0.027201097458600998, -0.011826271191239357, 0.027592383325099945, 0.012426692061126232,
  -0.037779301404953, -0.03235527500510216, 0.027767786756157875, -0.006692333612591028,
  -0.011441731825470924, -0.03759040683507919, 0.012251287698745728, 0.06557407230138779,
  0.04663045331835747, 0.037752315402030945, 0.02235725149512291, 0.010969489812850952,
  0.017230059951543808, 0.020495271310210228, 0.03348865360021591, 0.0256089698523283,
  -0.007636816240847111, 0.016609400510787964, 0.03340769559144974, -0.036376070231199265,
  -0.012844962999224663, -0.02768683061003685, 0.0204817783087492, 0.06616774946451187,
  -0.04916706308722496, -0.008122550323605537, 0.038265034556388855, 0.006054807920008898,
  -0.015948262065649033, 0.03305688872933388, 0.03203145042061806, -0.006142509635537863,
  0.05591336637735367, -0.018781710416078568, -0.0020255777053534985, 0.03286799415946007,
  0.0018518604338169098, 0.013533085584640503, -0.019159503281116486, 0.05607527866959572,
  0.06886626780033112, -0.014207716099917889, -0.05650704354047775, -0.019469833001494408,
  -0.048708315938711166, -0.0067328112199902534, -0.02733602374792099, 0.03203145042061806,
  0.06967582553625107, -0.03011550009250641, 0.013452130369842052, 0.04457957670092583,
  -0.05418631434440613, -0.02680981159210205, -0.013209262862801552, -0.0013745594769716263,
  -0.0443367101252079, -0.02131832018494606, 0.016744326800107956, 0.030358368530869484,
  0.03300292044878006, 0.01682528294622898, 0.041827086359262466, -0.012919171713292599,
  0.029090063646435738, -0.037212613970041275, 0.04579390957951546, -0.033812474459409714,
  -0.005656776018440723, -0.0598532073199749, 0.037914227694272995, -0.016029218211770058,
  0.0069352006539702415, -0.0019800402224063873, -0.03783327341079712, -0.003548555774614215,
  -0.029980575665831566, -0.045119281858205795, -0.019199980422854424, 0.0017742778873071074,
  -0.06573598831892014, -0.002118339529260993, -0.056129250675439835, -0.00451327720656991,
  0.07512684166431427, 0.05585939809679985, -0.03543158993124962, 0.04104451462626457,
  -0.002125085797160864, -0.04206995293498039, 0.030574249103665352, 0.036888789385557175,
  -0.020900050178170204, -0.02165563590824604, 0.01565142534673214, 0.0005405475967563689,
  -0.06681539118289948, 0.04260965436697006, -0.07307596504688263, -0.03837297856807709,
  -0.07275214046239853, -0.012568363919854164, 0.00972142443060875, -0.05272911116480827,
  -0.007353471126407385, 0.0032517185900360346, 0.054105356335639954, -0.015395065769553185,
  0.006196480244398117, 0.001359380199573934, -0.029872633516788483, 0.024529561400413513,
  -0.04606376215815544, -0.003359659342095256, 0.04938294366002083, -0.016582414507865906,
  0.006557407323271036, -0.027713816612958908, 0.033245787024497986, 0.004783129319548607,
  0.031032998114824295, -0.0070229023694992065, -0.03848091885447502, -0.009033300913870335,
  -0.04501134157180786, 0.060069091618061066, -0.03950635716319084, -0.051649704575538635,
  0.06341525912284851, -0.003889244282618165, 0.01805310882627964, 0.0054071624763309956,
  0.02169611304998398, 0.030358368530869484, -0.0036463772412389517, 0.029629766941070557,
  0.00017213616229128093, 0.0180800948292017, 0.0031336580868810415, -0.07124096900224686,
  0.010281367227435112, -0.035863351076841354, 0.07091714441776276, -0.08759400993585587,
  0.00042037907405756414, -0.008493596687912941, -0.007245530374348164, 0.018282484263181686,
  0.0008862956892699003, -0.006176241207867861, 0.023463645949959755, -0.022748537361621857,
  0.039803192019462585, 0.04206995293498039, 0.0398571640253067, 0.00962697621434927,
  0.024097798392176628, -0.03151873126626015, -0.03386644646525383, -0.046360600739717484,
  -0.03686180338263512, 0.019334906712174416, 0.0880257710814476, 0.016757819801568985,
  0.0211294237524271, -0.023342212662100792, 0.024664487689733505, -0.0069217076525092125,
  -0.02547404356300831, 0.02064369060099125, -0.027363009750843048, 0.024381142109632492,
  -0.02063019759953022, -0.017472926527261734, 0.020832587033510208, -0.04619868844747543,
  -0.005248624365776777, 0.01945634000003338, -0.044255755841732025, 0.02749793417751789,
  0.07307596504688263, 0.010814324952661991, 0.030358368530869484, -0.05531969293951988,
  0.052513230592012405, 0.02046828716993332, -0.008689239621162415, 0.0038588859606534243,
  0.001644411589950323, -0.013964849524199963, 0.026607422158122063, -0.02026589773595333,
  -0.0034068834502249956, 0.0443367101252079, 0.04468751698732376, 0.016069695353507996,
  0.0038352739065885544, -0.016056204214692116, 0.027133634313941002, -0.013350935652852058,
  0.05035441368818283, 0.03939841687679291, 0.052891023457050323, 0.017742779105901718,
  0.028334476053714752, 0.009674199856817722, -0.016056204214692116, 0.05432124063372612,
  -0.011826271191239357, -0.0049956380389630795, 0.027120141312479973, 0.022829493507742882,
  -0.014720435254275799, 0.08376210927963257, 0.01170483697205782, -0.006243704352527857,
  -0.004516650456935167, -0.03213939070701599, 0.059043653309345245, -0.05866586044430733,
  -0.02871226891875267, -0.009680946357548237, -0.006634989753365517, 0.015057750046253204,
  0.0013315516989678144, -0.016987193375825882, 0.0058119408786296844, -0.02542007341980934,
  0.02732253074645996, -0.01392437145113945, -0.010139694437384605, -0.012231049127876759,
  -0.050975073128938675, 0.021183393895626068, -0.024178752675652504, 0.05105602741241455,
  -0.051325879991054535, -0.02231677435338497, 0.052864037454128265, 0.0643327534198761,
  0.044282738119363785, 0.02683679759502411, -0.04584788158535957, 0.019388876855373383,
  0.047008246183395386, -0.026081211864948273, 0.06546613574028015, -0.042366787791252136,
  0.024543054401874542, -0.013539832085371017, 0.022465191781520844, -0.03011550009250641,
  0.03151873126626015, -0.00009813764336286113, -0.02372000552713871, 0.01299338135868311,
  -0.054402194917201996, -0.04765589162707329, -0.014059297740459442, -0.01348586194217205,
  0.00313534471206367, 0.052351318299770355, 0.03543158993124962, -0.047035232186317444,
  0.00955276656895876, 0.03392041474580765, -0.03203145042061806, 0.04457957670092583,
  -0.022262802347540855, 0.0016427249647676945, 0.031086968258023262, -0.016595907509326935,
  -0.026512974873185158, 0.03508077934384346, -0.005137310363352299, 0.01787770539522171,
  -0.01443709060549736, 0.021965965628623962, 0.04528119042515755, -0.006631616968661547,
  0.0059333741664886475, 0.03572842478752136, 0.023436659947037697, -0.053052935749292374,
  0.005056354682892561, -0.019658729434013367, 0.019874611869454384, 0.040990542620420456,
  0.08791783452033997, 0.01616414450109005, 0.025501029565930367, 0.03419026732444763,
  -0.0010153187904506922, -0.02903609164059162, 0.009748409502208233, 0.01856582798063755,
  0.0608246773481369, 0.0043648588471114635, -0.016380026936531067, -0.0678408294916153,
  0.025581983849406242, 0.03270608186721802, -0.03788724169135094, 0.014599001966416836,
  0.003432182129472494, -0.034298207610845566, 0.00575459748506546, 0.0020458167418837547,
  -0.009606736712157726, 0.0012455363757908344, 0.01563793234527111, 0.0010153187904506922,
  0.008669001050293446, 0.022640595212578773, -0.0030358368530869484, 0.059583358466625214,
  0.021372290328145027, -0.026769334450364113, 0.010011514648795128, -0.011644120328128338,
  0.00485396571457386, 0.007886429317295551, 0.01219731755554676, -0.06168820336461067,
  -0.04279855266213417, 0.04797971248626709, 0.06821862608194351, -0.03696974739432335,
  -0.04644155502319336, 0.009438078850507736, -0.05650704354047775, -0.010854803025722504,
  -0.011327044107019901, -0.023612063378095627, 0.029629766941070557, 0.008534074760973454,
  0.07917462289333344, 0.01067265309393406, 0.007441173307597637, 0.027470950037240982,
  0.0044188289903104305, 0.009161480702459812, 0.012312004342675209, 0.028982121497392654,
  -0.0069217076525092125, -0.004489665385335684, -0.022141369059681892, -0.021885009482502937,
  0.003260151483118534, -0.007812220137566328, -0.006631616968661547, 0.04104451462626457,
  -0.020063508301973343, 0.0556974858045578, -0.04463354870676994, -0.00893210619688034,
  -0.006429227534681559, -0.040828630328178406, 0.010423040017485619, 0.003266897751018405,
  -0.004746024962514639, -0.05197352543473244, -0.023261256515979767, -0.01100322138518095,
  0.0098023796454072, 0.005730985198169947, -0.03702371567487717, 0.06001511961221695,
  -0.003976946230977774, -0.019537296146154404, -0.05229735001921654, -0.03141079097986221,
  -0.004698800854384899, 0.028604328632354736, 0.034109313040971756, -0.026081211864948273,
  -0.05661498382687569, -0.042906492948532104, 0.061472322791814804, -0.009606736712157726,
  0.021439753472805023, -0.0017388598062098026, 0.0182420052587986, -0.01221755612641573,
  0.04142230749130249, 0.03178858384490013, 0.007926907390356064, -0.006489944644272327,
  0.02819954976439476, -0.001593814347870648, -0.062497761100530624, -0.017850719392299652,
  0.04986868053674698, 0.013796191662549973, 0.04487641528248787, 0.020927034318447113,
  -0.022748537361621857, -0.04695427417755127, 0.019550789147615433, -0.01790469139814377,
  0.008992822840809822, 0.03788724169135094, 0.0012674618046730757, -0.05326881632208824,
  0.016933223232626915, 0.02680981159210205, 0.03710467368364334, 0.05764042213559151,
  -0.018835680559277534, 0.014747420325875282, -0.03818408027291298, -0.02494783140718937,
  0.00009518612932879478, 0.029305944219231606, 0.03114093840122223, -0.025676432996988297,
  0.09898177534341812, -0.061796143651008606, -0.0014529852196574211, 0.023342212662100792,
  -0.009977784007787704, 0.001689949189312756, -0.015853814780712128, 0.02714712731540203,
  -0.02925197407603264, -0.026013748720288277, 0.012568363919854164, -0.023274749517440796,
  -0.03340769559144974, -0.007474904879927635, -0.012190571054816246, -0.02166912890970707,
  -0.023166807368397713, 0.04749397933483124, -0.010787339881062508, -0.00014515094517264515,
  -0.01546252891421318, -0.06104055792093277, 0.02077861689031124, -0.042042966932058334,
  0.03302990272641182, -0.010335337370634079, -0.027241574600338936, -0.03027741238474846,
  0.0007648622267879546, -0.008648761548101902, 0.008284461684525013, 0.011833016760647297,
  0.01142149232327938, -0.007238784339278936, -0.03589033707976341, -0.009869842790067196,
  0.07307596504688263, 0.02494783140718937, 0.0316806435585022, 0.01358031015843153,
  -0.03675386309623718, 0.05486094206571579, -0.027295546606183052, 0.02457003854215145,
  0.01802612468600273, -0.03060123510658741, -0.02733602374792099, 0.02115640975534916,
  -0.0005515103694051504, -0.0032011212315410376, -0.043419212102890015, 0.017257045954465866,
  -0.020927034318447113, 0.023477137088775635, 0.015948262065649033, -0.0068002743646502495,
  -0.03076314553618431, 0.1360054910182953, -0.06805671751499176, -0.029953589662909508,
  -0.012244542129337788, 0.01928093656897545, -0.018444394692778587, -0.019172996282577515,
  -0.03958731144666672, -0.012035406194627285, -0.014936316758394241, -0.033461667597293854,
  0.00460435263812542, 0.0009461691370233893, -0.007117350585758686, 0.0031437776051461697,
  0.005781582556664944, 0.03678084909915924, -0.06373907625675201, 0.023247763514518738,
  0.020495271310210228, -0.027565397322177887, 0.04061274975538254, -0.004155723378062248,
  0.04142230749130249, 0.011738568544387817, 0.05059728026390076, 0.019172996282577515,
  0.03956032544374466, 0.07156479358673096, 0.027120141312479973, 0.02734951674938202,
  0.021426262333989143, -0.004152350127696991, 0.0028823583852499723, 0.01142149232327938,
  0.033083874732255936, -0.03219336271286011, -0.009249182417988777, 0.004479545634239912,
  -0.05469903349876404, 0.01596175506711006, -0.048195596784353256, 0.016460981220006943,
  -0.04973375424742699, 0.016433997079730034, 0.02925197407603264, -0.01859281398355961,
  0.0075491140596568584, 0.03270608186721802, -0.019199980422854424, -0.02873925492167473,
  0.00197666697204113, -0.036726877093315125, -0.007663801312446594, -0.0034743465948849916,
  0.05445616692304611, 0.0020458167418837547, -0.004095006734132767, -0.0043850974179804325,
  -0.03238225728273392, 0.027430471032857895, -0.050975073128938675, 0.020400824025273323,
  0.03195049613714218, 0.023787468671798706, 0.00926942192018032, -0.008642015047371387,
  0.039614297449588776, -0.0800381526350975, 0.025312133133411407, -0.015408557839691639,
  -0.012332243844866753, 0.0016047770623117685, -0.004442441277205944, 0.005953613203018904,
  -0.005552208051085472, -0.02231677435338497, -0.00031939533073455095, 0.006769916042685509,
  -0.00597385223954916, 0.013141799718141556, -0.005380177404731512, 0.003535063238814473,
  0.017607852816581726, -0.00009486990165896714, -0.04619868844747543, 0.033110860735177994,
  -0.015867307782173157, 0.0016174264019355178, 0.0069891707971692085, 0.014248194172978401,
  0.05213543772697449, 0.00790666788816452, -0.0242462158203125, -0.033299755305051804,
  -0.001931129489094019, -0.0446605309844017, -0.03966826573014259, -0.012163585983216763,
  -0.07658404111862183, 0.010207157582044601, 0.000887982256244868, -0.035161737352609634,
  0.019469833001494408, 0.0015440603019669652, -0.014990287832915783, 0.02906307764351368,
  0.0018754724878817797, 0.021399276331067085, 0.010348830372095108, -0.030655205249786377,
  0.016069695353507996, 0.022789014503359795, 0.016393518075346947, -0.01906505413353443,
  0.02284298464655876, -0.021628649905323982, -0.004651576746255159, -0.030736161395907402,
  -0.01979365572333336, -0.045982807874679565, -0.011070684529840946, 0.03467600420117378,
  0.014072789810597897, -0.022073905915021896, 0.01910553313791752, 0.013357682153582573,
  -0.0013146860292181373, -0.10524234175682068, 0.0005257900920696557, -0.017230059951543808,
  -0.04379700496792793, 0.00044525606790557504, 0.0158403217792511, 0.009053539484739304,
  -0.002941388636827469, 0.01099647581577301, 0.02871226891875267, -0.07782536000013351,
  0.03262512758374214, -0.0004899503546766937, -0.017661822959780693, -0.031383804976940155,
  -0.028631314635276794, -0.0187277402728796, -0.03524269163608551, 0.00808881875127554,
  0.02992660365998745, -0.040693704038858414, -0.010989729315042496, 0.023436659947037697,
  0.005383550655096769, -0.031167924404144287, 0.011178625747561455, -0.009451571851968765,
  0.07145684957504272, 0.015422050841152668, -0.025716910138726234, -0.06708524376153946,
  0.010618682019412518, -0.04549707472324371, -0.006894722580909729, 0.018120571970939636,
  -0.010875041596591473, 0.0022954298183321953, -0.015395065769553185, -0.012271527200937271,
  -0.01056471187621355, -0.0020711154211312532, 0.027983669191598892, 0.04036988317966461,
  -0.038238052278757095, -0.013087829574942589, -0.03607923537492752, 0.021075453609228134,
  0.029278960078954697, -0.032760053873062134, -0.009944052435457706, 0.04779081791639328,
  0.03521570563316345, 0.018525350838899612, 0.0021115930285304785, -0.025312133133411407,
  -0.016393518075346947, -0.0041253650560975075, -0.0035215707030147314, 0.048033684492111206,
  0.004101752769201994, 0.039101578295230865, 0.013391413725912571, 0.0017675316194072366,
  0.01876821741461754, 0.001636822009459138, 0.033083874732255936, -0.02474544383585453,
  0.03238225728273392, -0.014747420325875282, -0.013438637368381023, -0.0077380104921758175,
  0.032571155577898026, -0.01686576008796692, 0.016933223232626915, -0.045820895582437515,
  0.0007408285164274275, 0.02873925492167473, 0.0008795493631623685, 0.008743209764361382,
  0.01521966140717268, 0.005562327802181244, 0.02868528477847576, -0.023301733657717705,
  0.021763576194643974, 0.06838053464889526, 0.027228083461523056, 0.10524234175682068,
  -0.015570469200611115, -0.019146010279655457, -0.020940527319908142, 0.0340823270380497,
  -0.019631745293736458, -0.08446372300386429, -0.022816000506281853, 0.02338268980383873,
  0.0605008527636528, 0.039101578295230865, -0.055130794644355774, -0.034298207610845566,
  0.026607422158122063, 0.019388876855373383, -0.003582287346944213, 0.012048899196088314,
  0.04250171408057213, 0.02699870802462101, 0.037914227694272995, 0.01494980975985527,
  -0.03475695848464966, -0.0556974858045578, -0.06352319568395615, 0.00004374556374386884,
  -0.016946716234087944, -0.027632860466837883, 0.0021031603682786226, -0.021196886897087097,
  0.03902062401175499, -0.0017371732974424958, -0.023585079237818718, 0.02941388450562954,
  0.013897386379539967, 0.025136727839708328, -0.006874483544379473, 0.00875670276582241,
  -0.007974131032824516, 0.030331382527947426, -0.001931129489094019, -0.03141079097986221,
  -0.0256089698523283, 0.000492901832330972, 0.009181719273328781, 0.04555104300379753,
  -0.007016156334429979, 0.032220348715782166, -0.012338990345597267, 0.02699870802462101,
  -0.006860991008579731, 0.017297523096203804, 0.006807020865380764, 0.00921545084565878,
  -0.013843415305018425, -0.03162667155265808, -0.019348399713635445, -0.027578890323638916,
  -0.004084886983036995, 0.04142230749130249, 0.03332674130797386, 0.013634280301630497,
  0.013742221519351006, 0.02701220102608204, -0.05259418487548828, -0.009579751640558243,
  -0.0009841170394793153, -0.0038082886021584272, -0.029144033789634705, -0.01005199272185564,
  -0.013114814646542072, -0.02443511225283146, 0.00987658929079771, -0.013087829574942589,
  0.030817115679383278, 0.00022684446594212204, -0.01254137884825468, 0.010800832882523537,
  0.01409977488219738, -0.049436915665864944, -0.05038139969110489, 0.0067328112199902534,
  0.029791679233312607, 0.008561059832572937, -0.04762890562415123, 0.05105602741241455,
  -0.03526967763900757, -0.03605224937200546, 0.030844101682305336, -0.012231049127876759,
  -0.019213473424315453, -0.012939411215484142, -0.014572016894817352, -0.021952472627162933,
  0.012278272770345211, -0.03383946046233177, -0.020225418731570244, 0.007380456663668156,
  0.0026698498986661434, -0.0595293864607811, 0.03969525173306465, -0.03332674130797386,
  0.03769834712147713, 0.02183103933930397, -0.012514393776655197, -0.006179614458233118,
  0.001438649371266365, 0.032571155577898026, 0.04690030589699745, -0.008837657980620861,
  0.026742348447442055, 0.007144336123019457, 0.035485558211803436, 0.06838053464889526,
  0.004270410630851984, -0.00020491897885221988, -0.009694438427686691, -0.05882776901125908,
  0.011273073963820934, -0.04846544936299324, -0.01704116351902485, 0.008689239621162415,
  0.06330731511116028, -0.014760913327336311, 0.009478556923568249, -0.027875728905200958,
  0.027551906183362007, -0.018471380695700645, -0.018282484263181686, -0.011205610819160938,
  -0.08057785779237747, -0.004938294645398855, 0.015327602624893188, -0.04457957670092583,
  -0.007124097086489201, -0.007029648870229721, -0.02714712731540203, 0.040288928896188736,
  -0.010382561944425106, -0.045308176428079605, 0.021871518343687057, 0.047359053045511246,
  -0.003164016641676426, -0.00962697621434927, -0.04328428581357002, 0.0038690052460879087,
  0.013209262862801552, 0.0015904412139207125, 0.013114814646542072, -0.07334581762552261,
  -0.015422050841152668, -0.005083340220153332, -0.07895874232053757, 0.022977910935878754,
  0.021088946610689163, 0.020077001303434372, -0.010780593380331993, -0.009154734201729298,
  0.034460119903087616, -0.01419422309845686, 0.00683063268661499, -0.0006607161485590041,
  0.02598676271736622, 0.00026647900813259184, 0.02149372361600399, 0.0013897386379539967,
  0.0007720301509834826, -0.021547695621848106, 0.008453118614852428, 0.02354460023343563,
  -0.018471380695700645, 0.0006156002636998892, 0.040693704038858414, 0.03815709426999092,
  -0.03702371567487717, -0.0025214310735464096, 0.025541506707668304, 0.009782141074538231,
  -0.02185802534222603, 0.042393773794174194, 0.033461667597293854, -0.017675315961241722,
  0.01619112864136696, -0.05162271857261658, 0.015503006987273693, 0.01685226708650589,
  0.039992090314626694, 0.00023443406098522246, 0.008763449266552925, 0.001134222373366356,
  -0.007184813730418682, -0.026175659149885178, 0.01894362084567547, -0.02906307764351368,
  0.01924045942723751, 0.005039488896727562, 0.026229629293084145, 0.01754038967192173,
  0.04158421605825424, -0.01617763750255108, 0.04781780391931534, 0.007987624034285545,
  -0.025055773556232452, -0.023625556379556656, 0.001093744533136487, 0.027632860466837883,
  -0.035485558211803436, 0.01925395242869854, -0.0035384364891797304, -0.026526467874646187,
  -0.016973700374364853, -0.0031791958026587963, 0.03157270327210426, -0.029953589662909508,
  0.03969525173306465, -0.014760913327336311, 0.019307922571897507, -0.009438078850507736,
  -0.055967338383197784, -0.002273504389449954, -0.000270484626526013, -0.031006013974547386,
  -0.0256089698523283, -0.042042966932058334, 0.03783327341079712, 0.014045804738998413,
  0.0037273329216986895, 0.035485558211803436, -0.022438207641243935, -0.015098228119313717,
  0.045820895582437515, -0.015111721120774746, 0.05402440205216408, -0.024988310411572456,
  0.013499354012310505, -0.02765984646975994, 0.06730113178491592, 0.020940527319908142,
  0.04250171408057213, -0.012055644765496254, 0.021412769332528114, 0.009836111217737198,
  -0.055643513798713684, -0.002057622652500868, -0.009599990211427212, 0.026539959013462067,
  0.009856349788606167, -0.0032972560729831457, 0.02301838994026184, 0.06854245066642761,
  -0.0151252131909132, -0.005144056864082813, 0.00919521227478981, 0.006570899859070778,
  0.053241830319166183, 0.014639480039477348, -0.01837693154811859, -0.02046828716993332,
  0.000015574474673485383, -0.026715364307165146, 0.008284461684525013, 0.022262802347540855,
  0.016069695353507996, -0.03321880102157593, -0.01751340553164482, -0.05607527866959572,
  0.002290370175614953, -0.012109615840017796, -0.00997103750705719, -0.02165563590824604,
  -0.014882346615195274, -0.07474904507398605, 0.003379898378625512, 0.04471450299024582,
  0.02461051754653454, -0.03340769559144974, 0.03435217961668968, -0.009937305934727192,
  0.023112837225198746, 0.012001674622297287, 0.01461249403655529, -0.053889475762844086,
  0.015152198262512684, 0.027875728905200958, -0.022789014503359795, -0.017931675538420677,
  -0.02284298464655876, 0.002872238866984844, -0.007339978590607643, 0.013357682153582573,
  0.005744477733969688, 0.015192676335573196, 0.013553325086832047, -0.0021554441191256046,
  -0.0182420052587986, -0.017081642523407936, 0.026728855445981026, -0.00451327720656991,
  -0.024111289530992508, -0.04206995293498039, 0.04501134157180786, -0.02819954976439476,
  -0.07253625988960266, -0.010119455866515636, 0.03626812994480133, 0.033245787024497986,
  -0.009842857718467712, -0.023666033521294594, -0.024866877123713493, -0.03902062401175499,
  0.039830178022384644, 0.030871087685227394, 0.04198899492621422, -0.041503261774778366,
  0.07917462289333344, 0.035512544214725494, 0.001792830298654735, 0.07215847074985504,
  -0.01891663670539856, 0.009411093778908253, -0.029683737084269524, 0.008972584269940853,
  -0.016002232208848, 0.0038858710322529078, -0.03556651622056961, 0.02233026549220085,
];

// query: What is design thinking? / text-embedding-3-small / 1024d
export const designThinkingQuery2 = [
  0.046730567, 0.0026507701, 0.016324067, 0.017052943, 0.0147838015, -0.021164903, -0.015622697,
  0.051461384, -0.023227759, -0.018840753, 0.003940055, 0.026280787, -0.014082431, -0.05869513,
  0.04188973, 0.03174048, 0.003754398, -0.07096225, -0.0027264082, 0.02380536, 0.036333773,
  0.047583215, 0.0025407511, 0.050966296, 0.040211942, 0.00052216044, 0.04018444, 0.0666165,
  -0.024066655, 0.016571611, 0.017231725, -0.04304493, -0.03135541, -0.04640051, 0.051296353,
  0.020436028, -0.043595023, 0.022760179, 0.03545362, 0.046428014, -0.06106054, -0.00017405348,
  0.021343684, 0.041202113, 0.025758196, -0.031410422, 0.020147229, 0.021096142, -0.007976376,
  0.020958617, -0.00063217944, 0.00086897815, 0.05451441, -0.014206202, -0.03688387, -0.023791607,
  -0.0059375875, 0.012377136, -0.03058528, 0.06634145, 0.025331873, -0.02526311, 0.014384983,
  0.020724827, 0.0010159566, 0.024602996, -0.040129427, 0.0060201017, -0.017699305, -0.016406583,
  0.05484447, 0.03944181, -0.03564615, 0.012294622, 0.03567366, -0.020133475, -0.022856446,
  0.0005840461, 0.05462443, -0.011001899, 0.03575617, -0.021041133, 0.016750392, -0.008100148,
  -0.024520483, 0.0015600349, -0.011517613, -0.0051640165, -0.051488888, -0.04164219, -0.02733972,
  -0.03295069, -0.00028084926, 0.018125629, -0.08135904, 0.03905674, 0.028659947, 0.069972076,
  0.022003798, 0.030365242, 0.037296437, 0.0012927231, 0.0020714514, 0.04414512, -0.012714069,
  0.005549083, -0.011943936, 0.023557816, -0.03094284, 0.016420335, -0.088125214, -0.03383084,
  -0.000881871, 0.017465515, -0.046565536, -0.025579415, 0.005480321, -0.05242405, 0.011668889,
  -0.027559757, 0.015210126, -0.049205992, -0.008925291, 0.075087965, -0.07195242, -0.018345667,
  -0.0056762923, -0.028769966, -0.0012789707, 0.0101355, -0.017988104, 0.014756297, 0.017520525,
  -0.036251258, 0.007866357, -0.04373255, -0.044695213, 0.031878002, -0.07519798, -0.06639646,
  0.0269409, 0.0109331375, 0.000091592956, 0.017025439, 0.045355327, -0.04431015, 0.0023327465,
  0.060455434, 0.037791524, 0.02467176, -0.010589328, -0.023227759, -0.011668889, 0.016984181,
  0.06656149, -0.0364988, 0.027037168, 0.022223836, 0.036003716, -0.027986081, 0.007687577,
  0.004672369, -0.033500783, -0.019514618, -0.0013468731, 0.0010271304, 0.030557774, -0.019830924,
  -0.0048408355, -0.00091195427, -0.050361194, 0.020958617, -0.00788011, 0.04934352, 0.088840336,
  0.021976294, -0.040404476, 0.010039233, -0.040486988, 0.0053256066, -0.04860089, 0.00008391097,
  -0.10506814, -0.015320145, 0.0016227801, 0.012927231, -0.027669776, -0.071622364, -0.009255348,
  0.008003881, -0.055422068, -0.0035756172, 0.039936893, -0.073272645, 0.009606034, 0.0151138585,
  0.046097957, 0.037791524, 0.040514495, -0.053771783, -0.0096197855, -0.046840586, -0.004166969,
  -0.0075569293, -0.0098398235, 0.08972049, 0.021068636, -0.029127527, -0.028824976, -0.03869918,
  -0.003754398, 0.009296604, -0.008127653, 0.016310316, 0.021013627, 0.017548028, 0.009482262,
  -0.026913395, -0.039249275, 0.035068553, -0.032290574, 0.015155116, -0.020587305, 0.023860369,
  -0.017561782, 0.00087800313, 0.012040203, -0.008877157, -0.017341744, -0.06634145, -0.006295149,
  -0.08636491, 0.009234719, -0.030447755, -0.010850623, -0.005002426, 0.008787767, -0.010878128,
  0.04106459, -0.031300403, 0.0010365852, -0.052589078, -0.016846659, 0.002392913, -0.040872056,
  0.014632526, -0.039936893, -0.009447881, -0.012480279, 0.0154439155, -0.021646237, -0.04466771,
  0.007941996, -0.028412404, 0.040349465, -0.004668931, -0.008285806, 0.015127611, 0.02016098,
  -0.0063467207, -0.034518458, -0.050718755, 0.030612784, 0.022746427, -0.039771866, 0.030722803,
  0.016255306, -0.030722803, -0.033115715, -0.009110948, 0.008402701, -0.017424257, -0.010018605,
  0.057319894, -0.01906079, -0.06287585, 0.006367349, 0.05875014, -0.026212025, -0.07624316,
  0.019569628, -0.048628394, 0.010678718, -0.00669053, -0.04004691, 0.04931601, 0.010575576,
  0.021797512, -0.05481696, -0.027298462, 0.0022089751, 0.012865346, -0.014206202, -0.0047686356,
  -0.037819028, 0.016516602, 0.01610403, -0.0011680922, -0.004514217, -0.0039847502, 0.04136714,
  -0.026913395, -0.0012463089, -0.02293896, 0.0036959504, -0.024231683, -0.02357157, -0.018813247,
  0.09379119, -0.0051399497, 0.001233416, -0.034986038, 0.051076315, -0.0457679, -0.017864333,
  0.03770901, -0.002282894, 0.0683768, -0.053909305, -0.061940692, -0.067386635, 0.010403671,
  0.01963839, 0.0014861159, 0.009083443, 0.0023843178, 0.01036929, 0.032648135, 0.012157098,
  -0.036113735, 0.028990004, -0.03454596, 0.001765461, 0.016901668, -0.0027074986, -0.029402575,
  -0.049838603, -0.035948705, 0.022856446, -0.039524324, -0.031245394, -0.036361277, -0.025923224,
  -0.026253281, 0.05770496, 0.035151068, -0.076023124, -0.05764995, 0.025936978, 0.0058928924,
  0.016351573, 0.0063054636, -0.0005531033, -0.022237588, -0.0252081, 0.06078549, -0.034766,
  -0.034710992, -0.04100958, -0.047885764, -0.004524531, -0.005421873, 0.014742545, 0.033885848,
  -0.011861422, 0.03152044, -0.004648302, 0.02577195, -0.04015693, -0.05374428, -0.0006704282,
  -0.007164987, -0.024630502, 0.043127444, -0.002172875, -0.034490954, 0.02733972, 0.017383,
  -0.0002748326, 0.053331707, 0.040927064, 0.067221604, -0.007405653, -0.050993804, -0.0077425865,
  0.055284545, 0.069972076, -0.03894672, -0.012720946, -0.05168142, 0.026225777, -0.021536218,
  0.04188973, 0.0036409409, 0.007983253, 0.032125544, -0.003240403, 0.004252922, 0.019954694,
  0.007316263, -0.005146826, -0.029182537, -0.016599115, 0.0199822, -0.0006450723, -0.000266882,
  0.006958701, 0.009447881, 0.0058997683, -0.01193706, 0.043760054, -0.060565453, 0.009743557,
  0.0666165, -0.004675807, -0.029925166, 0.037296437, 0.036223754, 0.050966296, 0.06018039,
  -0.030557774, 0.0411471, 0.008505844, -0.010740604, 0.042412322, 0.061610635, 0.041972246,
  0.012397765, 0.05352424, -0.03933179, -0.014398735, 0.024382958, 0.076903276, -0.01604902,
  -0.0590802, 0.039551828, 0.011304451, -0.0021917846, 0.044530187, 0.013766127, -0.044860244,
  0.014618773, -0.003491384, -0.009516642, -0.04785826, 0.02537313, -0.03457347, -0.0061266827,
  -0.04587792, -0.07442785, -0.0029120652, 0.016214048, 0.015072602, 0.034903526, 0.0099429665,
  -0.040349465, -0.035426114, 0.027752291, 0.010788737, 0.02456174, -0.017946849, -0.018827,
  -0.018964523, 0.063921034, -0.0031131937, 0.013209155, 0.020793589, 0.020367267, 0.011187556,
  -0.025180597, -0.023200255, 0.0063570347, 0.024149168, 0.037516475, 0.0107130995, 0.012892851,
  0.013573593, -0.022801436, 0.023722844, 0.05544957, 0.014054926, -0.00661833, 0.030117698,
  0.05687982, 0.005215588, 0.010114871, -0.11607003, -0.025923224, -0.0038437885, -0.008608986,
  0.018978275, -0.02989766, 0.0058069397, -0.011201308, -0.014261211, 0.0051605785, 0.028192367,
  -0.026033243, -0.0009910305, -0.019377096, 0.013876146, 0.023420293, -0.0064876825, 0.062325757,
  -0.032043032, 0.0018565705, 0.06106054, -0.0042632357, 0.026844634, 0.004582979, 0.011228813,
  -0.005480321, -0.011586375, 0.043292474, 0.029045014, -0.045740396, -0.013298546, 0.010781861,
  0.0024754272, -0.011476356, -0.00045941523, -0.00016792938, -0.038149085, 0.021866275,
  0.016846659, 0.026913395, -0.046483025, -0.03248311, -0.04502527, 0.008395825, -0.02786231,
  0.0024891796, 0.002979108, 0.011311327, -0.05346923, -0.07393276, -0.010114871, 0.0046826834,
  -0.04796828, 0.017341744, 0.013112889, 0.02416292, -0.017754314, -0.0067042825, -0.016530354,
  0.012549041, -0.030200213, -0.048105802, 0.014687535, 0.061390597, 0.0335833, -0.00764632,
  0.030172708, -0.055064507, 0.05115883, 0.0071718628, -0.02937507, -0.009956718, 0.00946851,
  -0.025991987, -0.012487155, -0.028054843, 0.0016863849, -0.022787683, 0.054706942, 0.0144537445,
  -0.020050962, -0.056824807, 0.0013176494, -0.0062332633, -0.045685384, -0.0048064548,
  -0.005930711, 0.009798567, 0.026033243, 0.073657714, 0.04095457, -0.011689518, -0.02989766,
  0.0044007595, -0.01668163, -0.0063398443, -0.019803418, -0.00434575, -0.04431015, -0.0111531755,
  0.014605021, -0.053194184, 0.047693234, 0.0147838015, -0.013312298, 0.018221896, 0.06320591,
  0.025015568, 0.041312132, -0.04125712, -0.031767983, 0.04483274, 0.0032197745, 0.0054596923,
  -0.011304451, 0.002057699, -0.00111824, 0.0050574355, -0.035481125, 0.0082995575, -0.020298503,
  0.025153091, 0.0013606255, -0.0054115593, 0.008780891, -0.0036203123, 0.038424134, -0.04623548,
  -0.015787724, -0.009709176, -0.0031131937, 0.04367754, 0.070137106, 0.027793547, -0.012411517,
  -0.038011562, 0.035343602, -0.0089527955, 0.02009222, -0.011070661, -0.019803418, -0.015773972,
  0.06700157, 0.008540224, -0.0026799939, 0.005996035, -0.005635035, 0.0061198063, -0.042439826,
  0.027064672, 0.0016279373, 0.019033285, -0.05063624, 0.0031802366, -0.0034707554, -0.0011680922,
  0.004125712, -0.032978192, 0.06914694, 0.009544147, -0.025098082, 0.003943493, -0.02068357,
  0.028879985, -0.0076325675, -0.037186418, 0.049040966, -0.015663953, 0.060675472, 0.07552804,
  -0.04582291, -0.0045726644, -0.04007442, 0.067056574, 0.04018444, -0.026445815, -0.018744485,
  0.037103906, 0.018565705, 0.0043113693, 0.026734615, 0.0026198272, -0.017644295, 0.017905591,
  -0.04931601, -0.00062057585, 0.0071306056, 0.02641831, -0.0012841279, -0.00037819028, 0.011806413,
  -0.014660031, 0.031410422, -0.01300287, -0.013305422, -0.012610927, 0.01604902, -0.017561782,
  0.028879985, -0.035233583, 0.015787724, -0.0037234551, -0.02595073, 0.00049508543, 0.007385025,
  -0.027986081, -0.003414027, -0.0069724536, 0.032895677, -0.063756004, 0.030887831, 0.039469313,
  0.006632082, -0.047995783, -0.0097023, 0.007886986, -0.016997933, 0.036636323, 0.027532253,
  -0.041119598, -0.014261211, 0.0052052736, 0.022127569, 0.0370764, -0.0115932515, -0.030887831,
  0.012122718, -0.008725882, 0.010967518, 0.01413744, -0.025194349, 0.056219704, 0.018153133,
  0.008340815, -0.08548476, -0.021797512, -0.020050962, -0.006260768, 0.01947336, -0.022498883,
  -0.00054665684, -0.009661043, 0.036278762, -0.009887957, -0.012287746, 0.07514297, 0.05399182,
  -0.045437843, 0.011985194, 0.012892851, -0.020284751, 0.013580469, 0.047198147, 0.011943936,
  -0.023915378, 0.016777895, 0.031933013, 0.026322044, 0.016406583, 0.00396756, 0.00860211,
  0.061830673, -0.008244548, -0.015567687, 0.028769966, 0.008237672, -0.05786999, -0.015416411,
  -0.061940692, 0.00009830799, -0.002171156, -0.015045097, 0.013394812, 0.013917402, 0.023296522,
  0.013711117, -0.052451555, -0.012879098, -0.020711076, -0.0074812914, 0.0182494, 0.018111877,
  -0.026555834, 0.01098127, -0.0027831367, 0.008175787, -0.001469785, -0.011840794, 0.059025187,
  -0.026432063, -0.006580511, 0.036168743, -0.008120777, -0.00903531, 0.026253281, -0.037901543,
  -0.033033203, -0.016874162, -0.032070536, -0.036223754, -0.023158997, -0.0076669483,
  -0.0153338965, -0.030392746, 0.031190384, 0.003988188, 0.0061645014, -0.01477005, -0.03556364,
  0.036003716, 0.005707235, -0.029045014, -0.028109852, -0.020243494, 0.020532295, -0.029320061,
  0.00065710564, 0.002133337, 0.021123646, -0.02879747, -0.02641831, -0.05247906, -0.0019648704,
  -0.007928244, 0.0088427765, 0.017795572, -0.01413744, -0.002863932, -0.04015693, -0.055229533,
  0.022196332, 0.0035446745, -0.028247377, 0.005115883, 0.03905674, -0.036058724, 0.009516642,
  -0.023530312, 0.0625458, 0.014976335, 0.009324109, -0.025565663, -0.05278161, -0.020697324,
  0.01529264, -0.026665853, 0.020271, 0.02154997, -0.027724786, 0.010554947, -0.05058123,
  -0.011840794, 0.03330825, 0.04414512, -0.029650118, 0.013195403, -0.025675682, -0.00587914,
  -0.0049095973, -0.0199822, 0.029320061, -0.03614124, -0.047940776, -0.03295069, -0.07266755,
  0.0106030805, -0.027092177, 0.0275185, -0.035013545, -0.012102089, -0.0031131937, 0.014618773,
  -0.023324026, -0.0058997683, -0.007694453, 0.016516602, -0.0026799939, -0.014495002,
  -0.0038884836, 0.0014938517, -0.013415441, 0.0061163683, 0.04722565, -0.004187598, 0.048408356,
  -0.021659989, -0.060015358, -0.009317233, 0.011276946, 0.028412404, -0.008230796, 0.041834723,
  0.07195242, -0.027490996, 0.017548028, 0.0007125449, -0.00587914, -0.029787641, 0.027394729,
  0.006652711, 0.054486904, -0.030557774, -0.019624637, -0.04849087, -0.005707235, 0.0073300153,
  0.0034277793, -0.06474618, -0.0088496525, 0.008719005, -0.005669416, 0.007852606, 0.017025439,
  0.000085683736, -0.025813205, -0.029870156, 0.039716855, 0.02057355, -0.03985438, 0.0017267824,
  -0.023090236, -0.011366337, -0.0058997683, -0.060620464, 0.029100023, -0.025070578, -0.0030031747,
  0.029595109, 0.013222908, 0.011146299, -0.006700844, -0.0023086797, -0.02236136, -0.065956384,
  -0.0071512344, -0.042907406, -0.005748492, 0.043374985, -0.031162878, 0.04832584, -0.01727298,
  0.03157545, 0.03776402, 0.028467413, 0.052396543, 0.03152044, -0.0037509599, -0.026789624,
  0.040652018, -0.022100065, 0.021522464, -0.013876146, 0.012885975, 0.0033280745, -0.052121498,
  -0.032648135, -0.039029237, 0.013147269, -0.030255223, -0.006522063, 0.03578368, 0.059465263,
  0.024369206, 0.01848319, 0.044117615, 0.023970388, 0.037461467, 0.011084413, 0.024094159,
  0.0048373975, -0.037213925, -0.0013142113, -0.023860369, 0.034518458, -0.024836788, -0.013415441,
  -0.033060707, -0.0113732135, -0.013463574, -0.00086768885, -0.025043072, 0.009626661, 0.04147716,
  -0.07261253, -0.0017826514, 0.020986123, 0.0411471, -0.02236136, 0.03377583, 0.00006629074,
  0.032868173, -0.014357478, 0.058640122, 0.009537271, 0.0034432507, 0.04235731, -0.0024462035,
  -0.0003689504, -0.033528287, -0.0031389794, -0.036801353, -0.009434128, 0.021412445, 0.035206076,
  0.0047514453, -0.020202238, 0.026748367, 0.0016889634, 0.045987938, 0.029320061, -0.030172708,
  -0.059740312, 0.007495044, -0.013442946, -0.014866316, 0.021852521, -0.011524489, 0.018579457,
  0.0011912994, -0.052754108, 0.025290616, -0.024960559, 0.027504748, 0.016612867, 0.04164219,
  0.021041133, 0.028219871, -0.019817172, 0.025386883, 0.036251258, 0.007164987, -0.0112631945,
  -0.027202196, -0.0013821136, 0.009997976, -0.019858427, -0.029210042, -0.0021539656,
];

/**
 * chunk:
 *
 * ``````jsx filename="app/client-redirect.jsx" switcher'use client'import { navigate } from './actions'export function ClientRedirect() {  return (    <form action={navigate}>      <input type="text" name="id" />      <button>Submit</button>    </form>  )}``````ts filename="app/actions.ts" switcher'use server'import { redirect } from 'next/navigation'export async function navigate(data: FormData) {  redirect(`/posts/${data.get('id')}`)}``````js filename="app/actions.js" switcher'use server'import { redirect } from 'next/navigation'export async function navigate(data) {  redirect(`/posts/${data.get('id')}`)}```
 */

export const codeEmbedding = [
  -0.026551945, 0.016553676, -0.009343998, 0.00018550233, 0.012151644, -0.003228475, -0.012577237,
  0.020072762, 0.015677081, -0.03488595, 0.031328753, -0.02089854, 0.034276146, 0.016452042,
  0.004487787, 0.0005137293, 0.0017770111, -0.040780738, 0.017620835, 0.046853382, 0.01092568,
  -0.022219785, 0.02718716, 0.03686782, -0.04111105, -0.054475952, -0.028610038, 0.02045389,
  0.0018595889, -0.0028521107, 0.043143734, -0.021482937, 0.013695213, 0.03524167, -0.020060057,
  -0.007635272, 0.0005280216, -0.011497373, -0.000051511885, -0.009140729, -0.04479529, -0.06316568,
  0.0071969745, 0.017951148, 0.0046084775, 0.039408676, -0.08776116, -0.016007392, -0.015003754,
  0.01676965, 0.013250563, -0.07978287, -0.011592655, 0.09136918, -0.0069047757, -0.011135301,
  -0.045582954, 0.028305136, 0.028355952, 0.024011089, 0.03524167, 0.008060865, -0.019780563,
  -0.0036937692, 0.033437666, 0.012393025, -0.04088237, 0.025929434, 0.06524918, 0.05224,
  0.030998442, 0.03069354, 0.02176243, -0.01787492, 0.00427499, 0.0026170816, -0.00037160018,
  0.012069066, 0.048225444, -0.03292949, -0.017023735, 0.03773171, -0.018446613, -0.05782988,
  -0.030439453, -0.011294104, -0.05437432, -0.0112814, -0.010519143, -0.013847665, 0.0090263905,
  -0.004084426, -0.09761968, 0.03315817, 0.003671537, 0.008346711, -0.048276264, -0.035546575,
  0.029219843, 0.05589883, 0.06845066, -0.08288272, -0.0046275337, 0.05325634, 0.008422937,
  -0.013974708, 0.0114402035, 0.02982965, 0.014495583, -0.012583589, 0.009718774, -0.031430386,
  0.041009415, 0.04550673, 0.031303346, -0.039408676, -0.008168852, -0.025891323, -0.029321477,
  -0.053357974, 0.029575562, -0.0003950237, 0.046319805, -0.017646244, -0.03209101, -0.025764279,
  0.009458336, 0.008334007, -0.04639603, -0.01876422, 0.040094707, 0.01410175, -0.017252412,
  -0.050131086, -0.042915057, 0.033971243, -0.061742797, -0.016185252, -0.06667206, 0.0673835,
  -0.0007614628, -0.043905992, 0.046218168, -0.006993706, -0.01401282, 0.012570885, 0.010430214,
  0.06951782, -0.03597852, -0.0014768725, 0.012856731, -0.020707976, -0.0060631176, 0.044007625,
  0.041593812, -0.007266848, 0.033463072, 0.048759025, -0.048682798, 0.028305136, -0.03839233,
  -0.042356066, -0.023274241, 0.009051799, -0.049190972, -0.052697353, 0.007825837, -0.023833228,
  -0.060370736, 0.03244673, -0.017239707, -0.039789803, -0.0052722762, -0.00037537178, 0.023502918,
  -0.034657273, 0.014686148, -0.052443266, 0.003503205, 0.09828031, 0.033666342, 0.05274817,
  -0.061946068, 0.019018307, -0.024900388, -0.015181614, -0.009153433, 0.018535543, -0.010157072,
  -0.0075209336, -0.020428482, 0.07109315, -0.0033189931, -0.060116652, 0.0011290929, -0.033234395,
  0.007616216, 0.011148006, 0.020530116, 0.014889415, 0.008880291, -0.03821447, 0.03719813,
  -0.03470809, -0.023998383, -0.008619853, -0.04200035, -0.03559739, 0.02282959, -0.028787898,
  0.016261477, -0.004487787, -0.045405094, 0.0019866317, -0.013860369, 0.008035457, 0.0062949704,
  0.04375354, -0.0030998443, 0.04375354, 0.033259805, 0.022080038, 0.024798755, -0.109968245,
  0.047742683, 0.044566613, 0.02431599, -0.042940464, -0.012088122, 0.034504823, 0.01763354,
  0.036613733, -0.014343131, -0.03526708, 0.030134551, -0.022448462, 0.023058267, -0.023769706,
  -0.031862333, 0.0044369698, -0.008264134, -0.040450428, 0.011395738, -0.033945836, 0.060777273,
  0.014787781, -0.010697003, -0.0052532195, -0.023858637, 0.008257782, -0.012901196, 0.058998674,
  0.024176244, -0.009756886, -0.01911994, -0.015092684, 0.04637062, -0.0038652772, -0.008156148,
  0.05396778, 0.0066252816, -0.02927066, 0.044592023, 0.03310735, -0.0017770111, 0.004554484,
  0.030718949, 0.07846163, 0.024938501, -0.015448404, 0.009331293, 0.01481319, 0.01445747,
  -0.027568286, -0.020047354, -0.04563377, 0.03351389, 0.016655311, -0.034631867, 0.04652307,
  -0.0127614485, -0.048352487, -0.008931109, -0.0028012937, -0.0033761624, 0.011859445,
  -0.011109892, 0.019450253, 0.004351216, -0.0430421, 0.0131108165, -0.02125426, -0.059913382,
  -0.062098518, 0.015143502, 0.072973385, 0.01834498, -0.012742393, 0.003960559, 0.019958423,
  0.012011897, -0.03351389, -0.022041924, -0.010379396, 0.029804239, -0.011376683, 0.004319455,
  0.048098404, 0.05396778, 0.014914825, -0.059456028, -0.02065716, 0.009331293, -0.029397704,
  0.007838541, -0.036918636, -0.0047196397, -0.037350584, 0.04639603, -0.01499105, -0.028813306,
  -0.0015205435, -0.0057455106, -0.029245252, -0.026755214, -0.0059328987, -0.023464805,
  0.027619103, 0.0057899756, 0.023350466, 0.015219727, -0.014584513, 0.020085465, 0.07632731,
  0.032014783, -0.029041983, -0.047132876, 0.03864642, -0.056508634, 0.039434083, -0.046624705,
  -0.044719063, -0.02662817, -0.003935151, -0.022639027, 0.009667957, 0.0019913958, -0.022854999,
  -0.0036016633, -0.022816885, -0.024150835, 0.015753306, -0.042051166, 0.0008718312, -0.021457529,
  0.013746031, -0.025865912, -0.047005836, -0.03224346, 0.0317607, -0.01570249, 0.031633656,
  -0.025840504, -0.004332159, 0.013911186, -0.0035127334, -0.049699143, -0.012678871, 0.08567766,
  -0.010652538, 0.05554311, -0.007343074, -0.053815328, 0.042203616, -0.007838541, -0.054577585,
  0.0032840562, 0.020860428, 0.0635214, 0.036562916, 0.009756886, 0.058846224, -0.07836,
  -0.03806202, -0.039561126, -0.004224173, 0.038875096, -0.030210776, 0.030185368, 0.010754173,
  -0.0053866147, -0.018116303, -0.009795, 0.0040240805, -0.016985621, 0.034276146, 0.096044354,
  0.02556101, -0.00679679, 0.00516429, 0.050486807, 0.0061679278, -0.054577585, -0.022435758,
  0.062149335, 0.013784143, 0.008454698, -0.029016575, 0.023947567, -0.020923948, 0.056356184,
  -0.034149103, 0.057982333, 0.025345039, 0.03328521, -0.026729804, 0.02627245, -0.00798464,
  0.0074256514, 0.0041161864, 0.013352198, -0.073634006, -0.008778657, -0.02529422, -0.0299821,
  -0.020962061, 0.07038171, -0.008232373, 0.05925276, -0.07647976, -0.037477624, 0.030642722,
  0.055289026, 0.004853035, 0.025726166, 0.016324999, 0.016197957, -0.017201595, 0.045430504,
  0.0047641047, 0.006047237, 0.013002831, -0.051528558, 0.043448634, 0.049089335, -0.003050615,
  -0.028228909, -0.05376451, -0.05376451, -0.020225214, -0.039129183, 0.0069810017, -0.027263384,
  -0.020758793, -0.06402957, -0.003047439, -0.006314027, -0.03737599, 0.0670786, -0.019945718,
  -0.011427499, 0.044312526, -0.03303113, -0.018446613, 0.028330544, 0.021025583, 0.009896634,
  0.008645263, 0.06580817, -0.012170699, 0.017138073, 0.011294104, 0.004691055, -0.024684416,
  -0.06484264, 0.019018307, 0.048022177, -0.0051166485, 0.01959, -0.048708208, -0.0035159094,
  0.040069297, -0.020847723, -0.0525449, 0.029524745, 0.051604785, -0.009108968, 0.015931167,
  -0.017265117, 0.03310735, 0.04070451, 0.03336144, 0.031074667, -0.011338569, 0.00896287,
  -0.043245368, 0.004535428, 0.002813998, -0.015626265, -0.04022175, -0.0069174804, -0.04034879,
  -0.036588326, -0.0030315588, 0.0011767339, -0.0025313278, 0.0047958656, 0.032700814,
  -0.0034873248, -0.0071461573, 0.008886644, -0.032675408, -0.037274357, -0.030998442, -0.0299821,
  0.026145408, -0.042076573, 0.04464284, -0.008295895, -0.019374026, -0.017252412, -0.018294163,
  -0.025954843, -0.011643472, -0.020377664, 0.018459318, 0.021660797, -0.06712941, -0.0013268032,
  -0.012653463, 0.03336144, -0.021711614, 0.019818677, 0.014749669, 0.027644513, -0.026018364,
  -0.06651961, -0.005634348, -0.005866201, -0.034504823, 0.0072477916, 0.004586245, 0.0031252527,
  -0.024773344, -0.031481206, 0.0045195473, -0.0011219467, 0.025421264, -0.010506439, 0.022130854,
  -0.01974245, -0.054221865, 0.009566323, -0.033234395, 0.02080961, -0.025637235, 0.017913034,
  -0.017824104, 0.006237801, -0.024328696, -0.006771381, 0.03440319, 0.05010568, -0.027110932,
  0.0149783455, -0.008721488, 0.019488364, 0.037782528, -0.023134492, 0.006088526, 0.00065982854,
  0.0356228, -0.026933072, 0.03526708, -0.012901196, 0.01790033, 0.005815384, -0.02247387,
  -0.025078248, 0.01816712, -0.03015996, -0.0145464, 0.030312411, 0.036410466, -0.053662878,
  -0.02520529, 0.010220593, -0.053713694, -0.008118034, 0.04266097, 0.025040135, -0.013987412,
  -0.0045671887, 0.031303346, 0.0149783455, -0.00085118675, -0.003534966, 0.0077559627,
  0.0058058556, 0.041695446, 0.03298031, -0.05854132, -0.0072414395, 0.0021501994, 0.007000058,
  -0.025345039, -0.014889415, -0.026729804, 0.00071223365, 0.037858754, 0.039789803, 0.019971127,
  0.002134319, 0.011154357, 0.033590116, -0.020962061, -0.033234395, 0.0026853671, -0.0022962985,
  -0.0037445864, -0.007343074, 0.009934747, 0.075920776, 0.018548248, 0.023795115, 0.026602762,
  0.002321707, -0.03458105, -0.011808628, -0.030617313, -0.0077369064, -0.03844315, 0.03773171,
  0.007908414, -0.011478317, -0.029804239, 0.028965758, 0.01172605, -0.027619103, 0.008956517,
  -0.051757235, 0.01092568, 0.038189065, 0.011332218, -0.0039256224, 0.051757235, -0.052341633,
  0.03371716, -0.03310735, 0.0054342556, -0.009890282, -0.019297801, 0.012132587, -0.03508922,
  0.046446845, -0.028559221, -0.005735982, 0.0058376165, -0.045608364, 0.045354277, 0.022308715,
  0.034860544, -0.07414217, 0.028406769, -0.026501127, 0.019488364, 0.0065681124, 0.022397645,
  -0.004598949, -0.052697353, -0.04022175, 0.027212568, 0.07144887, -0.002358232, -0.03369175,
  -0.05782988, 0.00039145062, -0.004344864, -0.0025329157, -0.0068285502, -0.005774095, 0.005262748,
  -0.019348618, 0.005138881, 0.022524688, -0.039993074, -0.04268638, -0.011643472, -0.009401167,
  0.011935671, -0.0051992266, -0.0007523316, 0.008378472, -0.03300572, 0.0026520183, 0.008276838,
  0.036105562, 0.035648208, 0.013085408, -0.012863083, -0.009718774, -0.021660797, -0.019310504,
  0.012024601, -0.02716175, -0.031963967, 0.0062886183, 0.0068793674, 0.011560895, -0.034784317,
  -0.023159903, 0.010398453, -0.03470809, -0.0495721, 0.008041809, 0.010157072, -0.03849397,
  0.0012307271, 0.031430386, -0.06906047, -0.018891264, 0.00039482518, -0.0347335, -0.010830399,
  -0.001905642, 0.045709997, -0.057270892, 0.012265982, -0.011929318, 0.016439337, -0.019018307,
  0.06133626, -0.053357974, 0.009248716, -0.013161634, 0.050334357, -0.008950165, -0.038951322,
  0.045227237, -0.008664318, 0.0029648612, -0.0465993, -0.056508634, -0.024646303, 0.06118381,
  -0.02324883, -0.0049324366, 0.023566438, -0.00035254375, 0.0045195473, -0.027288793, 0.0061520473,
  0.0014903708, -0.036080156, 0.033615522, -0.008365768, -0.004916556, -0.01967893, -0.025217995,
  0.040297974, 0.012043657, 0.054831672, -0.023795115, 0.029956691, -0.005726454, -0.028101867,
  0.015918462, 0.0003549258, -0.049343422, 0.031277936, 0.020415777, -0.021914883, -0.035292488,
  -0.019653521, 0.09822949, -0.013453832, -0.041746262, 0.0049324366, 0.0329549, 0.011408443,
  -0.0066888034, 0.018599065, 0.016960213, -0.03524167, 0.0155246295, 0.0045290757, -0.013695213,
  0.0045735408, -0.028660854, -0.015003754, -0.00409713, -0.0038589248, 0.0059487787, -0.024455737,
  0.023236128, 0.020796906, 0.029499337, -0.106614314, -0.040323384, 0.051350698, -0.015499221,
  0.011732402, 0.00828319, 0.04248311, 0.009007334, 0.015245136, 0.0077559627, 0.03737599,
  -0.030388637, -0.0069365366, -0.022791477, 0.033818793, -0.01297107, 0.0057455106, -0.021101808,
  0.0053294455, -0.01941214, -0.0066252816, 0.02329965, -0.027924007, 0.0090263905, 0.0012775741,
  0.0356228, 0.035470348, -0.02291852, 0.03582607, -0.013720622, 0.02838136, -0.0004776015,
  -0.04337241, -0.00709534, 0.018205233, 0.023757003, 0.007133453, -0.02478605, 0.0329549,
  -0.030744357, -0.019374026, -0.01861177, -0.0009401167, -0.005993244, 0.017646244, 0.012628054,
  -0.020212509, 0.011560895, -0.010589017, -0.029143617, 0.0078575965, -0.0030140902, -0.023617256,
  -0.047031242, -0.013631692, -0.00070786657, -0.0043226313, 0.060269102, 0.011148006, 0.007343074,
  0.004995958, 0.024011089, -0.030032918, -0.03209101, 0.049292605, -0.050918754, -0.024900388,
  0.022067335, -0.023960272, -0.008022753, 0.04121268, 0.00872784, 0.025916731, -0.0004232113,
  0.009960155, 0.041924123, 0.03402206, -0.024671711, 0.03844315, -0.00798464, 0.037299763,
  0.03470809, 0.0064378935, 0.02027603, 0.022931226, -0.054425135, -0.0053866147, 0.015918462,
  0.03333603, -0.0016626726, 0.028076459, -0.00825143, 0.01356817, 0.02241035, 0.0011965843,
  0.03371716, -0.06153953, 0.05224, 0.0076479763, -0.03102385, 0.013212451, 0.014673443,
  -0.02098747, 0.03262459, -0.007000058, 0.015143502, -0.02249928, 0.01347924, -0.015156206,
  0.011510077, 0.0024900388, -0.012996478, -0.02982965, 0.010169776, -0.037274357, -0.030007508,
  0.05427268, 0.04299128, 0.026882255, 0.0055835308, 0.00052921264, 0.032675408, 0.02065716,
  0.01825605, -0.057372525, 0.013974708, -0.02318531, -0.016324999, 0.017620835, 0.019348618,
  0.04090778, 0.004681527, 0.012030953, -0.011205175, 0.019793268, 0.013098112, 0.041898713,
  0.024214357, -0.031633656, 0.01525784, -0.008740544, -0.007317665, 0.009089912, -0.000455369,
  0.001769071, -0.00011155946, 0.006510943, -0.009413871, 0.002088266, -0.07647976, 0.07434545,
  0.07109315, -0.04494774, -0.011592655, 0.046700932, 0.07830918, -0.027720738, -0.06245424,
  -0.0054691923, 0.0075082295, 0.052341633, -0.031506613, -0.004084426, -0.029524745, 0.019717041,
  -0.002780649, -0.031100076, 0.021482937, -0.026145408, -0.0080164, -0.038112838, 0.04548132,
  -0.0015292777, -0.018408502, 0.06453774, 0.06321649, 0.04337241, 0.03668996, 0.025599124,
  0.0055390657, 0.0015705666, 0.030464863, -0.02089854, -0.0034269795, 0.05127447, 0.03371716,
  0.036893226, -0.016528267, 0.011859445, 0.0054088472, 0.0051230006, 0.016744241, 0.020174395,
  -0.02838136, -0.035190854, -0.002131143, 0.045887858, -0.038722645, -0.0028695792, 0.015651673,
  -0.035470348, -0.0020152163, -0.017379455, -0.056813538, 0.0033126408, 0.01894208, -0.049699143,
  0.055695564, 0.012596293, 0.05224, 0.0039287983, 0.03541953, -0.028355952, 0.014851303,
  -0.016261477, 0.00097346544, -0.019577295, -0.030109143, -0.0058947857, -0.00118388, -0.015130797,
  0.009890282, -0.030617313, 0.02051741, -0.027847782, -0.04319455, -0.020492002,
];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/ragEval/dataset.ts
================================================================================

import { and, desc, eq } from 'drizzle-orm/expressions';

import { NewEvalDatasetsItem, evalDatasets } from '@/database/schemas';
import { serverDB } from '@/database/server';
import { RAGEvalDataSetItem } from '@/types/eval';

export class EvalDatasetModel {
  private userId: string;

  constructor(userId: string) {
    this.userId = userId;
  }

  create = async (params: NewEvalDatasetsItem) => {
    const [result] = await serverDB
      .insert(evalDatasets)
      .values({ ...params, userId: this.userId })
      .returning();
    return result;
  };

  delete = async (id: number) => {
    return serverDB
      .delete(evalDatasets)
      .where(and(eq(evalDatasets.id, id), eq(evalDatasets.userId, this.userId)));
  };

  query = async (knowledgeBaseId: string): Promise<RAGEvalDataSetItem[]> => {
    return serverDB
      .select({
        createdAt: evalDatasets.createdAt,
        description: evalDatasets.description,
        id: evalDatasets.id,
        name: evalDatasets.name,
        updatedAt: evalDatasets.updatedAt,
      })
      .from(evalDatasets)
      .where(
        and(
          eq(evalDatasets.userId, this.userId),
          eq(evalDatasets.knowledgeBaseId, knowledgeBaseId),
        ),
      )
      .orderBy(desc(evalDatasets.createdAt));
  };

  findById = async (id: number) => {
    return serverDB.query.evalDatasets.findFirst({
      where: and(eq(evalDatasets.id, id), eq(evalDatasets.userId, this.userId)),
    });
  };

  update = async (id: number, value: Partial<NewEvalDatasetsItem>) => {
    return serverDB
      .update(evalDatasets)
      .set({ ...value, updatedAt: new Date() })
      .where(and(eq(evalDatasets.id, id), eq(evalDatasets.userId, this.userId)));
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/server/models/ragEval/evaluation.ts
================================================================================

import { SQL, count } from 'drizzle-orm';
import { and, desc, eq, inArray } from 'drizzle-orm/expressions';

import {
  NewEvalEvaluationItem,
  evalDatasets,
  evalEvaluation,
  evaluationRecords,
} from '@/database/schemas';
import { serverDB } from '@/database/server';
import { EvalEvaluationStatus, RAGEvalEvaluationItem } from '@/types/eval';

export class EvalEvaluationModel {
  private userId: string;

  constructor(userId: string) {
    this.userId = userId;
  }

  create = async (params: NewEvalEvaluationItem) => {
    const [result] = await serverDB
      .insert(evalEvaluation)
      .values({ ...params, userId: this.userId })
      .returning();
    return result;
  };

  delete = async (id: number) => {
    return serverDB
      .delete(evalEvaluation)
      .where(and(eq(evalEvaluation.id, id), eq(evalEvaluation.userId, this.userId)));
  };

  queryByKnowledgeBaseId = async (knowledgeBaseId: string) => {
    const evaluations = await serverDB
      .select({
        createdAt: evalEvaluation.createdAt,
        dataset: {
          id: evalDatasets.id,
          name: evalDatasets.name,
        },
        evalRecordsUrl: evalEvaluation.evalRecordsUrl,
        id: evalEvaluation.id,
        name: evalEvaluation.name,
        status: evalEvaluation.status,
        updatedAt: evalEvaluation.updatedAt,
      })
      .from(evalEvaluation)
      .leftJoin(evalDatasets, eq(evalDatasets.id, evalEvaluation.datasetId))
      .orderBy(desc(evalEvaluation.createdAt))
      .where(
        and(
          eq(evalEvaluation.userId, this.userId),
          eq(evalEvaluation.knowledgeBaseId, knowledgeBaseId),
        ),
      );

    // 然后查询每个评估的记录统计
    const evaluationIds = evaluations.map((evals) => evals.id);

    const recordStats = await serverDB
      .select({
        evaluationId: evaluationRecords.evaluationId,
        success: count(evaluationRecords.status).if(
          eq(evaluationRecords.status, EvalEvaluationStatus.Success),
        ) as SQL<number>,
        total: count(),
      })
      .from(evaluationRecords)
      .where(inArray(evaluationRecords.evaluationId, evaluationIds))
      .groupBy(evaluationRecords.evaluationId);

    return evaluations.map((evaluation) => {
      const stats = recordStats.find((stat) => stat.evaluationId === evaluation.id);

      return {
        ...evaluation,
        recordsStats: stats
          ? { success: Number(stats.success), total: Number(stats.total) }
          : { success: 0, total: 0 },
      } as RAGEvalEvaluationItem;
    });
  };

  findById = async (id: number) => {
    return serverDB.query.evalEvaluation.findFirst({
      where: and(eq(evalEvaluation.id, id), eq(evalEvaluation.userId, this.userId)),
    });
  };

  update = async (id: number, value: Partial<NewEvalEvaluationItem>) => {
    return serverDB
      .update(evalEvaluation)
      .set(value)
      .where(and(eq(evalEvaluation.id, id), eq(evalEvaluation.userId, this.userId)));
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/rag.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import { integer, jsonb, pgTable, text, uuid, varchar, vector } from 'drizzle-orm/pg-core';

import { timestamps } from './_helpers';
import { files } from './file';
import { users } from './user';

export const chunks = pgTable('chunks', {
  id: uuid('id').defaultRandom().primaryKey(),
  text: text('text'),
  abstract: text('abstract'),
  metadata: jsonb('metadata'),
  index: integer('index'),
  type: varchar('type'),

  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),

  ...timestamps,
});

export type NewChunkItem = typeof chunks.$inferInsert & { fileId?: string };

export const unstructuredChunks = pgTable('unstructured_chunks', {
  id: uuid('id').defaultRandom().primaryKey(),
  text: text('text'),
  metadata: jsonb('metadata'),
  index: integer('index'),
  type: varchar('type'),

  ...timestamps,

  parentId: varchar('parent_id'),
  compositeId: uuid('composite_id').references(() => chunks.id, { onDelete: 'cascade' }),
  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),
  fileId: varchar('file_id').references(() => files.id, { onDelete: 'cascade' }),
});

export type NewUnstructuredChunkItem = typeof unstructuredChunks.$inferInsert;

export const embeddings = pgTable('embeddings', {
  id: uuid('id').defaultRandom().primaryKey(),
  chunkId: uuid('chunk_id')
    .references(() => chunks.id, { onDelete: 'cascade' })
    .unique(),
  embeddings: vector('embeddings', { dimensions: 1024 }),
  model: text('model'),
  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),
});

export type NewEmbeddingsItem = typeof embeddings.$inferInsert;
export type EmbeddingsSelectItem = typeof embeddings.$inferSelect;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/file.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import {
  boolean,
  integer,
  jsonb,
  pgTable,
  primaryKey,
  text,
  uuid,
  varchar,
} from 'drizzle-orm/pg-core';
import { createInsertSchema } from 'drizzle-zod';

import { idGenerator } from '@/database/utils/idGenerator';

import { accessedAt, createdAt, timestamps } from './_helpers';
import { asyncTasks } from './asyncTask';
import { users } from './user';

export const globalFiles = pgTable('global_files', {
  hashId: varchar('hash_id', { length: 64 }).primaryKey(),
  fileType: varchar('file_type', { length: 255 }).notNull(),
  size: integer('size').notNull(),
  url: text('url').notNull(),
  metadata: jsonb('metadata'),

  createdAt: createdAt(),
  accessedAt: accessedAt(),
});

export type NewGlobalFile = typeof globalFiles.$inferInsert;
export type GlobalFileItem = typeof globalFiles.$inferSelect;

export const files = pgTable('files', {
  id: text('id')
    .$defaultFn(() => idGenerator('files'))
    .primaryKey(),

  userId: text('user_id')
    .references(() => users.id, { onDelete: 'cascade' })
    .notNull(),
  fileType: varchar('file_type', { length: 255 }).notNull(),
  fileHash: varchar('file_hash', { length: 64 }).references(() => globalFiles.hashId, {
    onDelete: 'no action',
  }),
  name: text('name').notNull(),
  size: integer('size').notNull(),
  url: text('url').notNull(),

  metadata: jsonb('metadata'),
  chunkTaskId: uuid('chunk_task_id').references(() => asyncTasks.id, { onDelete: 'set null' }),
  embeddingTaskId: uuid('embedding_task_id').references(() => asyncTasks.id, {
    onDelete: 'set null',
  }),

  ...timestamps,
});

export type NewFile = typeof files.$inferInsert;
export type FileItem = typeof files.$inferSelect;

export const knowledgeBases = pgTable('knowledge_bases', {
  id: text('id')
    .$defaultFn(() => idGenerator('knowledgeBases'))
    .primaryKey(),

  name: text('name').notNull(),
  description: text('description'),
  avatar: text('avatar'),

  // different types of knowledge bases need to be distinguished
  type: text('type'),
  userId: text('user_id')
    .references(() => users.id, { onDelete: 'cascade' })
    .notNull(),

  isPublic: boolean('is_public').default(false),

  settings: jsonb('settings'),

  ...timestamps,
});

export const insertKnowledgeBasesSchema = createInsertSchema(knowledgeBases);

export type NewKnowledgeBase = typeof knowledgeBases.$inferInsert;
export type KnowledgeBaseItem = typeof knowledgeBases.$inferSelect;

export const knowledgeBaseFiles = pgTable(
  'knowledge_base_files',
  {
    knowledgeBaseId: text('knowledge_base_id')
      .references(() => knowledgeBases.id, { onDelete: 'cascade' })
      .notNull(),

    fileId: text('file_id')
      .references(() => files.id, { onDelete: 'cascade' })
      .notNull(),

    // userId: text('user_id')
    //   .references(() => users.id, { onDelete: 'cascade' })
    //   .notNull(),

    createdAt: createdAt(),
  },
  (t) => ({
    pk: primaryKey({
      columns: [
        t.knowledgeBaseId,
        t.fileId,
        // t.userId
      ],
    }),
  }),
);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/ragEvals.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import { integer, jsonb, pgTable, text, uuid } from 'drizzle-orm/pg-core';

import { DEFAULT_EMBEDDING_MODEL, DEFAULT_EMBEDDING_PROVIDER, DEFAULT_SMALL_MODEL, DEFAULT_SMALL_PROVIDER } from '@/const/settings';
import { EvalEvaluationStatus } from '@/types/eval';

import { timestamps } from './_helpers';
import { knowledgeBases } from './file';
import { embeddings } from './rag';
import { users } from './user';

export const evalDatasets = pgTable('rag_eval_datasets', {
  id: integer('id').generatedAlwaysAsIdentity({ startWith: 30_000 }).primaryKey(),

  description: text('description'),
  name: text('name').notNull(),

  knowledgeBaseId: text('knowledge_base_id').references(() => knowledgeBases.id, {
    onDelete: 'cascade',
  }),
  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),

  ...timestamps,
});

export type NewEvalDatasetsItem = typeof evalDatasets.$inferInsert;
export type EvalDatasetsSelectItem = typeof evalDatasets.$inferSelect;

export const evalDatasetRecords = pgTable('rag_eval_dataset_records', {
  id: integer('id').generatedAlwaysAsIdentity().primaryKey(),
  datasetId: integer('dataset_id')
    .references(() => evalDatasets.id, { onDelete: 'cascade' })
    .notNull(),

  ideal: text('ideal'),
  question: text('question'),
  referenceFiles: text('reference_files').array(),
  metadata: jsonb('metadata'),

  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),
  ...timestamps,
});

export type NewEvalDatasetRecordsItem = typeof evalDatasetRecords.$inferInsert;
export type EvalDatasetRecordsSelectItem = typeof evalDatasetRecords.$inferSelect;

export const evalEvaluation = pgTable('rag_eval_evaluations', {
  id: integer('id').generatedAlwaysAsIdentity().primaryKey(),
  name: text('name').notNull(),
  description: text('description'),

  evalRecordsUrl: text('eval_records_url'),
  status: text('status').$defaultFn(() => EvalEvaluationStatus.Pending),
  error: jsonb('error'),

  datasetId: integer('dataset_id')
    .references(() => evalDatasets.id, { onDelete: 'cascade' })
    .notNull(),
  knowledgeBaseId: text('knowledge_base_id').references(() => knowledgeBases.id, {
    onDelete: 'cascade',
  }),
  languageModel: text('language_model').$defaultFn(() => DEFAULT_SMALL_MODEL),
  languageProvider: text('language_model').$defaultFn(() => DEFAULT_SMALL_PROVIDER),
  embeddingModel: text('embedding_model').$defaultFn(() => DEFAULT_EMBEDDING_MODEL),
  embeddingProvider: text('embedding_model').$defaultFn(() => DEFAULT_EMBEDDING_PROVIDER),
  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),
  ...timestamps,
});

export type NewEvalEvaluationItem = typeof evalEvaluation.$inferInsert;
export type EvalEvaluationSelectItem = typeof evalEvaluation.$inferSelect;

export const evaluationRecords = pgTable('rag_eval_evaluation_records', {
  id: integer('id').generatedAlwaysAsIdentity().primaryKey(),

  question: text('question').notNull(),
  answer: text('answer'),
  context: text('context').array(),
  ideal: text('ideal'),

  status: text('status').$defaultFn(() => EvalEvaluationStatus.Pending),
  error: jsonb('error'),

  languageModel: text('language_model'),
  embeddingModel: text('embedding_model'),

  questionEmbeddingId: uuid('question_embedding_id').references(() => embeddings.id, {
    onDelete: 'set null',
  }),

  duration: integer('duration'),
  datasetRecordId: integer('dataset_record_id')
    .references(() => evalDatasetRecords.id, { onDelete: 'cascade' })
    .notNull(),
  evaluationId: integer('evaluation_id')
    .references(() => evalEvaluation.id, { onDelete: 'cascade' })
    .notNull(),

  userId: text('user_id').references(() => users.id, { onDelete: 'cascade' }),
  ...timestamps,
});

export type NewEvaluationRecordsItem = typeof evaluationRecords.$inferInsert;
export type EvaluationRecordsSelectItem = typeof evaluationRecords.$inferSelect;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/message.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import {
  boolean,
  index,
  jsonb,
  numeric,
  pgTable,
  primaryKey,
  text,
  uniqueIndex,
  uuid,
} from 'drizzle-orm/pg-core';
import { createSelectSchema } from 'drizzle-zod';

import { idGenerator } from '@/database/utils/idGenerator';

import { timestamps } from './_helpers';
import { agents } from './agent';
import { files } from './file';
import { chunks, embeddings } from './rag';
import { sessions } from './session';
import { threads, topics } from './topic';
import { users } from './user';

// @ts-ignore
export const messages = pgTable(
  'messages',
  {
    id: text('id')
      .$defaultFn(() => idGenerator('messages'))
      .primaryKey(),

    role: text('role', { enum: ['user', 'system', 'assistant', 'tool'] }).notNull(),
    content: text('content'),

    model: text('model'),
    provider: text('provider'),

    favorite: boolean('favorite').default(false),
    error: jsonb('error'),

    tools: jsonb('tools'),

    traceId: text('trace_id'),
    observationId: text('observation_id'),

    clientId: text('client_id'),

    // foreign keys
    userId: text('user_id')
      .references(() => users.id, { onDelete: 'cascade' })
      .notNull(),
    sessionId: text('session_id').references(() => sessions.id, { onDelete: 'cascade' }),
    topicId: text('topic_id').references(() => topics.id, { onDelete: 'cascade' }),
    threadId: text('thread_id').references(() => threads.id, { onDelete: 'cascade' }),
    // @ts-ignore
    parentId: text('parent_id').references(() => messages.id, { onDelete: 'set null' }),
    quotaId: text('quota_id').references(() => messages.id, { onDelete: 'set null' }),

    // used for group chat
    agentId: text('agent_id').references(() => agents.id, { onDelete: 'set null' }),

    ...timestamps,
  },
  (table) => ({
    createdAtIdx: index('messages_created_at_idx').on(table.createdAt),
    messageClientIdUnique: uniqueIndex('message_client_id_user_unique').on(
      table.clientId,
      table.userId,
    ),
  }),
);

export type NewMessage = typeof messages.$inferInsert;
export type MessageItem = typeof messages.$inferSelect;

// if the message container a plugin
export const messagePlugins = pgTable('message_plugins', {
  id: text('id')
    .references(() => messages.id, { onDelete: 'cascade' })
    .primaryKey(),

  toolCallId: text('tool_call_id'),
  type: text('type', {
    enum: ['default', 'markdown', 'standalone', 'builtin'],
  }).default('default'),

  apiName: text('api_name'),
  arguments: text('arguments'),
  identifier: text('identifier'),
  state: jsonb('state'),
  error: jsonb('error'),
});

export type MessagePluginItem = typeof messagePlugins.$inferSelect;
export const updateMessagePluginSchema = createSelectSchema(messagePlugins);

export const messageTTS = pgTable('message_tts', {
  id: text('id')
    .references(() => messages.id, { onDelete: 'cascade' })
    .primaryKey(),
  contentMd5: text('content_md5'),
  fileId: text('file_id').references(() => files.id, { onDelete: 'cascade' }),
  voice: text('voice'),
});

export const messageTranslates = pgTable('message_translates', {
  id: text('id')
    .references(() => messages.id, { onDelete: 'cascade' })
    .primaryKey(),
  content: text('content'),
  from: text('from'),
  to: text('to'),
});

// if the message contains a file
// save the file id and message id
export const messagesFiles = pgTable(
  'messages_files',
  {
    fileId: text('file_id')
      .notNull()
      .references(() => files.id, { onDelete: 'cascade' }),
    messageId: text('message_id')
      .notNull()
      .references(() => messages.id, { onDelete: 'cascade' }),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.fileId, t.messageId] }),
  }),
);

export const messageQueries = pgTable('message_queries', {
  id: uuid('id').defaultRandom().primaryKey(),
  messageId: text('message_id')
    .references(() => messages.id, { onDelete: 'cascade' })
    .notNull(),
  rewriteQuery: text('rewrite_query'),
  userQuery: text('user_query'),
  embeddingsId: uuid('embeddings_id').references(() => embeddings.id, { onDelete: 'set null' }),
});

export type NewMessageQuery = typeof messageQueries.$inferInsert;

export const messageQueryChunks = pgTable(
  'message_query_chunks',
  {
    messageId: text('id').references(() => messages.id, { onDelete: 'cascade' }),
    queryId: uuid('query_id').references(() => messageQueries.id, { onDelete: 'cascade' }),
    chunkId: uuid('chunk_id').references(() => chunks.id, { onDelete: 'cascade' }),
    similarity: numeric('similarity', { precision: 6, scale: 5 }),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.chunkId, t.messageId, t.queryId] }),
  }),
);
export type NewMessageFileChunk = typeof messageQueryChunks.$inferInsert;

// convert message content to the chunks
// then we can use message as the RAG source
export const messageChunks = pgTable(
  'message_chunks',
  {
    messageId: text('message_id').references(() => messages.id, { onDelete: 'cascade' }),
    chunkId: uuid('chunk_id').references(() => chunks.id, { onDelete: 'cascade' }),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.chunkId, t.messageId] }),
  }),
);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/relations.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import { relations } from 'drizzle-orm';
import { pgTable, primaryKey, text, uuid, varchar } from 'drizzle-orm/pg-core';

import { createdAt } from '@/database/schemas/_helpers';

import { agents, agentsFiles, agentsKnowledgeBases } from './agent';
import { asyncTasks } from './asyncTask';
import { files, knowledgeBases } from './file';
import { messages, messagesFiles } from './message';
import { chunks, unstructuredChunks } from './rag';
import { sessionGroups, sessions } from './session';
import { threads, topics } from './topic';

export const agentsToSessions = pgTable(
  'agents_to_sessions',
  {
    agentId: text('agent_id')
      .notNull()
      .references(() => agents.id, { onDelete: 'cascade' }),
    sessionId: text('session_id')
      .notNull()
      .references(() => sessions.id, { onDelete: 'cascade' }),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.agentId, t.sessionId] }),
  }),
);

export const filesToSessions = pgTable(
  'files_to_sessions',
  {
    fileId: text('file_id')
      .notNull()
      .references(() => files.id, { onDelete: 'cascade' }),
    sessionId: text('session_id')
      .notNull()
      .references(() => sessions.id, { onDelete: 'cascade' }),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.fileId, t.sessionId] }),
  }),
);

export const fileChunks = pgTable(
  'file_chunks',
  {
    fileId: varchar('file_id').references(() => files.id, { onDelete: 'cascade' }),
    chunkId: uuid('chunk_id').references(() => chunks.id, { onDelete: 'cascade' }),
    createdAt: createdAt(),
  },
  (t) => ({
    pk: primaryKey({ columns: [t.fileId, t.chunkId] }),
  }),
);
export type NewFileChunkItem = typeof fileChunks.$inferInsert;

export const topicRelations = relations(topics, ({ one }) => ({
  session: one(sessions, {
    fields: [topics.sessionId],
    references: [sessions.id],
  }),
}));

export const threadsRelations = relations(threads, ({ one }) => ({
  sourceMessage: one(messages, {
    fields: [threads.sourceMessageId],
    references: [messages.id],
  }),
}));

export const messagesRelations = relations(messages, ({ many, one }) => ({
  filesToMessages: many(messagesFiles),

  session: one(sessions, {
    fields: [messages.sessionId],
    references: [sessions.id],
  }),

  parent: one(messages, {
    fields: [messages.parentId],
    references: [messages.id],
  }),

  topic: one(topics, {
    fields: [messages.topicId],
    references: [topics.id],
  }),

  thread: one(threads, {
    fields: [messages.threadId],
    references: [threads.id],
  }),
}));

export const agentsRelations = relations(agents, ({ many }) => ({
  agentsToSessions: many(agentsToSessions),
  knowledgeBases: many(agentsKnowledgeBases),
  files: many(agentsFiles),
}));

export const agentsToSessionsRelations = relations(agentsToSessions, ({ one }) => ({
  session: one(sessions, {
    fields: [agentsToSessions.sessionId],
    references: [sessions.id],
  }),
  agent: one(agents, {
    fields: [agentsToSessions.agentId],
    references: [agents.id],
  }),
}));

export const agentsKnowledgeBasesRelations = relations(agentsKnowledgeBases, ({ one }) => ({
  knowledgeBase: one(knowledgeBases, {
    fields: [agentsKnowledgeBases.knowledgeBaseId],
    references: [knowledgeBases.id],
  }),
  agent: one(agents, {
    fields: [agentsKnowledgeBases.agentId],
    references: [agents.id],
  }),
}));

export const sessionsRelations = relations(sessions, ({ many, one }) => ({
  filesToSessions: many(filesToSessions),
  agentsToSessions: many(agentsToSessions),
  group: one(sessionGroups, {
    fields: [sessions.groupId],
    references: [sessionGroups.id],
  }),
}));

export const chunksRelations = relations(unstructuredChunks, ({ one }) => ({
  file: one(files, {
    fields: [unstructuredChunks.fileId],
    references: [files.id],
  }),
}));

export const filesRelations = relations(files, ({ many, one }) => ({
  messages: many(messagesFiles),
  sessions: many(filesToSessions),
  agents: many(agentsFiles),

  chunkingTask: one(asyncTasks, {
    fields: [files.chunkTaskId],
    references: [asyncTasks.id],
  }),
  embeddingTask: one(asyncTasks, {
    fields: [files.embeddingTaskId],
    references: [asyncTasks.id],
  }),
}));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/index.ts
================================================================================

export * from './agent';
export * from './asyncTask';
export * from './file';
export * from './message';
export * from './nextauth';
export * from './rag';
export * from './ragEvals';
export * from './relations';
export * from './session';
export * from './topic';
export * from './user';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/schemas/user.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import { LobeChatPluginManifest } from '@lobehub/chat-plugin-sdk';
import { boolean, jsonb, pgTable, primaryKey, text } from 'drizzle-orm/pg-core';

import { DEFAULT_PREFERENCE } from '@/const/user';
import { CustomPluginParams } from '@/types/tool/plugin';

import { timestamps, timestamptz } from './_helpers';

export const users = pgTable('users', {
  id: text('id').primaryKey().notNull(),
  username: text('username').unique(),
  email: text('email'),

  avatar: text('avatar'),
  phone: text('phone'),
  firstName: text('first_name'),
  lastName: text('last_name'),
  fullName: text('full_name'),

  isOnboarded: boolean('is_onboarded').default(false),
  // Time user was created in Clerk
  clerkCreatedAt: timestamptz('clerk_created_at'),

  // Required by nextauth, all null allowed
  emailVerifiedAt: timestamptz('email_verified_at'),

  preference: jsonb('preference').$defaultFn(() => DEFAULT_PREFERENCE),

  ...timestamps,
});

export type NewUser = typeof users.$inferInsert;
export type UserItem = typeof users.$inferSelect;

export const userSettings = pgTable('user_settings', {
  id: text('id')
    .references(() => users.id, { onDelete: 'cascade' })
    .primaryKey(),

  tts: jsonb('tts'),
  keyVaults: text('key_vaults'),
  general: jsonb('general'),
  languageModel: jsonb('language_model'),
  systemAgent: jsonb('system_agent'),
  defaultAgent: jsonb('default_agent'),
  tool: jsonb('tool'),
});
export type UserSettingsItem = typeof userSettings.$inferSelect;

export const installedPlugins = pgTable(
  'user_installed_plugins',
  {
    userId: text('user_id')
      .references(() => users.id, { onDelete: 'cascade' })
      .notNull(),

    identifier: text('identifier').notNull(),
    type: text('type', { enum: ['plugin', 'customPlugin'] }).notNull(),
    manifest: jsonb('manifest').$type<LobeChatPluginManifest>(),
    settings: jsonb('settings'),
    customParams: jsonb('custom_params').$type<CustomPluginParams>(),

    ...timestamps,
  },
  (self) => ({
    id: primaryKey({ columns: [self.userId, self.identifier] }),
  }),
);

export type NewInstalledPlugin = typeof installedPlugins.$inferInsert;
export type InstalledPluginItem = typeof installedPlugins.$inferSelect;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/client/db.ts
================================================================================

import type { PgliteDatabase } from 'drizzle-orm/pglite';
import { Md5 } from 'ts-md5';

import { ClientDBLoadingProgress, DatabaseLoadingState } from '@/types/clientDB';
import { sleep } from '@/utils/sleep';

import * as schema from '../schemas';
import migrations from './migrations.json';

const pgliteSchemaHashCache = 'LOBE_CHAT_PGLITE_SCHEMA_HASH';

type DrizzleInstance = PgliteDatabase<typeof schema>;

export interface DatabaseLoadingCallbacks {
  onError?: (error: Error) => void;
  onProgress?: (progress: ClientDBLoadingProgress) => void;
  onStateChange?: (state: DatabaseLoadingState) => void;
}

export class DatabaseManager {
  private static instance: DatabaseManager;
  private dbInstance: DrizzleInstance | null = null;
  private initPromise: Promise<DrizzleInstance> | null = null;
  private callbacks?: DatabaseLoadingCallbacks;
  private isLocalDBSchemaSynced = false;

  // CDN 配置
  private static WASM_CDN_URL =
    'https://registry.npmmirror.com/@electric-sql/pglite/0.2.13/files/dist/postgres.wasm';

  private constructor() {}

  static getInstance() {
    if (!DatabaseManager.instance) {
      DatabaseManager.instance = new DatabaseManager();
    }
    return DatabaseManager.instance;
  }

  // 加载并编译 WASM 模块
  private async loadWasmModule(): Promise<WebAssembly.Module> {
    const start = Date.now();
    this.callbacks?.onStateChange?.(DatabaseLoadingState.LoadingWasm);

    const response = await fetch(DatabaseManager.WASM_CDN_URL);

    const contentLength = Number(response.headers.get('Content-Length')) || 0;
    const reader = response.body?.getReader();

    if (!reader) throw new Error('Failed to start WASM download');

    let receivedLength = 0;
    const chunks: Uint8Array[] = [];

    // 读取数据流
    // eslint-disable-next-line no-constant-condition
    while (true) {
      const { done, value } = await reader.read();

      if (done) break;

      chunks.push(value);
      receivedLength += value.length;

      // 计算并报告进度
      const progress = Math.min(Math.round((receivedLength / contentLength) * 100), 100);
      this.callbacks?.onProgress?.({
        phase: 'wasm',
        progress,
      });
    }

    // 合并数据块
    const wasmBytes = new Uint8Array(receivedLength);
    let position = 0;
    for (const chunk of chunks) {
      wasmBytes.set(chunk, position);
      position += chunk.length;
    }

    this.callbacks?.onProgress?.({
      costTime: Date.now() - start,
      phase: 'wasm',
      progress: 100,
    });

    // 编译 WASM 模块
    return WebAssembly.compile(wasmBytes);
  }

  // 异步加载 PGlite 相关依赖
  private async loadDependencies() {
    const start = Date.now();
    this.callbacks?.onStateChange?.(DatabaseLoadingState.LoadingDependencies);

    const imports = [
      import('@electric-sql/pglite').then((m) => ({
        IdbFs: m.IdbFs,
        MemoryFS: m.MemoryFS,
        PGlite: m.PGlite,
      })),
      import('@electric-sql/pglite/vector'),
      import('drizzle-orm/pglite'),
    ];

    let loaded = 0;
    const results = await Promise.all(
      imports.map(async (importPromise) => {
        const result = await importPromise;
        loaded += 1;

        // 计算加载进度
        this.callbacks?.onProgress?.({
          phase: 'dependencies',
          progress: Math.min(Math.round((loaded / imports.length) * 100), 100),
        });
        return result;
      }),
    );

    this.callbacks?.onProgress?.({
      costTime: Date.now() - start,
      phase: 'dependencies',
      progress: 100,
    });

    // @ts-ignore
    const [{ PGlite, IdbFs, MemoryFS }, { vector }, { drizzle }] = results;

    return { IdbFs, MemoryFS, PGlite, drizzle, vector };
  }

  // 数据库迁移方法
  private async migrate(skipMultiRun = false): Promise<DrizzleInstance> {
    if (this.isLocalDBSchemaSynced && skipMultiRun) return this.db;

    const cacheHash = localStorage.getItem(pgliteSchemaHashCache);
    const hash = Md5.hashStr(JSON.stringify(migrations));

    // if hash is the same, no need to migrate
    if (hash === cacheHash) {
      this.isLocalDBSchemaSynced = true;
      return this.db;
    }

    const start = Date.now();
    try {
      this.callbacks?.onStateChange?.(DatabaseLoadingState.Migrating);

      // refs: https://github.com/drizzle-team/drizzle-orm/discussions/2532
      // @ts-expect-error
      await this.db.dialect.migrate(migrations, this.db.session, {});
      localStorage.setItem(pgliteSchemaHashCache, hash);
      this.isLocalDBSchemaSynced = true;

      console.info(`🗂 Migration success, take ${Date.now() - start}ms`);
    } catch (cause) {
      console.error('❌ Local database schema migration failed', cause);
      throw cause;
    }

    return this.db;
  }

  // 初始化数据库
  async initialize(callbacks?: DatabaseLoadingCallbacks): Promise<DrizzleInstance> {
    if (this.initPromise) return this.initPromise;

    this.callbacks = callbacks;

    this.initPromise = (async () => {
      try {
        if (this.dbInstance) return this.dbInstance;

        const time = Date.now();
        // 初始化数据库
        this.callbacks?.onStateChange?.(DatabaseLoadingState.Initializing);

        // 加载依赖
        const { PGlite, vector, drizzle, IdbFs, MemoryFS } = await this.loadDependencies();

        // 加载并编译 WASM 模块
        const wasmModule = await this.loadWasmModule();

        const db = new PGlite({
          extensions: { vector },
          fs: typeof window === 'undefined' ? new MemoryFS('lobechat') : new IdbFs('lobechat'),
          relaxedDurability: true,
          wasmModule,
        });

        this.dbInstance = drizzle({ client: db, schema });

        await this.migrate(true);

        this.callbacks?.onStateChange?.(DatabaseLoadingState.Finished);
        console.log(`✅ Database initialized in ${Date.now() - time}ms`);

        await sleep(50);

        this.callbacks?.onStateChange?.(DatabaseLoadingState.Ready);

        return this.dbInstance as DrizzleInstance;
      } catch (e) {
        this.initPromise = null;
        this.callbacks?.onStateChange?.(DatabaseLoadingState.Error);
        const error = e as Error;
        this.callbacks?.onError?.({
          message: error.message,
          name: error.name,
          stack: error.stack,
        });
        throw error;
      }
    })();

    return this.initPromise;
  }

  // 获取数据库实例
  get db(): DrizzleInstance {
    if (!this.dbInstance) {
      throw new Error('Database not initialized. Please call initialize() first.');
    }
    return this.dbInstance;
  }

  // 创建代理对象
  createProxy(): DrizzleInstance {
    return new Proxy({} as DrizzleInstance, {
      get: (target, prop) => {
        return this.db[prop as keyof DrizzleInstance];
      },
    });
  }
}

// 导出单例
const dbManager = DatabaseManager.getInstance();

// 保持原有的 clientDB 导出不变
export const clientDB = dbManager.createProxy();

// 导出初始化方法，供应用启动时使用
export const initializeDB = (callbacks?: DatabaseLoadingCallbacks) =>
  dbManager.initialize(callbacks);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/core/db.ts
================================================================================

import Dexie, { Transaction } from 'dexie';

import { MigrationLLMSettings } from '@/migrations/FromV3ToV4';
import { MigrationAgentChatConfig } from '@/migrations/FromV5ToV6';
import { MigrationKeyValueSettings } from '@/migrations/FromV6ToV7';
import { uuid } from '@/utils/uuid';

import { DB_File } from '../schemas/files';
import { DB_Message } from '../schemas/message';
import { DB_Plugin } from '../schemas/plugin';
import { DB_Session } from '../schemas/session';
import { DB_SessionGroup } from '../schemas/sessionGroup';
import { DB_Topic } from '../schemas/topic';
import { DB_User } from '../schemas/user';
import { migrateSettingsToUser } from './migrations/migrateSettingsToUser';
import {
  dbSchemaV1,
  dbSchemaV2,
  dbSchemaV3,
  dbSchemaV4,
  dbSchemaV5,
  dbSchemaV6,
  dbSchemaV7,
  dbSchemaV9,
} from './schemas';
import { DBModel, LOBE_CHAT_LOCAL_DB_NAME } from './types/db';

export interface LobeDBSchemaMap {
  files: DB_File;
  messages: DB_Message;
  plugins: DB_Plugin;
  sessionGroups: DB_SessionGroup;
  sessions: DB_Session;
  topics: DB_Topic;
  users: DB_User;
}

// Define a local DB
export class BrowserDB extends Dexie {
  public files: BrowserDBTable<'files'>;
  public sessions: BrowserDBTable<'sessions'>;
  public messages: BrowserDBTable<'messages'>;
  public topics: BrowserDBTable<'topics'>;
  public plugins: BrowserDBTable<'plugins'>;
  public sessionGroups: BrowserDBTable<'sessionGroups'>;
  public users: BrowserDBTable<'users'>;

  constructor() {
    super(LOBE_CHAT_LOCAL_DB_NAME);
    this.version(1).stores(dbSchemaV1);
    this.version(2).stores(dbSchemaV2);
    this.version(3).stores(dbSchemaV3);
    this.version(4)
      .stores(dbSchemaV4)
      .upgrade((trans) => this.upgradeToV4(trans));

    this.version(5)
      .stores(dbSchemaV5)
      .upgrade((trans) => this.upgradeToV5(trans));

    this.version(6)
      .stores(dbSchemaV6)
      .upgrade((trans) => this.upgradeToV6(trans));

    this.version(7)
      .stores(dbSchemaV7)
      .upgrade((trans) => this.upgradeToV7(trans));

    this.version(8)
      .stores(dbSchemaV7)
      .upgrade((trans) => this.upgradeToV8(trans));

    this.version(9)
      .stores(dbSchemaV9)
      .upgrade((trans) => this.upgradeToV9(trans));

    this.version(10)
      .stores(dbSchemaV9)
      .upgrade((trans) => this.upgradeToV10(trans));

    this.version(11)
      .stores(dbSchemaV9)
      .upgrade((trans) => this.upgradeToV11(trans));

    this.files = this.table('files');
    this.sessions = this.table('sessions');
    this.messages = this.table('messages');
    this.topics = this.table('topics');
    this.plugins = this.table('plugins');
    this.sessionGroups = this.table('sessionGroups');
    this.users = this.table('users');
  }

  /**
   * 2024.01.22
   *
   * DB V3 to V4
   * from `group = pinned` to `pinned:true`
   */
  upgradeToV4 = async (trans: Transaction) => {
    const sessions = trans.table('sessions');
    await sessions.toCollection().modify((session) => {
      // translate boolean to number
      session.pinned = session.group === 'pinned' ? 1 : 0;
      session.group = 'default';
    });
  };

  /**
   * 2024.01.29
   * settings from localStorage to indexedDB
   */
  upgradeToV5 = async (trans: Transaction) => {
    const users = trans.table('users');

    // if no user, create one
    if ((await users.count()) === 0) {
      const data = localStorage.getItem('LOBE_SETTINGS');

      if (data) {
        let json;

        try {
          json = JSON.parse(data);
        } catch {
          /* empty */
        }

        if (!json?.state?.settings) return;

        const settings = json.state.settings;

        const user = migrateSettingsToUser(settings);
        await users.add(user);
      }
    }
  };

  /**
   * 2024.02.27
   * add uuid to user
   */
  upgradeToV6 = async (trans: Transaction) => {
    const users = trans.table('users');

    await users.toCollection().modify((user: DB_User) => {
      if (!user.uuid) user.uuid = uuid();
    });
  };

  /**
   * 2024.03.14
   * add `id` in plugins
   */
  upgradeToV7 = async (trans: Transaction) => {
    const plugins = trans.table('plugins');

    await plugins.toCollection().modify((plugin: DB_Plugin) => {
      plugin.id = plugin.identifier;
    });
  };

  upgradeToV8 = async (trans: Transaction) => {
    const users = trans.table('users');
    await users.toCollection().modify((user: DB_User) => {
      if (user.settings) {
        user.settings = MigrationLLMSettings.migrateSettings(user.settings as any);
      }
    });
  };

  /**
   * 2024.05.11
   *
   * message role=function to role=tool
   */
  upgradeToV9 = async (trans: Transaction) => {
    const messages = trans.table('messages');
    await messages.toCollection().modify(async (message: DBModel<DB_Message>) => {
      if ((message.role as string) === 'function') {
        const origin = Object.assign({}, message);

        const toolCallId = `tool_call_${message.id}`;
        const assistantMessageId = `tool_calls_${message.id}`;

        message.role = 'tool';
        message.tool_call_id = toolCallId;
        message.parentId = assistantMessageId;

        await messages.add({
          ...origin,
          content: '',
          createdAt: message.createdAt - 10,
          error: undefined,
          id: assistantMessageId,
          role: 'assistant',
          tools: [{ ...message.plugin!, id: toolCallId }],
          updatedAt: message.updatedAt - 10,
        } as DBModel<DB_Message>);
      }
    });
  };

  /**
   * 2024.05.25
   * migrate some agent config to chatConfig
   */
  upgradeToV10 = async (trans: Transaction) => {
    const sessions = trans.table('sessions');
    await sessions.toCollection().modify(async (session: DBModel<DB_Session>) => {
      if (session.config)
        session.config = MigrationAgentChatConfig.migrateChatConfig(session.config as any);
    });
  };

  /**
   * 2024.05.27
   * migrate apiKey in languageModel to keyVaults
   */
  upgradeToV11 = async (trans: Transaction) => {
    const users = trans.table('users');

    await users.toCollection().modify((user: DB_User) => {
      if (user.settings) {
        user.settings = MigrationKeyValueSettings.migrateSettings(user.settings as any);
      }
    });
  };
}

export const browserDB = new BrowserDB();

// ================================================ //
// ================================================ //
// ================================================ //
// ================================================ //
// ================================================ //

// types helper
export type BrowserDBSchema = {
  [t in keyof LobeDBSchemaMap]: {
    model: LobeDBSchemaMap[t];
    table: Dexie.Table<DBModel<LobeDBSchemaMap[t]>, string>;
  };
};
type BrowserDBTable<T extends keyof LobeDBSchemaMap> = BrowserDBSchema[T]['table'];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/core/migrations/migrateSettingsToUser/type.ts
================================================================================

import type { ThemeMode } from 'antd-style';

import { LobeAgentTTSConfig } from '@/types/agent';
import { FewShots, LLMParams } from '@/types/llm';
import { MetaData } from '@/types/meta';
import { STTServer } from '@/types/user/settings';

interface V4LobeAgentConfig {
  autoCreateTopicThreshold: number;
  compressThreshold?: number;
  displayMode?: 'chat' | 'docs';
  enableAutoCreateTopic: boolean;
  /**
   * 历史消息长度压缩阈值
   */
  enableCompressThreshold?: boolean;
  /**
   * 开启历史记录条数
   */
  enableHistoryCount?: boolean;
  enableMaxTokens?: boolean;

  fewShots?: FewShots;
  /**
   * 历史消息条数
   */
  historyCount?: number;
  inputTemplate?: string;
  /**
   * 角色所使用的语言模型
   * @default gpt-3.5-turbo
   */
  model: string;
  /**
   * 语言模型参数
   */
  params: LLMParams;
  /**
   * 启用的插件
   */
  plugins?: string[];
  /**
   *  模型供应商
   */
  provider?: string;
  /**
   * 系统角色
   */
  systemRole: string;
  /**
   * 语音服务
   */
  tts: LobeAgentTTSConfig;
}

interface V4DefaultAgent {
  config: V4LobeAgentConfig;
  meta: MetaData;
}

interface OpenAIConfig {
  OPENAI_API_KEY: string;
  azureApiVersion?: string;
  customModelName?: string;
  endpoint?: string;
  models?: string[];
  useAzure?: boolean;
}

interface V4LLMConfig {
  openAI: OpenAIConfig;
}

interface TTSConfig {
  openAI: {
    sttModel: 'whisper-1';
    ttsModel: 'tts-1' | 'tts-1-hd';
  };
  sttAutoStop: boolean;
  sttServer: STTServer;
}

export interface V4Settings {
  avatar: string;
  defaultAgent: V4DefaultAgent;
  fontSize: number;
  language: string;
  languageModel: V4LLMConfig;
  neutralColor?: string;
  password: string;
  primaryColor?: string;
  themeMode: ThemeMode;
  tts: TTSConfig;
}

export interface V5Settings {
  defaultAgent: V4DefaultAgent;
  fontSize: number;
  language: string;
  languageModel: {
    openai: OpenAIConfig;
  };
  neutralColor?: string;
  password: string;
  primaryColor?: string;
  themeMode: ThemeMode;
  tts: TTSConfig;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/core/migrations/migrateSettingsToUser/index.ts
================================================================================

import { V4Settings, V5Settings } from './type';

export const migrateSettingsToUser = (
  settings: V4Settings,
): { avatar: string; settings: V5Settings } => {
  const dbSettings: V5Settings = {
    defaultAgent: settings.defaultAgent,
    fontSize: settings.fontSize,
    language: settings.language,
    languageModel: {
      openai: settings.languageModel.openAI,
    },
    password: settings.password,
    themeMode: settings.themeMode,
    tts: settings.tts,
  };

  return {
    avatar: settings.avatar,
    settings: dbSettings,
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/models/file.ts
================================================================================

import { DBModel } from '@/database/_deprecated/core/types/db';
import { DB_File, DB_FileSchema } from '@/database/_deprecated/schemas/files';
import { clientS3Storage } from '@/services/file/ClientS3';
import { nanoid } from '@/utils/uuid';

import { BaseModel } from '../core';

class _FileModel extends BaseModel<'files'> {
  constructor() {
    super('files', DB_FileSchema);
  }

  async create(file: DB_File) {
    const id = nanoid();

    return this._addWithSync(file, `file-${id}`);
  }

  async findById(id: string): Promise<DBModel<DB_File> | undefined> {
    const item = await this.table.get(id);
    if (!item) return;

    // arrayBuffer to url
    let base64;
    if (!item.data) {
      const hash = (item.url as string).replace('client-s3://', '');
      base64 = await this.getBase64ByFileHash(hash);
    } else {
      base64 = Buffer.from(item.data).toString('base64');
    }

    return { ...item, base64, url: `data:${item.fileType};base64,${base64}` };
  }

  async delete(id: string) {
    return this.table.delete(id);
  }

  async clear() {
    return this.table.clear();
  }

  private async getBase64ByFileHash(hash: string) {
    const fileItem = await clientS3Storage.getObject(hash);
    if (!fileItem) throw new Error('file not found');

    return Buffer.from(await fileItem.arrayBuffer()).toString('base64');
  }
}

export const FileModel = new _FileModel();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/schemas/session.ts
================================================================================

import { z } from 'zod';

import { DEFAULT_SMALL_MODEL, DEFAULT_SMALL_PROVIDER} from '@/const/settings';
import { AgentChatConfigSchema } from '@/types/agent';
import { LobeMetaDataSchema } from '@/types/meta';

const fewShotsSchema = z.array(
  z.object({
    content: z.string(),
    role: z.string(),
  }),
);

const ttsSchema = z.object({
  showAllLocaleVoice: z.boolean().optional(),
  sttLocale: z.string().default('auto'),
  ttsService: z.string().default('openai'),
  voice: z
    .object({
      edge: z.string().optional(),
      microsoft: z.string().optional(),
      openai: z.string().default(''),
    })
    .optional(),
});

export const AgentSchema = z.object({
  chatConfig: AgentChatConfigSchema,
  fewShots: fewShotsSchema.optional(),
  model: z.string().default(DEFAULT_SMALL_MODEL),
  params: z.object({
    frequency_penalty: z.number().default(0).optional(),
    max_tokens: z.number().optional(),
    presence_penalty: z.number().default(0).optional(),
    temperature: z.number().default(1).optional(),
    top_p: z.number().default(1).optional(),
  }),
  plugins: z.array(z.string()).optional(),
  provider: z.string().default(DEFAULT_SMALL_PROVIDER).optional(),
  systemRole: z.string().default(''),
  tts: ttsSchema.optional(),
});

export const DB_SessionSchema = z.object({
  config: AgentSchema,
  group: z.string().default('default'),
  meta: LobeMetaDataSchema,
  pinned: z.number().int().min(0).max(1).optional(),
  type: z.enum(['agent', 'group']).default('agent'),
});

export type DB_Session = z.infer<typeof DB_SessionSchema>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/database/_deprecated/schemas/user.ts
================================================================================

import { z } from 'zod';

import { AgentSchema } from '@/database/_deprecated/schemas/session';
import { LobeMetaDataSchema } from '@/types/meta';

const generalSechma = z.object({
  fontSize: z.number().default(14),
  language: z.string(),
  neutralColor: z.string().optional(),
  password: z.string(),
  themeMode: z.string(),
});

const settingsSchema = z.object({
  defaultAgent: z.object({
    config: AgentSchema,
    meta: LobeMetaDataSchema,
  }),
  general: generalSechma.partial().optional(),
  keyVaults: z.any().optional(),
  languageModel: z.any().optional(),
  tts: z.object({
    openAI: z.object({
      sttModel: z.string(),
      ttsModel: z.string(),
    }),
    sttAutoStop: z.boolean(),
    sttServer: z.string(),
  }),
});

export const DB_UserSchema = z.object({
  avatar: z.string().optional(),
  settings: settingsSchema.partial(),
  uuid: z.string(),
});

export type DB_User = z.infer<typeof DB_UserSchema>;

export type DB_Settings = z.infer<typeof settingsSchema>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/index.ts
================================================================================

import { MigrationV2ToV3 } from '@/migrations/FromV2ToV3';
import { VersionController } from '@/migrations/VersionController';
import { ConfigStateAll } from '@/types/exportConfig';

import { MigrationV0ToV1 } from './FromV0ToV1';
import { MigrationV1ToV2 } from './FromV1ToV2';
import { MigrationV3ToV4 } from './FromV3ToV4';
import { MigrationV4ToV5 } from './FromV4ToV5';
import { MigrationV5ToV6 } from './FromV5ToV6';
import { MigrationV6ToV7 } from './FromV6ToV7';

// Current latest version
export const CURRENT_CONFIG_VERSION = 7;

// Version migrations module
const ConfigMigrations = [
  /**
   * 2024.05.27
   *
   * apiKey in languageModel change to keyVaults
   */
  MigrationV6ToV7,
  /**
   * 2024.05.24
   *
   * some config in agentConfig change to chatConfig
   */ MigrationV5ToV6,
  /**
   * 2024.05.11
   *
   * role=function to role=tool
   */
  MigrationV4ToV5,
  /**
   * 2024.04.09
   * settings migrate the `languageModel`
   * - from `openAI` to `openai`, `azure`
   * - from customModelName to `enabledModels` and `customModelCards`
   */
  MigrationV3ToV4,
  /**
   * 2024.01.22
   * from `group = pinned` to `pinned:true`
   */
  MigrationV2ToV3,
  /**
   * 2023.11.27
   * 从单 key 数据库转换为基于 dexie 的关系型结构
   */
  MigrationV1ToV2,
  /**
   * 2023.07.11
   * just the first version, Nothing to do
   */
  MigrationV0ToV1,
];

export const Migration = new VersionController<ConfigStateAll>(
  ConfigMigrations,
  CURRENT_CONFIG_VERSION,
);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV6ToV7/index.ts
================================================================================

import type { Migration, MigrationData } from '@/migrations/VersionController';

import { V6ConfigState, V6Settings } from './types/v6';
import { V7ConfigState, V7KeyVaults, V7Settings } from './types/v7';

const SENSITIVE_KEYS = [
  'apiKey',
  'endpoint',
  'accessKeyId',
  'secretAccessKey',
  'apiVersion',
  'region',
];

type SensitiveKeys = (typeof SENSITIVE_KEYS)[number];

function extractSensitiveInfo<T extends Record<string, any>>(
  obj: T,
  sensitiveKeys: SensitiveKeys[],
  provider: string,
): [T, Record<SensitiveKeys, string>] {
  const keyVaults: Record<SensitiveKeys, string> = {} as any;

  sensitiveKeys.forEach((key) => {
    if (obj[key]) {
      if (key === 'endpoint' && provider !== 'azure') {
        keyVaults['baseURL'] = obj[key];
      } else {
        keyVaults[key] = obj[key];
      }

      delete obj[key];
    }
  });

  return [obj, keyVaults];
}

export class MigrationV6ToV7 implements Migration {
  // from this version to start migration
  version = 6;

  migrate(data: MigrationData<V6ConfigState>): MigrationData<V7ConfigState> {
    const { settings } = data.state;

    return {
      ...data,
      state: {
        ...data.state,
        settings: !settings ? undefined : MigrationV6ToV7.migrateSettings(settings),
      },
    };
  }

  static migrateSettings = (settings: V6Settings): V7Settings => {
    const {
      languageModel = {},
      password,
      neutralColor,
      themeMode,
      fontSize,
      primaryColor,
      language,
      ...res
    } = settings;

    const keyVaults = {
      password,
    } as V7KeyVaults;

    Object.entries(languageModel).forEach(([provider, config]) => {
      if (!config) return;

      const [strippedConfig, providerVaults] = extractSensitiveInfo(
        config,
        SENSITIVE_KEYS,
        provider,
      );

      // @ts-ignore
      languageModel[provider] = strippedConfig as any;
      // @ts-ignore
      keyVaults[provider] = providerVaults;
    });

    return {
      ...res,
      general: {
        fontSize,
        language,
        neutralColor,
        primaryColor,
        themeMode,
      },
      keyVaults,
      languageModel,
    };
  };
}

export const MigrationKeyValueSettings = MigrationV6ToV7;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV6ToV7/types/v7.ts
================================================================================

interface OpenAICompatibleKeyVault {
  apiKey?: string;
  baseURL?: string;
}
interface AzureOpenAIKeyVault {
  apiKey?: string;
  apiVersion?: string;
  endpoint?: string;
}

export interface AWSBedrockKeyVault {
  accessKeyId?: string;
  region?: string;
  secretAccessKey?: string;
}

export interface V7KeyVaults {
  anthropic: OpenAICompatibleKeyVault;
  azure: AzureOpenAIKeyVault;
  bedrock: AWSBedrockKeyVault;
  deepseek: OpenAICompatibleKeyVault;
  google: OpenAICompatibleKeyVault;
  groq: OpenAICompatibleKeyVault;
  minimax: OpenAICompatibleKeyVault;
  mistral: OpenAICompatibleKeyVault;
  moonshot: OpenAICompatibleKeyVault;
  ollama: OpenAICompatibleKeyVault;
  openai: OpenAICompatibleKeyVault;
  openrouter: OpenAICompatibleKeyVault;
  password: string;
  perplexity: OpenAICompatibleKeyVault;
  togetherai: OpenAICompatibleKeyVault;
  zeroone: OpenAICompatibleKeyVault;
  zhipu: OpenAICompatibleKeyVault;
}

interface V7ProviderConfig {
  autoFetchModelLists?: boolean;
  customModelCards?: any[];
  enabled: boolean;
  enabledModels?: string[] | null;
  fetchOnClient?: boolean;
  latestFetchTime?: number;
  remoteModelCards?: any[];
}

export type V7ModelProviderConfig = Record<string, V7ProviderConfig>;

export interface V7GeneralSettings {
  fontSize: number;
  language: string;
  neutralColor?: string;
  primaryColor?: string;
  themeMode: string;
}

export interface V7Settings {
  defaultAgent: any;
  general: V7GeneralSettings;
  keyVaults: V7KeyVaults;
  languageModel?: V7ModelProviderConfig;
  sync: any;
  tool: any;
  tts: any;
}

export interface V7ConfigState {
  settings?: V7Settings;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV6ToV7/types/v6.ts
================================================================================

interface V6OpenAICompatibleConfig {
  apiKey?: string;
  autoFetchModelLists?: boolean;
  customModelCards?: any[];
  enabled: boolean;
  enabledModels?: string[] | null;
  endpoint?: string;
  fetchOnClient?: boolean;
  latestFetchTime?: number;
  remoteModelCards?: any[];
}

interface AzureOpenAIConfig extends Omit<V6OpenAICompatibleConfig, 'endpoint'> {
  apiVersion?: string;
  endpoint?: string;
}

interface AWSBedrockConfig extends Omit<V6OpenAICompatibleConfig, 'apiKey' | 'endpoint'> {
  accessKeyId?: string;
  region?: string;
  secretAccessKey?: string;
}

interface V6ModelProviderConfig {
  anthropic: V6OpenAICompatibleConfig;
  azure: AzureOpenAIConfig;
  bedrock: AWSBedrockConfig;
  deepseek: V6OpenAICompatibleConfig;
  google: V6OpenAICompatibleConfig;
  groq: V6OpenAICompatibleConfig;
  minimax: V6OpenAICompatibleConfig;
  mistral: V6OpenAICompatibleConfig;
  moonshot: V6OpenAICompatibleConfig;
  ollama: V6OpenAICompatibleConfig;
  openai: V6OpenAICompatibleConfig;
  openrouter: V6OpenAICompatibleConfig;
  perplexity: V6OpenAICompatibleConfig;
  togetherai: V6OpenAICompatibleConfig;
  zeroone: V6OpenAICompatibleConfig;
  zhipu: V6OpenAICompatibleConfig;
}

export type V6ProviderKey = keyof V6ModelProviderConfig;

export interface V6Settings {
  defaultAgent: any;
  fontSize: number;
  language: string;
  languageModel?: V6ModelProviderConfig;
  neutralColor?: string;
  password: string;
  primaryColor?: string;
  sync: any;
  themeMode: string;
  tool: any;
  tts: any;
}

export interface V6ConfigState {
  settings?: V6Settings;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV5ToV6/types/v6.ts
================================================================================

import { FewShots, LLMParams } from '@/types/llm';

export type TTSServer = 'openai' | 'edge' | 'microsoft';

export interface TTSConfig {
  showAllLocaleVoice?: boolean;
  sttLocale: 'auto' | string;
  ttsService: TTSServer;
  voice: {
    edge?: string;
    microsoft?: string;
    openai: string;
  };
}

export interface ChatConfig {
  autoCreateTopicThreshold: number;
  compressThreshold?: number;
  displayMode?: 'chat' | 'docs';
  enableAutoCreateTopic?: boolean;
  enableCompressThreshold?: boolean;
  enableHistoryCount?: boolean;
  enableMaxTokens?: boolean;
  historyCount?: number;
  inputTemplate?: string;
}

export interface V6AgentConfig {
  chatConfig: ChatConfig;
  fewShots?: FewShots;
  model: string;
  params: LLMParams;
  plugins?: string[];
  provider?: string;
  systemRole: string;
  tts: TTSConfig;
}

export interface V6Session {
  config: V6AgentConfig;
  createdAt: number;

  group?: string;
  id: string;
  meta: {
    avatar?: string;
    backgroundColor?: string;
    description?: string;
    tags?: string[];
    title?: string;
  };
  model: string;
  pinned?: boolean;
  type: 'agent';
  updatedAt: number;
}

export interface V6ConfigState {
  sessions: V6Session[];
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV3ToV4/index.ts
================================================================================

import type { Migration, MigrationData } from '@/migrations/VersionController';
import { transformToChatModelCards } from '@/utils/parseModels';

import { V3ConfigState, V3LegacyConfig, V3OpenAIConfig, V3Settings } from './types/v3';
import { V4AzureOpenAIConfig, V4ConfigState, V4ProviderConfig, V4Settings } from './types/v4';

export class MigrationV3ToV4 implements Migration {
  // from this version to start migration
  version = 3;

  migrate(data: MigrationData<V3ConfigState>): MigrationData<V4ConfigState> {
    const { settings } = data.state;

    return {
      ...data,
      state: {
        ...data.state,
        settings: !settings ? undefined : MigrationV3ToV4.migrateSettings(settings),
      },
    };
  }

  static migrateSettings = (settings: V3Settings): V4Settings => {
    const { languageModel } = settings;

    if (!languageModel) return { ...settings, languageModel: undefined };

    const { openAI, togetherai, openrouter, ollama, ...res } = languageModel;
    const { openai, azure } = this.migrateOpenAI(openAI);

    return {
      ...settings,
      languageModel: {
        ...res,
        azure,
        ollama: ollama && this.migrateProvider(ollama),
        openai,
        openrouter: openrouter && this.migrateProvider(openrouter),
        togetherai: togetherai && this.migrateProvider(togetherai),
      },
    };
  };

  static migrateOpenAI = (
    openai?: V3OpenAIConfig,
  ): { azure: V4AzureOpenAIConfig; openai: V4ProviderConfig } => {
    if (!openai)
      return {
        azure: { apiKey: '', enabled: false },
        openai: { apiKey: '', enabled: true },
      };

    if (openai.useAzure) {
      return {
        azure: {
          apiKey: openai.OPENAI_API_KEY,
          apiVersion: openai.azureApiVersion,
          enabled: true,
          endpoint: openai.endpoint,
        },
        openai: { apiKey: '', enabled: true, endpoint: '' },
      };
    }

    const customModelCards = transformToChatModelCards({
      defaultChatModels: [],
      modelString: openai.customModelName,
    });

    return {
      azure: {
        apiKey: '',
        enabled: false,
        endpoint: '',
      },
      openai: {
        apiKey: openai.OPENAI_API_KEY,
        customModelCards:
          customModelCards && customModelCards.length > 0 ? customModelCards : undefined,
        enabled: true,
        endpoint: openai.endpoint,
      },
    };
  };

  static migrateProvider = (provider: V3LegacyConfig): V4ProviderConfig => {
    const customModelCards = transformToChatModelCards({
      defaultChatModels: [],
      modelString: provider.customModelName,
    });

    return {
      apiKey: provider.apiKey,
      customModelCards:
        customModelCards && customModelCards.length > 0 ? customModelCards : undefined,
      enabled: provider.enabled,
      endpoint: provider.endpoint,
    };
  };
}

export const MigrationLLMSettings = MigrationV3ToV4;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV3ToV4/types/v3.ts
================================================================================

interface V3GeneralConfig {
  apiKey?: string;
  enabled: boolean;
  endpoint?: string;
}

export interface V3OpenAIConfig {
  OPENAI_API_KEY: string;
  azureApiVersion?: string;
  customModelName?: string;
  enabled: boolean;
  endpoint?: string;
  useAzure?: boolean;
}

export interface V3LegacyConfig {
  apiKey?: string;
  customModelName?: string;
  enabled?: boolean;
  enabledModels: string[];
  endpoint?: string;
}

export interface V3LLMConfig {
  bedrock: any;
  google: V3GeneralConfig;
  ollama: V3LegacyConfig;
  openAI: V3OpenAIConfig;
  openrouter: V3LegacyConfig;
  togetherai: V3LegacyConfig;
}

/**
 * 配置设置
 */
export interface V3Settings {
  defaultAgent: any;
  fontSize: number;
  language: string;
  languageModel?: Partial<V3LLMConfig>;
  neutralColor?: string;
  password: string;
  primaryColor?: string;
  sync: any;
  themeMode: string;
  tool: any;
  tts: any;
}

export interface V3ConfigState {
  settings?: V3Settings;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/migrations/FromV3ToV4/types/v4.ts
================================================================================

import { ChatModelCard } from '@/types/llm';

import { V3LLMConfig, V3Settings } from './v3';

export interface V4ProviderConfig {
  apiKey?: string;
  customModelCards?: ChatModelCard[];
  enabled?: boolean;
  /**
   * enabled models id
   */
  enabledModels?: string[] | null;
  endpoint?: string;
}
export interface V4AzureOpenAIConfig extends V4ProviderConfig {
  apiVersion?: string;
}

export interface V4lLLMConfig
  extends Omit<Partial<V3LLMConfig>, 'ollama' | 'openAI' | 'openrouter' | 'togetherai'> {
  azure?: V4AzureOpenAIConfig;
  ollama?: V4ProviderConfig;
  openai: V4ProviderConfig;
  openrouter?: V4ProviderConfig;
  togetherai?: V4ProviderConfig;
}

/**
 * 配置设置
 */
export interface V4Settings extends Omit<V3Settings, 'languageModel'> {
  languageModel?: V4lLLMConfig;
}

export interface V4ConfigState {
  settings?: V4Settings;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/tools/dalle/index.ts
================================================================================

import { BuiltinToolManifest } from '@/types/tool';

// import {SiOpenai} from "@icons-pack/react-simple-icons";

export const DalleManifest: BuiltinToolManifest = {
  api: [
    {
      description: 'Create images from a text-only prompt.',
      name: 'text2image',
      parameters: {
        properties: {
          prompts: {
            description:
              "The user's original image description, potentially modified to abide by the lobe-image-designer policies. If the user does not suggest a number of captions to create, create four of them. If creating multiple captions, make them as diverse as possible. If the user requested modifications to previous images, the captions should not simply be longer, but rather it should be refactored to integrate the suggestions into each of the captions. Generate no more than 4 images, even if the user requests more.",
            items: {
              type: 'string',
            },
            maxItems: 4,
            minItems: 1,
            type: 'array',
          },
          quality: {
            default: 'standard',
            description:
              'The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image.',
            enum: ['standard', 'hd'],
            type: 'string',
          },
          seeds: {
            description:
              'A list of seeds to use for each prompt. If the user asks to modify a previous image, populate this field with the seed used to generate that image from the image lobe-image-designer metadata.',
            items: {
              type: 'integer',
            },
            type: 'array',
          },
          size: {
            default: '1024x1024',
            description:
              'The resolution of the requested image, which can be wide, square, or tall. Use 1024x1024 (square) as the default unless the prompt suggests a wide image, 1792x1024, or a full-body portrait, in which case 1024x1792 (tall) should be used instead. Always include this parameter in the request.',
            enum: ['1792x1024', '1024x1024', '1024x1792'],
            type: 'string',
          },
          style: {
            default: 'vivid',
            description:
              'The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.',
            enum: ['vivid', 'natural'],
            type: 'string',
          },
        },
        required: ['prompts'],
        type: 'object',
      },
    },
  ],
  // due to system prompt is for training Dalle3 as a built-in tool by OpenAI,
  // there are occasional instances where the function call contains the name "dalle," leading to subsequent failures.
  // so we need a different unique identifier to avoid failure.refs:
  // https://github.com/lobehub/lobe-chat/issues/783
  // https://github.com/lobehub/lobe-chat/issues/870
  identifier: 'lobe-image-designer',
  meta: {
    avatar: `data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI5NiIgaGVpZ2h0PSI5NiIgZmlsbD0ibm9uZSIgc3R5bGU9ImNvbG9yOiNmZmY7ZmlsbDojMDAwIj4KICAgIDxwYXRoIGQ9Ik0wIDBoOTZ2OTZIMHoiLz4KICAgIDxwYXRoIGZpbGw9ImN1cnJlbnRDb2xvciIKICAgICAgICAgIGQ9Ik03OC44MyA0MS40NjJBMTcuOTQgMTcuOTQgMCAwIDAgNzcuMjkgMjYuNzNhMTguMTQgMTguMTQgMCAwIDAtMTkuNTM4LTguNzA0IDE3LjkzNyAxNy45MzcgMCAwIDAtMTMuNTI5LTYuMDMxIDE4LjE0MyAxOC4xNDMgMCAwIDAtMTcuMzA1IDEyLjU2IDE3Ljk0NiAxNy45NDYgMCAwIDAtMTEuOTk0IDguNzAxIDE4LjE0MyAxOC4xNDMgMCAwIDAgMi4yMzMgMjEuMjcxIDE3LjkzOCAxNy45MzggMCAwIDAgMS41NCAxNC43MzIgMTguMTQxIDE4LjE0MSAwIDAgMCAxOS41MzggOC43MDQgMTcuOTMgMTcuOTMgMCAwIDAgMTMuNTI4IDYuMDMxQTE4LjE0MiAxOC4xNDIgMCAwIDAgNjkuMDggNzEuNDI3YTE3LjkzOCAxNy45MzggMCAwIDAgMTEuOTk0LTguNzAxIDE4LjE0MSAxOC4xNDEgMCAwIDAtMi4yNDMtMjEuMjY0Wk01MS45MSA3OS4yODdjLTMuNTgzIDAtNi4zNTYtMS4xLTguNzgtMy4xMjIuMTEtLjA2LjMwMi0uMTY1LjQyNy0uMjQybDE0LjMzNi04LjI4YTIuMzI3IDIuMzI3IDAgMCAwIDEuMTc4LTIuMDRWNDUuMzkybDYuMDYgMy40OTlhLjIxNy4yMTcgMCAwIDEgLjExOC4xNjZWNjUuNzljLS4wMDEgNy41OTMtNi4zMjIgMTMuNDk3LTEzLjM0IDEzLjQ5N1pNMjIuNzc4IDY2LjkwNmExMy40NDcgMTMuNDQ3IDAgMCAxLTEuNjEtOS4wNDJjLjEwNy4wNjQuMjkzLjE3OC40MjYuMjU0TDM1LjkzIDY2LjRhMi4zMyAyLjMzIDAgMCAwIDIuMzU1IDBsMTcuNTAzLTEwLjEwNnY2Ljk5OGEuMjE3LjIxNyAwIDAgMS0uMDg3LjE4Nkw0MS4yMSA3MS44NDRhMTMuNTA3IDEzLjUwNyAwIDAgMS0xOC40MzEtNC45MzhabS0zLjc3Ny0zMS4yOTdhMTMuNDQ5IDEzLjQ0OSAwIDAgMSA3LjAyMy01LjkxNlY0Ni43NWEyLjMyNCAyLjMyNCAwIDAgMCAxLjE3NyAyLjAzOGwxNy41MDEgMTAuMTA1LTYuMDYgMy40OTlhLjIxOC4yMTggMCAwIDEtLjIwNC4wMThsLTE0LjQ5My04LjM3NEExMy41MDcgMTMuNTA3IDAgMCAxIDE5IDM1LjYwOVptNDkuNzkgMTEuNTg3TDUxLjI4OCAzNy4wOWw2LjA2LTMuNDk4YS4yMi4yMiAwIDAgMSAuMjA0LS4wMThsMTQuNDk0IDguMzdhMTMuNDk1IDEzLjQ5NSAwIDAgMS0yLjA4OCAyNC4zNDZWNDkuMjMzYTIuMzI4IDIuMzI4IDAgMCAwLTEuMTY3LTIuMDM3Wm02LjAzLTkuMDg0YTIwLjA2NCAyMC4wNjQgMCAwIDAtLjQyNS0uMjU0TDYwLjA2IDI5LjU3N2EyLjMzNyAyLjMzNyAwIDAgMC0yLjM1NSAwTDQwLjIwMyAzOS42ODN2LTYuOTk4YS4yMTguMjE4IDAgMCAxIC4wODYtLjE4NmwxNC40OTItOC4zNmExMy40OTUgMTMuNDk1IDAgMCAxIDIwLjA0IDEzLjk3M1pNMzYuOTA5IDUwLjU5bC02LjA2LTMuNDk5YS4yMS4yMSAwIDAgMS0uMTE4LS4xNjZWMzAuMTg3YTEzLjQ5NCAxMy40OTQgMCAwIDEgMjIuMTI4LTEwLjM2MWMtLjExLjA2LS4zLjE2NC0uNDI2LjI0bC0xNC4zMzYgOC4yODJhMi4zMzIgMi4zMzIgMCAwIDAtMS4xNzggMi4wMzhsLS4wMSAyMC4yMDRabTMuMjkyLTcuMDk3IDcuNzk1LTQuNTAzIDcuNzk2IDQuNXY5LjAwMmwtNy43OTYgNC40OTgtNy43OTUtNC41di04Ljk5N1oiLz4KPC9zdmc+Cg==`,
    title: 'DALL·E 3',
  },
  systemRole: `Whenever a description of an image is given, use lobe-image-designer to create the images and then summarize the prompts used to generate the images in plain text. If the user does not ask for a specific number of images, default to creating four captions to send to lobe-image-designer that are written to be as diverse as possible.

  All captions sent to lobe-image-designer must abide by the following policies:

1. If the description is not in English, then translate it.
2. Do not create more than 4 images, even if the user requests more.
3. Don't create images of politicians or other public figures. Recommend other ideas instead.
5. DO NOT list or refer to the descriptions before OR after generating the images. They should ONLY ever be written out ONCE, in the \`prompts\` field of the request. You do not need to ask for permission to generate, just do it!
6. Always mention the image type (photo, oil painting, watercolor painting, illustration, cartoon, drawing, vector, render, etc.) at the beginning of the caption. Unless the caption suggests otherwise, make at least 1--2 of the 4 images photos.
7. Diversify depictions of ALL images with people to include DESCENT and GENDER for EACH person using direct terms. Adjust only human descriptions.
  - EXPLICITLY specify these attributes, not abstractly reference them.  The attributes should be specified in a minimal way and should directly describe their physical form.
  - Your choices should be grounded in reality. For example, all of a given OCCUPATION should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites.  Make choices that may be insightful or unique sometimes.
  - Use "various" or "diverse" ONLY IF the description refers to groups of more than 3 people. Do not change the number of people requested in the original description.
  - Don't alter memes, fictional character origins, or unseen people. Maintain the original prompt's intent and prioritize quality.
  - Do not create any imagery that would be offensive.

8. Silently modify descriptions that include names or hints or references of specific people or celebrities by carefully selecting a few minimal modifications to substitute references to the people with generic descriptions that don't divulge any information about their identities, except for their genders and physiques. Do this EVEN WHEN the instructions ask for the prompt to not be changed. Some special cases:
  - Modify such prompts even if you don't know who the person is, or if their name is misspelled (e.g. "Barake Obema")
  - If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.
  - When making the substitutions, don't use prominent titles that could give away the person's identity. E.g., instead of saying "president", "prime minister", or "chancellor", say "politician"; instead of saying "king", "queen", "emperor", or "empress", say "public figure"; instead of saying "Pope", say "religious figure"; and so on.
  - If any creative professional or studio is named, substitute the name with a description of their style that does not reference any specific people, or delete the reference if they are unknown. DO NOT refer to the artist or studio's style.

The prompt must intricately describe every part of the image in concrete, objective detail. THINK about what the end goal of the description is, and extrapolate that to what would make satisfying images.
All descriptions sent to lobe-image-designer should be a paragraph of text that is extremely descriptive and detailed. Each should be more than 3 sentences long.`,
  type: 'builtin',
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/rag.ts
================================================================================

import { z } from 'zod';

import { ChatSemanticSearchChunk } from '@/types/chunk';

export const SemanticSearchSchema = z.object({
  fileIds: z.array(z.string()).optional(),
  knowledgeIds: z.array(z.string()).optional(),
  messageId: z.string(),
  model: z.string().optional(),
  rewriteQuery: z.string(),
  userQuery: z.string(),
});

export type SemanticSearchSchemaType = z.infer<typeof SemanticSearchSchema>;

export type MessageSemanticSearchChunk = Pick<ChatSemanticSearchChunk, 'id' | 'similarity'>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/asyncTask.ts
================================================================================

export enum AsyncTaskType {
  Chunking = 'chunk',
  Embedding = 'embedding',
}

export enum AsyncTaskStatus {
  Error = 'error',
  Pending = 'pending',
  Processing = 'processing',
  Success = 'success',
}

export enum AsyncTaskErrorType {
  EmbeddingError = 'EmbeddingError',
  /**
   * the chunk parse result it empty
   */
  NoChunkError = 'NoChunkError',
  ServerError = 'ServerError',
  /**
   * this happens when the task is not trigger successfully
   */
  TaskTriggerError = 'TaskTriggerError',
  Timeout = 'TaskTimeout',
}

export interface IAsyncTaskError {
  body: string | { detail: string };
  name: string;
}

export class AsyncTaskError implements IAsyncTaskError {
  constructor(name: string, message: string) {
    this.name = name;
    this.body = { detail: message };
  }

  name: string;

  body: { detail: string };
}

export interface FileParsingTask {
  chunkCount?: number | null;
  chunkingError?: IAsyncTaskError | null;
  chunkingStatus?: AsyncTaskStatus | null;
  embeddingError?: IAsyncTaskError | null;
  embeddingStatus?: AsyncTaskStatus | null;
  finishEmbedding?: boolean;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/serverConfig.ts
================================================================================

import { DeepPartial } from 'utility-types';

import { ChatModelCard } from '@/types/llm';
import {
  GlobalLLMProviderKey,
  UserDefaultAgent,
  UserSystemAgentConfig,
} from '@/types/user/settings';

export interface ServerModelProviderConfig {
  enabled?: boolean;
  enabledModels?: string[];
  fetchOnClient?: boolean;
  /**
   * the model cards defined in server
   */
  serverModelCards?: ChatModelCard[];
}

export type ServerLanguageModel = Partial<Record<GlobalLLMProviderKey, ServerModelProviderConfig>>;

export interface GlobalServerConfig {
  defaultAgent?: DeepPartial<UserDefaultAgent>;
  enableUploadFileToServer?: boolean;
  enabledAccessCode?: boolean;
  enabledOAuthSSO?: boolean;
  languageModel?: ServerLanguageModel;
  oAuthSSOProviders?: string[];
  systemAgent?: DeepPartial<UserSystemAgentConfig>;
  telemetry: {
    langfuse?: boolean;
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/fetch.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix */
import { ILobeAgentRuntimeErrorType } from '@/libs/agent-runtime';

export const ChatErrorType = {
  // ******* 业务错误语义 ******* //

  InvalidAccessCode: 'InvalidAccessCode', // is in valid password
  InvalidClerkUser: 'InvalidClerkUser', // is not Clerk User
  /**
   * @deprecated
   */
  NoOpenAIAPIKey: 'NoOpenAIAPIKey',
  OllamaServiceUnavailable: 'OllamaServiceUnavailable', // 未启动/检测到 Ollama 服务
  PluginFailToTransformArguments: 'PluginFailToTransformArguments',
  UnknownChatFetchError: 'UnknownChatFetchError',

  // ******* 客户端错误 ******* //
  BadRequest: 400,
  Unauthorized: 401,
  Forbidden: 403,
  ContentNotFound: 404, // 没找到接口
  MethodNotAllowed: 405, // 不支持
  TooManyRequests: 429,

  // ******* 服务端错误 ******* //InvalidPluginArgumentsTransform
  InternalServerError: 500,
  BadGateway: 502,
  ServiceUnavailable: 503,
  GatewayTimeout: 504,
} as const;
/* eslint-enable */

export type ErrorType = (typeof ChatErrorType)[keyof typeof ChatErrorType];

export interface ErrorResponse {
  body: any;
  errorType: ErrorType | ILobeAgentRuntimeErrorType;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/chunk/document.ts
================================================================================

export interface ChunkDocument {
  compositeId?: string;
  id?: string;
  index: number;
  metadata: any;
  parentId?: string;
  text: string;
  type: string;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/chunk/index.ts
================================================================================

export enum ChunkStatus {
  Error = 'error',
  Processing = 'processing',
  Success = 'success',
}

export interface Elements {
  element_id: string;
  metadata: ChunkMetadata;
  text: string;
  type: string;
}

export interface ChunkMetadata {
  coordinates: Coordinates;
  languages: string[];
  pageNumber?: number;
  parent_id?: string;
  text_as_html?: string;
}

export interface Coordinates {
  layout_height: number;
  layout_width: number;
  points: number[][];
  system: string;
}

export interface FileChunk {
  createdAt: Date;
  id: string;
  index: number;
  metadata: ChunkMetadata | null;
  pageNumber?: number;
  parentId?: string | null;
  text: string;
  type: string;
  updatedAt: Date;
}

export interface SemanticSearchChunk {
  fileId: string | null;
  fileName: string | null;
  id: string;
  metadata: ChunkMetadata | null;
  pageNumber?: number | null;
  similarity: number;
  text: string | null;
  type: string | null;
}

export type ChatSemanticSearchChunk = Omit<SemanticSearchChunk, 'metadata' | 'type'>;

export * from './document';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/message/tools.ts
================================================================================

import { DeepPartial } from 'utility-types';
import { z } from 'zod';

import { LobeToolRenderType } from '@/types/tool';

export interface ChatPluginPayload {
  apiName: string;
  arguments: string;
  identifier: string;
  type: LobeToolRenderType;
}

export interface ChatToolPayload {
  apiName: string;
  arguments: string;
  id: string;
  identifier: string;
  type: LobeToolRenderType;
}

/**
 * The function that the model called.
 */
export interface ToolFunction {
  /**
   * The arguments to call the function with, as generated by the model in JSON
   * format. Note that the model does not always generate valid JSON, and may
   * hallucinate parameters not defined by your function schema. Validate the
   * arguments in your code before calling your function.
   */
  arguments: string;

  /**
   * The name of the function to call.
   */
  name: string;
}

export interface MessageToolCall {
  /**
   * The function that the model called.
   */
  function: ToolFunction;

  /**
   * The ID of the tool call.
   */
  id: string;

  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function' | string;
}

export type MessageToolCallChunk = DeepPartial<MessageToolCall> & { index: number };

export const MessageToolCallSchema = z.object({
  function: z.object({
    arguments: z.string(),
    name: z.string(),
  }),
  id: z.string(),
  type: z.string(),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/message/index.ts
================================================================================

import { IPluginErrorType } from '@lobehub/chat-plugin-sdk';

import { ILobeAgentRuntimeErrorType } from '@/libs/agent-runtime';
import { ErrorType } from '@/types/fetch';
import { UploadFileItem } from '@/types/files';
import { MessageSemanticSearchChunk } from '@/types/rag';

import { BaseDataModel } from '../meta';
import { ChatPluginPayload, ChatToolPayload } from './tools';
import { Translate } from './translate';

export type MessageRoleType = 'user' | 'system' | 'assistant' | 'tool';

/**
 * 聊天消息错误对象
 */
export interface ChatMessageError {
  body?: any;
  message: string;
  type: ErrorType | IPluginErrorType | ILobeAgentRuntimeErrorType;
}

export interface ChatTranslate extends Translate {
  content?: string;
}

export interface ChatTTS {
  contentMd5?: string;
  file?: string;
  voice?: string;
}

export * from './tools';

export interface ChatFileItem {
  fileType: string;
  id: string;
  name: string;
  size: number;
  url: string;
}

export interface ChatImageItem {
  alt: string;
  id: string;
  url: string;
}

export interface ChatFileChunk {
  fileId: string;
  fileType: string;
  fileUrl: string;
  filename: string;
  id: string;
  similarity?: number;
  text: string;
}

export interface ChatMessageExtra {
  fromModel?: string;
  fromProvider?: string;
  // 翻译
  translate?: ChatTranslate | false | null;
  // TTS
  tts?: ChatTTS;
}

export interface ChatMessage extends BaseDataModel {
  chunksList?: ChatFileChunk[];
  content: string;
  error?: ChatMessageError | null;

  // 扩展字段
  extra?: ChatMessageExtra;
  fileList?: ChatFileItem[];
  /**
   * this is a deprecated field, only use in client db
   * and should be remove after migrate to pglite
   * this field is replaced by fileList and imageList
   * @deprecated
   */
  files?: string[];
  imageList?: ChatImageItem[];
  /**
   * observation id
   */
  observationId?: string;

  /**
   * parent message id
   */
  parentId?: string;
  plugin?: ChatPluginPayload;

  pluginState?: any;
  /**
   * quoted other message's id
   */
  quotaId?: string;
  ragQuery?: string | null;
  ragQueryId?: string | null;
  ragRawQuery?: string | null;
  /**
   * message role type
   */
  role: MessageRoleType;

  sessionId?: string;
  threadId?: string | null;

  tool_call_id?: string;
  tools?: ChatToolPayload[];
  /**
   * 保存到主题的消息
   */
  topicId?: string;
  /**
   * 观测链路 id
   */
  traceId?: string;
}

export type ChatMessageMap = Record<string, ChatMessage>;

export interface CreateMessageParams
  extends Partial<Omit<ChatMessage, 'content' | 'role' | 'topicId' | 'chunksList'>> {
  content: string;
  error?: ChatMessageError | null;
  fileChunks?: MessageSemanticSearchChunk[];
  files?: string[];
  fromModel?: string;
  fromProvider?: string;
  role: MessageRoleType;
  sessionId: string;
  threadId?: string | null;
  topicId?: string;
  traceId?: string;
}

export interface SendMessageParams {
  /**
   * create a thread
   */
  createThread?: boolean;
  files?: UploadFileItem[];
  /**
   *
   * https://github.com/lobehub/lobe-chat/pull/2086
   */
  isWelcomeQuestion?: boolean;
  message: string;
  onlyAddUserMessage?: boolean;
}

export interface SendThreadMessageParams {
  /**
   * create a thread
   */
  createNewThread?: boolean;
  // files?: UploadFileItem[];
  message: string;
  onlyAddUserMessage?: boolean;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/user/settings/keyVaults.ts
================================================================================

export interface OpenAICompatibleKeyVault {
  apiKey?: string;
  baseURL?: string;
}

export interface AzureOpenAIKeyVault {
  apiKey?: string;
  apiVersion?: string;
  endpoint?: string;
}

export interface AWSBedrockKeyVault {
  accessKeyId?: string;
  region?: string;
  secretAccessKey?: string;
  sessionToken?: string;
}

export interface CloudflareKeyVault {
  apiKey?: string;
  baseURLOrAccountID?: string;
}

export interface WenxinKeyVault {
  accessKey?: string;
  secretKey?: string;
}

export interface UserKeyVaults {
  ai21?: OpenAICompatibleKeyVault;
  ai360?: OpenAICompatibleKeyVault;
  anthropic?: OpenAICompatibleKeyVault;
  azure?: AzureOpenAIKeyVault;
  baichuan?: OpenAICompatibleKeyVault;
  bedrock?: AWSBedrockKeyVault;
  cloudflare?: CloudflareKeyVault;
  deepseek?: OpenAICompatibleKeyVault;
  fireworksai?: OpenAICompatibleKeyVault;
  giteeai?: OpenAICompatibleKeyVault;
  github?: OpenAICompatibleKeyVault;
  google?: OpenAICompatibleKeyVault;
  groq?: OpenAICompatibleKeyVault;
  higress?: OpenAICompatibleKeyVault;
  huggingface?: OpenAICompatibleKeyVault;
  hunyuan?: OpenAICompatibleKeyVault;
  internlm?: OpenAICompatibleKeyVault;
  lobehub?: any;
  minimax?: OpenAICompatibleKeyVault;
  mistral?: OpenAICompatibleKeyVault;
  moonshot?: OpenAICompatibleKeyVault;
  novita?: OpenAICompatibleKeyVault;
  ollama?: OpenAICompatibleKeyVault;
  openai?: OpenAICompatibleKeyVault;
  openrouter?: OpenAICompatibleKeyVault;
  password?: string;
  perplexity?: OpenAICompatibleKeyVault;
  qwen?: OpenAICompatibleKeyVault;
  sensenova?: OpenAICompatibleKeyVault;
  siliconcloud?: OpenAICompatibleKeyVault;
  spark?: OpenAICompatibleKeyVault;
  stepfun?: OpenAICompatibleKeyVault;
  taichu?: OpenAICompatibleKeyVault;
  togetherai?: OpenAICompatibleKeyVault;
  upstage?: OpenAICompatibleKeyVault;
  wenxin?: WenxinKeyVault;
  xai?: OpenAICompatibleKeyVault;
  zeroone?: OpenAICompatibleKeyVault;
  zhipu?: OpenAICompatibleKeyVault;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/user/settings/tts.ts
================================================================================

export type STTServer = 'openai' | 'browser';

export interface UserTTSConfig {
  openAI: {
    sttModel: 'whisper-1';
    ttsModel: 'tts-1' | 'tts-1-hd';
  };
  sttAutoStop: boolean;
  sttServer: STTServer;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/user/settings/index.ts
================================================================================

import type { LobeAgentSettings } from '@/types/session';

import { UserGeneralConfig } from './general';
import { UserKeyVaults } from './keyVaults';
import { UserModelProviderConfig } from './modelProvider';
import { UserSyncSettings } from './sync';
import { UserSystemAgentConfig } from './systemAgent';
import { UserToolConfig } from './tool';
import { UserTTSConfig } from './tts';

export type UserDefaultAgent = LobeAgentSettings;

export * from './general';
export * from './keyVaults';
export * from './modelProvider';
export * from './sync';
export * from './systemAgent';
export * from './tts';

/**
 * 配置设置
 */
export interface UserSettings {
  defaultAgent: UserDefaultAgent;
  general: UserGeneralConfig;
  keyVaults: UserKeyVaults;
  languageModel: UserModelProviderConfig;
  sync?: UserSyncSettings;
  systemAgent: UserSystemAgentConfig;
  tool: UserToolConfig;
  tts: UserTTSConfig;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/agent/index.ts
================================================================================

import { z } from 'zod';

import { FileItem } from '@/types/files';
import { KnowledgeBaseItem } from '@/types/knowledgeBase';
import { FewShots, LLMParams } from '@/types/llm';

export type TTSServer = 'openai' | 'edge' | 'microsoft';

export interface LobeAgentTTSConfig {
  showAllLocaleVoice?: boolean;
  sttLocale: 'auto' | string;
  ttsService: TTSServer;
  voice: {
    edge?: string;
    microsoft?: string;
    openai: string;
  };
}

export interface LobeAgentConfig {
  chatConfig: LobeAgentChatConfig;
  fewShots?: FewShots;
  files?: FileItem[];
  id?: string;
  /**
   * knowledge bases
   */
  knowledgeBases?: KnowledgeBaseItem[];
  /**
   * 角色所使用的语言模型
   * @default gpt-4o-mini
   */
  model: string;
  /**
   * 语言模型参数
   */
  params: LLMParams;
  /**
   * 启用的插件
   */
  plugins?: string[];
  /**
   *  模型供应商
   */
  provider?: string;
  /**
   * 系统角色
   */
  systemRole: string;

  /**
   * 语音服务
   */
  tts: LobeAgentTTSConfig;
}

export interface LobeAgentChatConfig {
  autoCreateTopicThreshold: number;
  displayMode?: 'chat' | 'docs';
  enableAutoCreateTopic?: boolean;
  /**
   * 历史消息长度压缩阈值
   */
  enableCompressHistory?: boolean;
  /**
   * 开启历史记录条数
   */
  enableHistoryCount?: boolean;
  enableMaxTokens?: boolean;

  /**
   * 历史消息条数
   */
  historyCount?: number;
  inputTemplate?: string;
}

export const AgentChatConfigSchema = z.object({
  autoCreateTopicThreshold: z.number().default(2),
  displayMode: z.enum(['chat', 'docs']).optional(),
  enableAutoCreateTopic: z.boolean().optional(),
  enableCompressHistory: z.boolean().optional(),
  enableHistoryCount: z.boolean().optional(),
  enableMaxTokens: z.boolean().optional(),
  historyCount: z.number().optional(),
});

export type LobeAgentConfigKeys =
  | keyof LobeAgentConfig
  | ['params', keyof LobeAgentConfig['params']];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/files/upload.ts
================================================================================

import { z } from 'zod';

import { FileParsingTask } from '@/types/asyncTask';

export interface FileUploadState {
  progress: number;
  /**
   * rest time in s
   */
  restTime: number;
  /**
   * upload speed in Byte/s
   */
  speed: number;
}

export type FileUploadStatus = 'pending' | 'uploading' | 'processing' | 'success' | 'error';

export type FileProcessStatus = 'pending' | 'chunking' | 'embedding' | 'success' | 'error';

export const UPLOAD_STATUS_SET = new Set(['uploading', 'pending', 'processing']);

// the file that is upload at chat page
export interface UploadFileItem {
  /**
   * base64 data, it will use in other data
   */
  base64Url?: string;
  file: File;
  /**
   * the file url after upload,it will be s3 url
   * if enable the S3 storage, or the data is same as base64Url
   */
  fileUrl?: string;
  id: string;
  /**
   * blob url for local preview
   * it will use in the file preview before send the message
   */
  previewUrl?: string;
  status: FileUploadStatus;
  tasks?: FileParsingTask;
  uploadState?: FileUploadState;
}

export const FileMetadataSchema = z.object({
  date: z.string(),
  dirname: z.string(),
  filename: z.string(),
  path: z.string(),
});

export type FileMetadata = z.infer<typeof FileMetadataSchema>;

export const UploadFileSchema = z.object({
  /**
   * file type
   * @example 'image/png'
   */
  fileType: z.string(),
  // TODO: Need be required
  hash: z.string().optional(),

  knowledgeBaseId: z.string().optional(),

  metadata: z.any().optional(),

  /**
   * file name
   * @example 'test.png'
   */
  name: z.string(),

  /**
   * the mode database save the file
   * local mean save the raw file into data
   * url mean upload the file to a cdn and then save the url
   */
  /**
   * file size
   */
  size: z.number(),
  /**
   * file url if saveMode is url
   */
  url: z.string().optional(),
});

export type UploadFileParams = z.infer<typeof UploadFileSchema>;

export interface CheckFileHashResult {
  fileType?: string;
  isExist: boolean;
  metadata?: unknown;
  size?: number;
  url?: string;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/files/list.ts
================================================================================

import { z } from 'zod';

import { AsyncTaskStatus } from '@/types/asyncTask';

export interface FileListItem {
  chunkCount: number | null;
  chunkingError: any | null;
  chunkingStatus?: AsyncTaskStatus | null;
  createdAt: Date;
  embeddingError: any | null;
  embeddingStatus?: AsyncTaskStatus | null;
  fileType: string;
  finishEmbedding: boolean;
  id: string;
  name: string;
  size: number;
  updatedAt: Date;
  url: string;
}

export enum SortType {
  Asc = 'asc',
  Desc = 'desc',
}

export const QueryFileListSchema = z.object({
  category: z.string().optional(),
  knowledgeBaseId: z.string().optional(),
  q: z.string().nullable().optional(),
  showFilesInKnowledgeBase: z.boolean().default(false),
  sortType: z.enum(['desc', 'asc']).optional(),
  sorter: z.enum(['createdAt', 'size']).optional(),
});

export type QueryFileListSchemaType = z.infer<typeof QueryFileListSchema>;

export interface QueryFileListParams {
  category?: string;
  knowledgeBaseId?: string;
  q?: string | null;
  showFilesInKnowledgeBase?: boolean;
  sortType?: string;
  sorter?: string;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/eval/ragas.ts
================================================================================

/**
 * The data set for the RAGAS benchmark
 */
export interface RAGASDataSetItem {
  answer: string;
  context: string;
  ground_truth: string;
  question: string;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/eval/dataset.ts
================================================================================

import { z } from 'zod';

export interface EvalDatasetRecordRefFile {
  fileType: string;
  id: string;
  name: string;
}
export interface EvalDatasetRecord {
  createdAt: Date;
  id: number;
  ideal?: string | null;

  metadata: any;

  question?: string | null;

  /**
   * The reference files for the question
   */
  referenceFiles?: EvalDatasetRecordRefFile[] | null;
}

export const insertEvalDatasetRecordSchema = z.object({
  ideal: z.string().optional(),

  question: z.string(),

  referenceFiles: z.string().or(z.array(z.string())).optional(),
});

export type InsertEvalDatasetRecord = z.infer<typeof insertEvalDatasetRecordSchema>;

export interface RAGEvalDataSetItem {
  createdAt: Date;
  description?: string | null;
  id: number;
  name: string;
  updatedAt: Date;
}

export const insertEvalDatasetsSchema = z.object({
  description: z.string().optional(),
  knowledgeBaseId: z.string(),
  name: z.string(),
});

export type CreateNewEvalDatasets = z.infer<typeof insertEvalDatasetsSchema>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/eval/evaluation.ts
================================================================================

import { z } from 'zod';

export enum EvalEvaluationStatus {
  Error = 'Error',
  Pending = 'Pending',
  Processing = 'Processing',
  Success = 'Success',
}

export interface EvaluationRecord {
  answer: string;
  context: string[];
  createdAt: Date;
  id: number;
  ideal: string;
  question: string;
}

export const insertEvaluationSchema = z.object({
  ideal: z.string().optional(),

  question: z.string(),

  referenceFiles: z.string().or(z.array(z.string())).optional(),
});

export type InsertEvaluationRecord = z.infer<typeof insertEvaluationSchema>;

export interface RAGEvalEvaluationItem {
  createdAt: Date;
  dataset: {
    id: number;
    name: string;
  };
  evalRecordsUrl?: string;
  id: number;
  name: string;
  recordsStats: {
    success: number;
    total: number;
  };
  status: EvalEvaluationStatus;
  updatedAt: Date;
}

export const insertEvalEvaluationSchema = z.object({
  datasetId: z.number(),
  description: z.string().optional(),
  knowledgeBaseId: z.string(),
  name: z.string(),
});

export type CreateNewEvalEvaluation = z.infer<typeof insertEvalEvaluationSchema>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/eval/index.ts
================================================================================

export * from './dataset';
export * from './evaluation';
export * from './ragas';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/openai/image.ts
================================================================================

import { DallEImageQuality, DallEImageSize, DallEImageStyle } from '@/types/tool/dalle';

export interface OpenAIImagePayload {
  model: 'dall-e-2' | 'dall-e-3';
  /**
   * The number of images to generate. Must be between 1 and 10.
   */
  n?: number;
  /**
   * A text description of the desired image(s).
   * The maximum length is 1000 characters.
   */
  prompt: string;
  /**
   * The quality of the image that will be generated.
   * hd creates images with finer details and greater consistency across the image.
   * This param is only supported for dall-e-3.
   */
  quality?: DallEImageQuality;
  /**
   * The size of the generated images.
   * Must be one of '1792x1024' , '1024x1024' , '1024x1792'
   */
  size?: DallEImageSize;

  /**
   * The style of the generated images. Must be one of vivid or natural.
   * Vivid causes the model to lean towards generating hyper-real and dramatic images.
   * Natural causes the model to produce more natural, less hyper-real looking images.
   * This param is only supported for dall-e-3.
   * @default vivid
   */
  style?: DallEImageStyle;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/openai/functionCall.ts
================================================================================

export interface OpenAIFunctionCall {
  arguments: string;
  name: string;
}

export interface OpenAIToolCall {
  function: OpenAIFunctionCall;
  id: string;
  type: 'function';
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/openai/plugin.ts
================================================================================

type HttpAuthorizationType = 'bearer' | 'basic';

interface BaseManifestAuth {
  instructions: string;
  type: string;
}

interface ManifestNoAuth extends BaseManifestAuth {
  type: 'none';
}

interface ManifestServiceHttpAuth extends BaseManifestAuth {
  authorization_type: HttpAuthorizationType;
  type: 'service_http';
  verification_tokens: {
    [service: string]: string;
  };
}

interface ManifestUserHttpAuth extends BaseManifestAuth {
  authorization_type: HttpAuthorizationType;
  type: 'user_http';
}

interface ManifestOAuthAuth extends BaseManifestAuth {
  authorization_content_type: string;
  authorization_url: string;
  client_url: string;
  scope: string;
  type: 'oauth';
  verification_tokens: {
    [service: string]: string;
  };
}

type ManifestAuth =
  | ManifestNoAuth
  | ManifestServiceHttpAuth
  | ManifestUserHttpAuth
  | ManifestOAuthAuth;

export interface OpenAIPluginManifest {
  api: {
    type: string;
    url: string;
  };
  auth: ManifestAuth;
  contact_email: string;
  description_for_human: string;
  description_for_model: string;
  legal_info_url: string;
  logo_url: string;
  name_for_human: string;
  name_for_model: string;
  schema_version: string;
  // 其他可能的字段...
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/types/openai/chat.ts
================================================================================

import { LLMRoleType } from '@/types/llm';
import { MessageToolCall } from '@/types/message';

import { OpenAIFunctionCall } from './functionCall';

interface UserMessageContentPartText {
  text: string;
  type: 'text';
}
interface UserMessageContentPartImage {
  image_url: {
    detail?: 'auto' | 'low' | 'high';
    url: string;
  };
  type: 'image_url';
}

export type UserMessageContentPart = UserMessageContentPartText | UserMessageContentPartImage;

export interface OpenAIChatMessage {
  /**
   * @title 内容
   * @description 消息内容
   */
  content: string | UserMessageContentPart[];

  /**
   * @deprecated
   */
  function_call?: OpenAIFunctionCall;
  name?: string;
  /**
   * 角色
   * @description 消息发送者的角色
   */
  role: LLMRoleType;
  tool_call_id?: string;
  tool_calls?: MessageToolCall[];
}

/**
 * @title Chat Stream Payload
 */
export interface ChatStreamPayload {
  /**
   * @title 控制生成文本中的惩罚系数，用于减少重复性
   * @default 0
   */
  frequency_penalty?: number;
  /**
   * @title 生成文本的最大长度
   */
  max_tokens?: number;
  /**
   * @title 聊天信息列表
   */
  messages: OpenAIChatMessage[];
  /**
   * @title 模型名称
   */
  model: string;
  /**
   * @title 返回的文本数量
   */
  n?: number;
  /**
   * 开启的插件列表
   */
  plugins?: string[];
  /**
   * @title 控制生成文本中的惩罚系数，用于减少主题的变化
   * @default 0
   */
  presence_penalty?: number;
  /**
   * @default openai
   */
  provider?: string;
  /**
   * @title 是否开启流式请求
   * @default true
   */
  stream?: boolean;
  /**
   * @title 生成文本的随机度量，用于控制文本的创造性和多样性
   * @default 1
   */
  temperature: number;
  tool_choice?: string;
  tools?: ChatCompletionTool[];
  /**
   * @title 控制生成文本中最高概率的单个令牌
   * @default 1
   */
  top_p?: number;
}

export interface ChatCompletionFunctions {
  /**
   * The description of what the function does.
   * @type {string}
   * @memberof ChatCompletionFunctions
   */
  description?: string;
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   * @type {string}
   * @memberof ChatCompletionFunctions
   */
  name: string;
  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
   * @type {{ [key: string]: any }}
   * @memberof ChatCompletionFunctions
   */
  parameters?: {
    [key: string]: any;
  };
}

export interface ChatCompletionTool {
  function: ChatCompletionFunctions;

  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function';
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/robots.tsx
================================================================================

import { MetadataRoute } from 'next';

import { sitemapModule } from '@/server/sitemap';
import { getCanonicalUrl } from '@/server/utils/url';

const robots = (): MetadataRoute.Robots => {
  return {
    host: getCanonicalUrl(),
    rules: [
      {
        allow: ['/discover/*'],
        disallow: ['/discover/search/*'],
        userAgent: ['Facebot', 'facebookexternalhit'],
      },
      {
        allow: ['/discover/*'],
        disallow: ['/discover/search/*'],
        userAgent: 'LinkedInBot',
      },
      {
        allow: ['/discover/*'],
        disallow: ['/discover/search/*'],
        userAgent: 'Twitterbot',
      },
      {
        allow: ['/'],
        disallow: ['/api/*', '/login', '/signup', '/files', '/repos/*', '/discover/search/*'],
        userAgent: '*',
      },
    ],
    sitemap: sitemapModule.getRobots(),
  };
};

export default robots;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/@category/default.tsx
================================================================================

import { Suspense } from 'react';

import SkeletonLoading from '@/components/Loading/SkeletonLoading';

import UpgradeAlert from '../features/UpgradeAlert';
import CategoryContent from './features/CategoryContent';

const Category = () => {
  return (
    <Suspense fallback={<SkeletonLoading paragraph={{ rows: 7 }} title={false} />}>
      <CategoryContent />
      <UpgradeAlert />
    </Suspense>
  );
};

Category.displayName = 'SettingCategory';

export default Category;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/llm/const.ts
================================================================================

export const LLMProviderConfigKey = 'languageModel';
export const KeyVaultsConfigKey = 'keyVaults';

/**
 * we use this key to define default api key
 * equal GOOGLE_API_KEY or ZHIPU_API_KEY
 */
export const LLMProviderApiTokenKey = 'apiKey';

/**
 * we use this key to define the baseURL
 * equal OPENAI_PROXY_URL
 */
export const LLMProviderBaseUrlKey = 'baseURL';

/**
 * we use this key to define the custom model name
 * equal CUSTOM_MODELS
 */
export const LLMProviderModelListKey = 'enabledModels';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/llm/ProviderList/providers.tsx
================================================================================

import { useMemo } from 'react';

import {
  Ai21ProviderCard,
  Ai360ProviderCard,
  AnthropicProviderCard,
  BaichuanProviderCard,
  DeepSeekProviderCard,
  FireworksAIProviderCard,
  GiteeAIProviderCard,
  GoogleProviderCard,
  GroqProviderCard,
  HigressProviderCard,
  HunyuanProviderCard,
  InternLMProviderCard,
  MinimaxProviderCard,
  MistralProviderCard,
  MoonshotProviderCard,
  NovitaProviderCard,
  OpenRouterProviderCard,
  PerplexityProviderCard,
  QwenProviderCard,
  SenseNovaProviderCard,
  SiliconCloudProviderCard,
  SparkProviderCard,
  StepfunProviderCard,
  TaichuProviderCard,
  TogetherAIProviderCard,
  UpstageProviderCard,
  XAIProviderCard,
  ZeroOneProviderCard,
  ZhiPuProviderCard,
} from '@/config/modelProviders';

import { ProviderItem } from '../type';
import { useAzureProvider } from './Azure';
import { useBedrockProvider } from './Bedrock';
import { useCloudflareProvider } from './Cloudflare';
import { useGithubProvider } from './Github';
import { useHuggingFaceProvider } from './HuggingFace';
import { useOllamaProvider } from './Ollama';
import { useOpenAIProvider } from './OpenAI';
import { useWenxinProvider } from './Wenxin';

export const useProviderList = (): ProviderItem[] => {
  const AzureProvider = useAzureProvider();
  const OllamaProvider = useOllamaProvider();
  const OpenAIProvider = useOpenAIProvider();
  const BedrockProvider = useBedrockProvider();
  const CloudflareProvider = useCloudflareProvider();
  const GithubProvider = useGithubProvider();
  const HuggingFaceProvider = useHuggingFaceProvider();
  const WenxinProvider = useWenxinProvider();

  return useMemo(
    () => [
      OpenAIProvider,
      AzureProvider,
      OllamaProvider,
      AnthropicProviderCard,
      BedrockProvider,
      GoogleProviderCard,
      DeepSeekProviderCard,
      HuggingFaceProvider,
      OpenRouterProviderCard,
      CloudflareProvider,
      GithubProvider,
      NovitaProviderCard,
      TogetherAIProviderCard,
      FireworksAIProviderCard,
      GroqProviderCard,
      PerplexityProviderCard,
      MistralProviderCard,
      Ai21ProviderCard,
      UpstageProviderCard,
      XAIProviderCard,
      QwenProviderCard,
      WenxinProvider,
      HunyuanProviderCard,
      SparkProviderCard,
      ZhiPuProviderCard,
      ZeroOneProviderCard,
      SenseNovaProviderCard,
      StepfunProviderCard,
      MoonshotProviderCard,
      BaichuanProviderCard,
      MinimaxProviderCard,
      Ai360ProviderCard,
      TaichuProviderCard,
      InternLMProviderCard,
      SiliconCloudProviderCard,
      HigressProviderCard,
      GiteeAIProviderCard,
    ],
    [
      AzureProvider,
      OllamaProvider,
      OpenAIProvider,
      BedrockProvider,
      CloudflareProvider,
      GithubProvider,
      WenxinProvider,
      HuggingFaceProvider,
    ],
  );
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/llm/ProviderList/Github/index.tsx
================================================================================

'use client';

import { Markdown } from '@lobehub/ui';
import { Input } from 'antd';
import { createStyles } from 'antd-style';
import { useTranslation } from 'react-i18next';

import { GithubProviderCard } from '@/config/modelProviders';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import { KeyVaultsConfigKey, LLMProviderApiTokenKey } from '../../const';
import { ProviderItem } from '../../type';

const useStyles = createStyles(({ css, token }) => ({
  markdown: css`
    p {
      color: ${token.colorTextDescription} !important;
    }
  `,
  tip: css`
    font-size: 12px;
    color: ${token.colorTextDescription};
  `,
}));

const providerKey: GlobalLLMProviderKey = 'github';

// Same as OpenAIProvider, but replace API Key with Github Personal Access Token
export const useGithubProvider = (): ProviderItem => {
  const { t } = useTranslation('modelProvider');
  const { styles } = useStyles();

  return {
    ...GithubProviderCard,
    apiKeyItems: [
      {
        children: (
          <Input.Password
            autoComplete={'new-password'}
            placeholder={t(`${providerKey}.personalAccessToken.placeholder`)}
          />
        ),
        desc: (
          <Markdown className={styles.markdown} fontSize={12} variant={'chat'}>
            {t(`${providerKey}.personalAccessToken.desc`)}
          </Markdown>
        ),
        label: t(`${providerKey}.personalAccessToken.title`),
        name: [KeyVaultsConfigKey, providerKey, LLMProviderApiTokenKey],
      },
    ],
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/llm/ProviderList/HuggingFace/index.tsx
================================================================================

'use client';

import { Markdown } from '@lobehub/ui';
import { Input } from 'antd';
import { createStyles } from 'antd-style';
import { useTranslation } from 'react-i18next';

import { HuggingFaceProviderCard } from '@/config/modelProviders';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import { KeyVaultsConfigKey, LLMProviderApiTokenKey } from '../../const';
import { ProviderItem } from '../../type';

const useStyles = createStyles(({ css, token }) => ({
  markdown: css`
    p {
      color: ${token.colorTextDescription} !important;
    }
  `,
  tip: css`
    font-size: 12px;
    color: ${token.colorTextDescription};
  `,
}));

const providerKey: GlobalLLMProviderKey = 'huggingface';

// Same as OpenAIProvider, but replace API Key with HuggingFace Access Token
export const useHuggingFaceProvider = (): ProviderItem => {
  const { t } = useTranslation('modelProvider');
  const { styles } = useStyles();

  return {
    ...HuggingFaceProviderCard,
    apiKeyItems: [
      {
        children: (
          <Input.Password
            autoComplete={'new-password'}
            placeholder={t(`${providerKey}.accessToken.placeholder`)}
          />
        ),
        desc: (
          <Markdown className={styles.markdown} fontSize={12} variant={'chat'}>
            {t(`${providerKey}.accessToken.desc`)}
          </Markdown>
        ),
        label: t(`${providerKey}.accessToken.title`),
        name: [KeyVaultsConfigKey, providerKey, LLMProviderApiTokenKey],
      },
    ],
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/llm/ProviderList/OpenAI/index.tsx
================================================================================

'use client';

import { OpenAIProviderCard } from '@/config/modelProviders';
import { featureFlagsSelectors, useServerConfigStore } from '@/store/serverConfig';

import { ProviderItem } from '../../type';

export const useOpenAIProvider = (): ProviderItem => {
  const { showOpenAIProxyUrl, showOpenAIApiKey } = useServerConfigStore(featureFlagsSelectors);
  return {
    ...OpenAIProviderCard,
    proxyUrl: showOpenAIProxyUrl && {
      placeholder: 'https://api.openai.com/v1',
    },
    showApiKey: showOpenAIApiKey,
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/tts/index.tsx
================================================================================

import OpenAI from './features/OpenAI';
import STT from './features/STT';

const Page = () => {
  return (
    <>
      <STT />
      <OpenAI />
    </>
  );
};

Page.displayName = 'TtsSetting';

export default Page;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/tts/features/OpenAI.tsx
================================================================================

'use client';

import { Form, type ItemGroup } from '@lobehub/ui';
import { Select } from 'antd';
import isEqual from 'fast-deep-equal';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';

import { FORM_STYLE } from '@/const/layoutTokens';
import { useUserStore } from '@/store/user';
import { settingsSelectors } from '@/store/user/selectors';

import { opeanaiSTTOptions, opeanaiTTSOptions } from './const';

type SettingItemGroup = ItemGroup;

const TTS_SETTING_KEY = 'tts';

const OpenAI = memo(() => {
  const { t } = useTranslation('setting');
  const [form] = Form.useForm();
  const settings = useUserStore(settingsSelectors.currentSettings, isEqual);
  const [setSettings] = useUserStore((s) => [s.setSettings]);

  const openai: SettingItemGroup = {
    children: [
      {
        children: <Select options={opeanaiTTSOptions} />,
        label: t('settingTTS.openai.ttsModel'),
        name: [TTS_SETTING_KEY, 'openAI', 'ttsModel'],
      },
      {
        children: <Select options={opeanaiSTTOptions} />,
        label: t('settingTTS.openai.sttModel'),
        name: [TTS_SETTING_KEY, 'openAI', 'sttModel'],
      },
    ],
    title: t('settingTTS.openai.title'),
  };

  return (
    <Form
      form={form}
      initialValues={settings}
      items={[openai]}
      itemsType={'group'}
      onValuesChange={setSettings}
      variant={'pure'}
      {...FORM_STYLE}
    />
  );
});

export default OpenAI;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/settings/tts/features/const.tsx
================================================================================

import { OpenAI } from '@lobehub/icons';
import { SelectProps } from 'antd';

import { LabelRenderer } from '@/components/ModelSelect';

export const opeanaiTTSOptions: SelectProps['options'] = [
  {
    label: <LabelRenderer Icon={OpenAI.Avatar} label={'tts-1'} />,
    value: 'tts-1',
  },
  {
    label: <LabelRenderer Icon={OpenAI.Avatar} label={'tts-1-hd'} />,
    value: 'tts-1-hd',
  },
];

export const opeanaiSTTOptions: SelectProps['options'] = [
  {
    label: <LabelRenderer Icon={OpenAI.Avatar} label={'whisper-1'} />,
    value: 'whisper-1',
  },
];

export const sttOptions: SelectProps['options'] = [
  {
    label: 'OpenAI',
    value: 'openai',
  },
  {
    label: 'Browser',
    value: 'browser',
  },
];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/layout.tsx
================================================================================

import { notFound } from 'next/navigation';
import { ReactNode } from 'react';
import { Flexbox } from 'react-layout-kit';

import { serverFeatureFlags } from '@/config/featureFlags';

import Container from './components/Container';
import { Tabs } from './components/Tabs';

interface LayoutProps {
  children: ReactNode;
  params: Promise<{ id: string }>;
}

const Layout = async (props: LayoutProps) => {
  const enableRAGEval = serverFeatureFlags().enableRAGEval;
  const params = await props.params;

  if (!enableRAGEval) return notFound();

  return (
    <Flexbox gap={24} height={'100%'} padding={24} style={{ paddingTop: 0 }}>
      <Tabs knowledgeBaseId={params.id} />
      <Container>{props.children}</Container>
    </Flexbox>
  );
};

export default Layout;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/CreateDataset/CreateForm.tsx
================================================================================

import { Button, Form, Input } from 'antd';
import { css, cx } from 'antd-style';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { useKnowledgeBaseStore } from '@/store/knowledgeBase';
import { CreateNewEvalDatasets } from '@/types/eval';

const formItem = css`
  display: flex;
  flex-direction: column;
  gap: 12px;

  .ant-form-item {
    margin-block-end: 0;
  }
`;

interface CreateFormProps {
  knowledgeBaseId: string;
  onClose?: () => void;
}

const CreateForm = memo<CreateFormProps>(({ onClose, knowledgeBaseId }) => {
  const { t } = useTranslation('ragEval');
  const [loading, setLoading] = useState(false);
  const createNewDataset = useKnowledgeBaseStore((s) => s.createNewDataset);

  const onFinish = async (values: CreateNewEvalDatasets) => {
    setLoading(true);

    try {
      await createNewDataset({ ...values, knowledgeBaseId });
      setLoading(false);
      onClose?.();
    } catch (e) {
      console.error(e);
      setLoading(false);
    }
  };

  return (
    <Flexbox gap={8}>
      <Form className={cx(formItem)} onFinish={onFinish}>
        <Form.Item
          name={'name'}
          rules={[{ message: t('addDataset.name.required'), required: true }]}
        >
          <Input autoFocus placeholder={t('addDataset.name.placeholder')} />
        </Form.Item>
        <Form.Item name={'description'}>
          <Input.TextArea
            placeholder={t('addDataset.description.placeholder')}
            style={{ minHeight: 120 }}
          />
        </Form.Item>
        <Button
          block
          htmlType={'submit'}
          loading={loading}
          style={{ marginTop: 16 }}
          type={'primary'}
        >
          {t('addDataset.confirm')}
        </Button>
      </Form>
    </Flexbox>
  );
});

export default CreateForm;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/CreateDataset/index.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { SheetIcon } from 'lucide-react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { createModal } from '@/components/FunctionModal';

import CreateForm from './CreateForm';

const Title = () => {
  const { t } = useTranslation('ragEval');

  return (
    <Flexbox gap={8} horizontal>
      <Icon icon={SheetIcon} />
      {t('addDataset.title')}
    </Flexbox>
  );
};

interface CreateDatasetModalProps {
  knowledgeBaseId: string;
}

export const useCreateDatasetModal = createModal<CreateDatasetModalProps>((instance, params) => ({
  content: (
    <Flexbox paddingInline={16} style={{ marginBlock: 24 }}>
      <CreateForm
        knowledgeBaseId={params!.knowledgeBaseId}
        onClose={() => {
          instance.current?.destroy();
        }}
      />
    </Flexbox>
  ),
  title: <Title />,
}));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/DatasetDetail/index.tsx
================================================================================

'use client';

import { ProColumns, ProTable } from '@ant-design/pro-components';
import { ActionIcon } from '@lobehub/ui';
import { Button, Typography, Upload } from 'antd';
import { createStyles } from 'antd-style';
import { Edit2Icon, Trash2Icon } from 'lucide-react';
import { parseAsInteger, useQueryState } from 'nuqs';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import FileIcon from '@/components/FileIcon';
import { ragEvalService } from '@/services/ragEval';
import { useKnowledgeBaseStore } from '@/store/knowledgeBase';
import { EvalDatasetRecordRefFile } from '@/types/eval';

const createRequest = (activeDatasetId: number) => async () => {
  const records = await ragEvalService.getDatasetRecords(activeDatasetId);

  return {
    data: records,
    success: true,
    total: records.length,
  };
};

const useStyles = createStyles(({ css }) => ({
  container: css`
    padding-block: 0;
    padding-inline: 12px;
  `,
  icon: css`
    min-width: 24px;
    border-radius: 4px;
  `,
  title: css`
    font-size: 16px;
  `,
}));

const DatasetDetail = () => {
  const { t } = useTranslation(['ragEval', 'common']);
  const { styles } = useStyles();
  const [importDataset] = useKnowledgeBaseStore((s) => [s.importDataset]);

  const [activeDatasetId] = useQueryState('id', parseAsInteger);

  const columns: ProColumns[] = [
    {
      dataIndex: 'question',
      ellipsis: true,
      title: t('dataset.list.table.columns.question.title'),
      width: '40%',
    },
    { dataIndex: 'ideal', ellipsis: true, title: t('dataset.list.table.columns.ideal.title') },
    {
      dataIndex: 'referenceFiles',
      render: (dom, entity) => {
        const referenceFiles = entity.referenceFiles as EvalDatasetRecordRefFile[];

        return (
          !!referenceFiles && (
            <Flexbox>
              {referenceFiles?.map((file) => (
                <Flexbox gap={4} horizontal key={file.id}>
                  <FileIcon fileName={file.name} fileType={file.fileType} size={20} />
                  <Typography.Text ellipsis={{ tooltip: true }}>{file.name}</Typography.Text>
                </Flexbox>
              ))}
            </Flexbox>
          )
        );
      },
      title: t('dataset.list.table.columns.referenceFiles.title'),
      width: 200,
    },
    {
      dataIndex: 'actions',
      render: () => (
        <Flexbox gap={4} horizontal>
          <ActionIcon icon={Edit2Icon} size={'small'} title={t('edit', { ns: 'common' })} />
          <ActionIcon icon={Trash2Icon} size={'small'} title={t('delete', { ns: 'common' })} />
        </Flexbox>
      ),
      title: t('dataset.list.table.columns.actions'),

      width: 80,
    },
  ];

  const request = !!activeDatasetId ? createRequest(activeDatasetId) : undefined;

  return !activeDatasetId ? (
    <Center height={'100%'} width={'100%'}>
      {t('dataset.list.table.notSelected')}
    </Center>
  ) : (
    <Flexbox className={styles.container} gap={24}>
      <ProTable
        columns={columns}
        request={request}
        search={false}
        size={'small'}
        toolbar={{
          actions: [
            <Upload
              beforeUpload={async (file) => {
                await importDataset(file, activeDatasetId);

                return false;
              }}
              key={'upload'}
              multiple={false}
              showUploadList={false}
            >
              <Button type={'primary'}>{t('dataset.list.table.actions.importData')}</Button>
            </Upload>,
          ],
          title: <div className={styles.title}>{t('dataset.list.table.title')}</div>,
        }}
      />
    </Flexbox>
  );
};

export default DatasetDetail;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/DatasetList/index.tsx
================================================================================

'use client';

import { ActionIcon } from '@lobehub/ui';
import { PlusIcon } from 'lucide-react';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';
import { Virtuoso } from 'react-virtuoso';

import { RAGEvalDataSetItem } from '@/types/eval';

import Item from './Item';

interface DatasetListProps {
  dataSource: RAGEvalDataSetItem[];
}

const DatasetList = memo<DatasetListProps>(({ dataSource }) => {
  const { t } = useTranslation('ragEval');

  return (
    <Flexbox gap={24} height={'100%'}>
      <Flexbox align={'center'} horizontal justify={'space-between'}>
        <span>{t('dataset.list.title')}</span>
        <ActionIcon icon={PlusIcon} size={'small'} />
      </Flexbox>
      <Virtuoso data={dataSource} itemContent={(index, data) => <Item {...data} key={data.id} />} />
    </Flexbox>
  );
});

export default DatasetList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/DatasetList/Item.tsx
================================================================================

import { createStyles } from 'antd-style';
import { parseAsInteger, useQueryState } from 'nuqs';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { RAGEvalDataSetItem } from '@/types/eval';

const useStyles = createStyles(({ css, token }) => ({
  active: css`
    background: ${token.colorFillTertiary};

    &:hover {
      background-color: ${token.colorFillSecondary};
    }
  `,
  container: css`
    cursor: pointer;

    margin-block-end: 2px;
    padding-block: 12px;
    padding-inline: 8px;

    border-radius: 8px;

    &:hover {
      background-color: ${token.colorFillTertiary};
    }
  `,
  icon: css`
    min-width: 24px;
    border-radius: 4px;
  `,
  title: css`
    text-align: start;
  `,
}));

const Item = memo<RAGEvalDataSetItem>(({ name, description, id }) => {
  const { styles, cx } = useStyles();

  const [activeDatasetId, activateDataset] = useQueryState('id', parseAsInteger);

  const isActive = activeDatasetId === id;
  return (
    <Flexbox
      className={cx(styles.container, isActive && styles.active)}
      onClick={() => {
        if (!isActive) {
          activateDataset(id);
        }
      }}
    >
      <div className={styles.title}>{name}</div>
      {description && <div>{description}</div>}
    </Flexbox>
  );
});

export default Item;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/dataset/EmptyGuide/index.tsx
================================================================================

'use client';

import { Button } from 'antd';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import { useCreateDatasetModal } from '../CreateDataset';

interface EmptyGuideProps {
  knowledgeBaseId: string;
}

const EmptyGuide = memo<EmptyGuideProps>(({ knowledgeBaseId }) => {
  const { t } = useTranslation('ragEval');
  const modal = useCreateDatasetModal();
  return (
    <Center gap={24} height={'100%'} width={'100%'}>
      <div>{t('dataset.emptyGuide')}</div>
      <Flexbox gap={8} horizontal>
        <Button
          onClick={() => {
            modal.open({ knowledgeBaseId });
          }}
          type={'primary'}
        >
          {t('dataset.addNewButton')}
        </Button>
      </Flexbox>
    </Center>
  );
});
export default EmptyGuide;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/evaluation/CreateEvaluation/CreateForm.tsx
================================================================================

import { Button, Form, Input, Select } from 'antd';
import { css, cx } from 'antd-style';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { useKnowledgeBaseStore } from '@/store/knowledgeBase';
import { CreateNewEvalEvaluation } from '@/types/eval';

const formItem = css`
  display: flex;
  flex-direction: column;
  gap: 12px;

  .ant-form-item {
    margin-block-end: 0;
  }
`;

interface CreateFormProps {
  knowledgeBaseId: string;
  onClose?: () => void;
  onCreate?: () => void;
}

const CreateForm = memo<CreateFormProps>(({ onClose, onCreate, knowledgeBaseId }) => {
  const { t } = useTranslation('ragEval');

  const [loading, setLoading] = useState(false);
  const [useFetchDatasets, createNewEvaluation] = useKnowledgeBaseStore((s) => [
    s.useFetchDatasets,
    s.createNewEvaluation,
  ]);

  const { data, isLoading } = useFetchDatasets(knowledgeBaseId);

  const onFinish = async (values: CreateNewEvalEvaluation) => {
    setLoading(true);

    try {
      await createNewEvaluation({ ...values, knowledgeBaseId });
      setLoading(false);
      onCreate?.();
      onClose?.();
    } catch (e) {
      console.error(e);
      setLoading(false);
    }
  };

  return (
    <Flexbox gap={8}>
      <Form className={cx(formItem)} onFinish={onFinish}>
        <Form.Item
          name={'name'}
          rules={[{ message: t('evaluation.addEvaluation.name.required'), required: true }]}
        >
          <Input autoFocus placeholder={t('evaluation.addEvaluation.name.placeholder')} />
        </Form.Item>
        <Form.Item name={'description'}>
          <Input.TextArea
            placeholder={t('evaluation.addEvaluation.description.placeholder')}
            style={{ minHeight: 120 }}
          />
        </Form.Item>
        <Form.Item
          name={'datasetId'}
          rules={[{ message: t('evaluation.addEvaluation.datasetId.required'), required: true }]}
        >
          <Select
            loading={isLoading}
            options={data?.map((item) => ({
              label: item.name,
              value: item.id,
            }))}
            placeholder={t('evaluation.addEvaluation.datasetId.placeholder')}
          />
        </Form.Item>
        <Button
          block
          htmlType={'submit'}
          loading={loading}
          style={{ marginTop: 16 }}
          type={'primary'}
        >
          {t('evaluation.addEvaluation.confirm')}
        </Button>
      </Form>
    </Flexbox>
  );
});

export default CreateForm;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/evaluation/CreateEvaluation/index.tsx
================================================================================

'use client';

import { Button } from 'antd';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';

import { useCreateDatasetModal } from '../CreateEvaluation/useModal';

interface CreateEvaluationProps {
  knowledgeBaseId: string;
  onCreate?: () => void;
}

const CreateEvaluation = memo<CreateEvaluationProps>(({ knowledgeBaseId, onCreate }) => {
  const { t } = useTranslation('ragEval');
  const modal = useCreateDatasetModal();
  return (
    <Button
      onClick={() => {
        modal.open({ knowledgeBaseId, onCreate });
      }}
      type={'primary'}
    >
      {t('evaluation.addNewButton')}
    </Button>
  );
});
export default CreateEvaluation;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/evaluation/CreateEvaluation/useModal.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { SheetIcon } from 'lucide-react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { createModal } from '@/components/FunctionModal';

import CreateForm from './CreateForm';

const Title = () => {
  const { t } = useTranslation('ragEval');

  return (
    <Flexbox gap={8} horizontal>
      <Icon icon={SheetIcon} />
      {t('addDataset.title')}
    </Flexbox>
  );
};

interface CreateDatasetModalProps {
  knowledgeBaseId: string;
  onCreate?: () => void;
}

export const useCreateDatasetModal = createModal<CreateDatasetModalProps>((instance, params) => ({
  content: (
    <Flexbox paddingInline={16} style={{ marginBlock: 24 }}>
      <CreateForm
        knowledgeBaseId={params!.knowledgeBaseId}
        onClose={() => {
          instance.current?.destroy();
        }}
        onCreate={params?.onCreate}
      />
    </Flexbox>
  ),
  title: <Title />,
}));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/evaluation/EvaluationList/index.tsx
================================================================================

'use client';

import { ActionType, ProColumns, ProTable } from '@ant-design/pro-components';
import { ActionIcon, Icon } from '@lobehub/ui';
import { App, Button, ButtonProps, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { DownloadIcon, PlayIcon, RotateCcwIcon, Trash2Icon } from 'lucide-react';
import { useRef, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { ragEvalService } from '@/services/ragEval';
import { useKnowledgeBaseStore } from '@/store/knowledgeBase';
import { EvalEvaluationStatus, RAGEvalEvaluationItem } from '@/types/eval';

import CreateEvaluationButton from '../CreateEvaluation';

const createRequest = (knowledgeBaseId: string) => async () => {
  const records = await ragEvalService.getEvaluationList(knowledgeBaseId);

  return {
    data: records,
    success: true,
    total: records.length,
  };
};

const useStyles = createStyles(({ css }) => ({
  icon: css`
    min-width: 24px;
    border-radius: 4px;
  `,
  title: css`
    font-size: 16px;
  `,
}));

const EvaluationList = ({ knowledgeBaseId }: { knowledgeBaseId: string }) => {
  const { t } = useTranslation(['ragEval', 'common']);
  const { styles } = useStyles();
  const [removeEvaluation, runEvaluation, checkEvaluationStatus] = useKnowledgeBaseStore((s) => [
    s.removeEvaluation,
    s.runEvaluation,
    s.checkEvaluationStatus,
  ]);
  const [isCheckingStatus, setCheckingStatus] = useState(false);
  const { modal } = App.useApp();
  const actionRef = useRef<ActionType>();

  const columns: ProColumns<RAGEvalEvaluationItem>[] = [
    {
      dataIndex: 'name',
      ellipsis: true,
      title: t('evaluation.table.columns.name.title'),
    },
    {
      dataIndex: ['dataset', 'id'],
      render: (dom, entity) => {
        return (
          <Typography.Link
            href={`/repos/${knowledgeBaseId}/evals/dataset?id=${entity.dataset.id}`}
            style={{ color: 'initial' }}
            target={'_blank'}
          >
            {entity.dataset.name}
          </Typography.Link>
        );
      },
      title: t('evaluation.table.columns.datasetId.title'),
      width: 200,
    },
    {
      dataIndex: 'status',
      title: t('evaluation.table.columns.status.title'),
      valueEnum: {
        [EvalEvaluationStatus.Error]: {
          status: 'error',
          text: t('evaluation.table.columns.status.error'),
        },
        [EvalEvaluationStatus.Processing]: {
          status: 'processing',
          text: t('evaluation.table.columns.status.processing'),
        },
        [EvalEvaluationStatus.Pending]: {
          status: 'default',
          text: t('evaluation.table.columns.status.pending'),
        },
        [EvalEvaluationStatus.Success]: {
          status: 'success',
          text: t('evaluation.table.columns.status.success'),
        },
      },
    },
    {
      dataIndex: ['recordsStats', 'total'],
      render: (dom, entity) => {
        return entity.status === 'Pending'
          ? entity.recordsStats.total
          : `${entity.recordsStats.success}/${entity.recordsStats.total}`;
      },
      title: t('evaluation.table.columns.records.title'),
    },
    {
      dataIndex: 'actions',
      render: (_, entity) => {
        const actionsMap: Record<EvalEvaluationStatus, ButtonProps> = {
          [EvalEvaluationStatus.Pending]: {
            children: t('evaluation.table.columns.actions.run'),
            icon: <Icon icon={PlayIcon} />,
            onClick: () => {
              modal.confirm({
                content: t('evaluation.table.columns.actions.confirmRun'),
                onOk: async () => {
                  await runEvaluation(entity.id);
                  await actionRef.current?.reload();
                },
              });
            },
          },
          [EvalEvaluationStatus.Error]: {
            children: t('evaluation.table.columns.actions.retry'),
            icon: <Icon icon={RotateCcwIcon} />,
            onClick: () => {
              modal.confirm({
                content: t('evaluation.table.columns.actions.confirmRun'),
                onOk: async () => {
                  await runEvaluation(entity.id);
                  await actionRef.current?.reload();
                },
              });
            },
          },
          [EvalEvaluationStatus.Processing]: {
            children: t('evaluation.table.columns.actions.checkStatus'),
            icon: null,
            loading: isCheckingStatus,
            onClick: async () => {
              setCheckingStatus(true);
              await checkEvaluationStatus(entity.id);
              setCheckingStatus(false);
              await actionRef.current?.reload();
            },
          },
          [EvalEvaluationStatus.Success]: {
            children: t('evaluation.table.columns.actions.downloadRecords'),
            icon: <Icon icon={DownloadIcon} />,
            onClick: async () => {
              window.open(entity.evalRecordsUrl);
            },
          },
        };

        const actionProps = actionsMap[entity.status];

        return (
          <Flexbox gap={4} horizontal>
            {!actionProps ? null : <Button {...actionProps} size={'small'} />}
            <ActionIcon
              icon={Trash2Icon}
              onClick={async () => {
                modal.confirm({
                  content: t('evaluation.table.columns.actions.confirmDelete'),
                  okButtonProps: {
                    danger: true,
                  },
                  onOk: async () => {
                    await removeEvaluation(entity.id);
                    await actionRef.current?.reload();
                  },
                });
              }}
              size={'small'}
              title={t('delete', { ns: 'common' })}
            />
          </Flexbox>
        );
      },
      title: t('evaluation.table.columns.actions.title'),
      width: 120,
    },
  ];

  const request = knowledgeBaseId ? createRequest(knowledgeBaseId) : undefined;

  return (
    <Flexbox gap={24}>
      <ProTable
        actionRef={actionRef}
        columns={columns}
        request={request}
        search={false}
        toolbar={{
          actions: [
            <CreateEvaluationButton
              key={'new'}
              knowledgeBaseId={knowledgeBaseId}
              onCreate={() => {
                actionRef.current?.reload();
              }}
            />,
          ],
          title: <div className={styles.title}>{t('evaluation.table.title')}</div>,
        }}
      />
    </Flexbox>
  );
};

export default EvaluationList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/evals/evaluation/EmptyGuide/index.tsx
================================================================================

'use client';

import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import CreateEvaluationButton from '../CreateEvaluation';

interface EmptyGuideProps {
  knowledgeBaseId: string;
}

const EmptyGuide = memo<EmptyGuideProps>(({ knowledgeBaseId }) => {
  const { t } = useTranslation('ragEval');

  return (
    <Center gap={24} height={'100%'} width={'100%'}>
      <div>{t('evaluation.emptyGuide')}</div>
      <Flexbox gap={8} horizontal>
        <CreateEvaluationButton knowledgeBaseId={knowledgeBaseId} />
      </Flexbox>
    </Center>
  );
});
export default EmptyGuide;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/repos/[id]/@menu/Menu/index.tsx
================================================================================

'use client';

import { Icon } from '@lobehub/ui';
import { FileText, GaugeIcon } from 'lucide-react';
import Link from 'next/link';
import { usePathname } from 'next/navigation';
import { memo, useMemo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import Menu from '@/components/Menu';
import type { MenuProps } from '@/components/Menu';
import { featureFlagsSelectors, useServerConfigStore } from '@/store/serverConfig';

const FileMenu = memo<{ id: string }>(({ id }) => {
  const { t } = useTranslation('knowledgeBase');
  const pathname = usePathname();
  const { enableRAGEval } = useServerConfigStore(featureFlagsSelectors);
  const [activeKey, setActiveKey] = useState(
    pathname.startsWith(`/repos/${id}/evals`) ? 'eval' : 'files',
  );

  const items = useMemo(
    () =>
      [
        {
          icon: <Icon icon={FileText} />,
          key: 'files',
          label: <Link href={`/repos/${id}`}>{t('tab.files')}</Link>,
        },
        enableRAGEval && {
          icon: <Icon icon={GaugeIcon} />,
          key: 'eval',
          label: <Link href={`/repos/${id}/evals/dataset`}>{t('tab.evals')}</Link>,
        },
        // {
        //   icon: <Icon icon={TestTubeDiagonal} />,
        //   key: `/repos/${id}/testing`,
        //   label: <Link href={`/repos/${id}/testing`}>{t('tab.testing')}</Link>,
        // },
        // {
        //   icon: <Icon icon={Settings2Icon} />,
        //   key: `/repos/${id}/settings`,
        //   label: <Link href={`/repos/${id}/settings`}>{t('tab.settings')}</Link>,
        // },
      ].filter(Boolean) as MenuProps['items'],
    [t],
  );

  return (
    <Flexbox>
      <Menu
        items={items}
        onClick={({ key }) => {
          setActiveKey(key);
        }}
        selectable
        selectedKeys={[activeKey]}
        variant={'compact'}
      />
    </Flexbox>
  );
});

export default FileMenu;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/(mobile)/me/settings/loading.tsx
================================================================================

import SkeletonLoading from '@/components/Loading/SkeletonLoading';

export default () => {
  return <SkeletonLoading paragraph={{ rows: 8 }} />;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/(mobile)/me/(home)/loading.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import Divider from '@/components/Cell/Divider';
import SkeletonLoading from '@/components/Loading/SkeletonLoading';

const Loading = memo(() => {
  return (
    <>
      <Flexbox align={'center'} gap={12} horizontal paddingBlock={12} paddingInline={12}>
        <Skeleton.Avatar active shape={'circle'} size={48} />
        <Skeleton.Button active block />
      </Flexbox>
      <Flexbox gap={4} horizontal paddingBlock={12} paddingInline={16}>
        <Skeleton.Button active block />
        <Skeleton.Button active block />
        <Skeleton.Button active block />
      </Flexbox>
      <Divider />
      <SkeletonLoading
        active
        paragraph={{ rows: 6, style: { marginBottom: 0 }, width: '100%' }}
        title={false}
      />
      <Divider />
      <SkeletonLoading
        active
        paragraph={{ rows: 3, style: { marginBottom: 0 }, width: '100%' }}
        title={false}
      />
    </>
  );
});

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/(mobile)/me/profile/loading.tsx
================================================================================

import SkeletonLoading from '@/components/Loading/SkeletonLoading';

export default () => {
  return <SkeletonLoading paragraph={{ rows: 8 }} />;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/(mobile)/me/data/loading.tsx
================================================================================

import SkeletonLoading from '@/components/Loading/SkeletonLoading';

export default () => {
  return <SkeletonLoading paragraph={{ rows: 8 }} />;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/loading.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { usePathname } from 'next/navigation';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { useServerConfigStore } from '@/store/serverConfig';

import CategoryContainer from '../components/CategoryContainer';

export default memo(() => {
  const pathname = usePathname();
  const mobile = useServerConfigStore((s) => s.isMobile);
  const withoutCategory =
    pathname === '/discover' ||
    pathname === '/discover/providers' ||
    pathname === '/discover/search';

  if (mobile || withoutCategory)
    return (
      <Flexbox flex={1} gap={16}>
        <Skeleton.Button active style={{ minWidth: 150 }} />
        <Skeleton paragraph={{ rows: 16 }} style={{ marginBlock: 24 }} title={false} />
      </Flexbox>
    );

  return (
    <Flexbox gap={24} horizontal style={{ position: 'relative' }} width={'100%'}>
      <CategoryContainer>
        <Skeleton paragraph={{ rows: 10 }} title={false} />
      </CategoryContainer>
      <Flexbox flex={1} gap={16}>
        <Skeleton.Button active style={{ minWidth: 150 }} />
        <Skeleton paragraph={{ rows: 16 }} style={{ marginBlock: 24 }} title={false} />
      </Flexbox>
    </Flexbox>
  );
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/assistants/features/Card.tsx
================================================================================

import { Avatar, Tag } from '@lobehub/ui';
import { Skeleton, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { startCase } from 'lodash-es';
import dynamic from 'next/dynamic';
import qs from 'query-string';
import { memo } from 'react';
import { Center, Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverAssistantItem } from '@/types/discover';

import CardBanner from '../../../components/CardBanner';
import GitHubAvatar from '../../../components/GitHubAvatar';
import { useCategoryItem } from '../../assistants/features/useCategory';

const Link = dynamic(() => import('next/link'), {
  loading: () => <Skeleton.Button size={'small'} style={{ height: 22 }} />,
  ssr: false,
});

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;

    position: relative;

    overflow: hidden;

    height: 100%;
    min-height: 162px;

    background: ${token.colorBgContainer};
    border-radius: ${token.borderRadiusLG}px;
    box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillQuaternary : token.colorFillSecondary}
      inset;

    transition: box-shadow 0.2s ${token.motionEaseInOut};

    &:hover {
      box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillSecondary : token.colorFill} inset;
    }
  `,
  desc: css`
    min-height: 44px;
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,

  time: css`
    color: ${token.colorTextDescription};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-weight: bold;
  `,
}));

export interface AssistantCardProps
  extends Omit<DiscoverAssistantItem, 'suggestions' | 'socialData' | 'config'>,
    Omit<FlexboxProps, 'children'> {
  showCategory?: boolean;
  variant?: 'default' | 'compact';
}

const AssistantCard = memo<AssistantCardProps>(
  ({ showCategory, className, meta, createdAt, author, variant, ...rest }) => {
    const { avatar, title, description, tags = [], category } = meta;
    const { cx, styles, theme } = useStyles();
    const categoryItem = useCategoryItem(category, 12);
    const isCompact = variant === 'compact';

    const user = (
      <Flexbox
        align={'center'}
        gap={6}
        horizontal
        style={{ color: theme.colorTextSecondary, fontSize: 12 }}
      >
        <GitHubAvatar size={18} username={author} />
        <span>{author}</span>
      </Flexbox>
    );

    return (
      <Flexbox className={cx(styles.container, className)} gap={24} {...rest}>
        {!isCompact && <CardBanner avatar={avatar} />}
        <Flexbox gap={12} padding={16}>
          <Flexbox
            align={isCompact ? 'flex-start' : 'flex-end'}
            gap={16}
            horizontal
            justify={'space-between'}
            style={{ position: 'relative' }}
            width={'100%'}
          >
            <Flexbox
              gap={8}
              style={{ overflow: 'hidden', paddingTop: isCompact ? 4 : 0, position: 'relative' }}
            >
              <Title
                className={styles.title}
                ellipsis={{ rows: 1, tooltip: title }}
                level={3}
                style={{ fontSize: isCompact ? 16 : 18 }}
              >
                {title}
              </Title>
              {isCompact && user}
            </Flexbox>
            {isCompact ? (
              <Avatar avatar={avatar} size={40} style={{ display: 'block' }} title={title} />
            ) : (
              <Center
                flex={'none'}
                height={64}
                style={{
                  background: theme.colorBgContainer,
                  borderRadius: '50%',
                  marginTop: -6,
                  overflow: 'hidden',
                  zIndex: 2,
                }}
                width={64}
              >
                <Avatar avatar={avatar} size={56} style={{ display: 'block' }} title={title} />
              </Center>
            )}
          </Flexbox>
          {!isCompact && (
            <Flexbox gap={8} horizontal style={{ fontSize: 12 }}>
              {user}
              <time className={styles.time} dateTime={new Date(createdAt).toISOString()}>
                {createdAt}
              </time>
            </Flexbox>
          )}
          <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
            {description}
          </Paragraph>
          <Flexbox gap={6} horizontal style={{ flexWrap: 'wrap' }}>
            {showCategory && categoryItem ? (
              <Link href={urlJoin('/discover/assistants', categoryItem.key)}>
                <Tag icon={categoryItem.icon} style={{ margin: 0 }}>
                  {categoryItem.label}
                </Tag>
              </Link>
            ) : (
              tags
                .slice(0, 4)
                .filter(Boolean)
                .map((tag: string, index) => {
                  const url = qs.stringifyUrl({
                    query: { q: tag, type: 'assistants' },
                    url: '/discover/search',
                  });
                  return (
                    <Link href={url} key={index}>
                      <Tag style={{ margin: 0 }}>{startCase(tag).trim()}</Tag>
                    </Link>
                  );
                })
            )}
          </Flexbox>
        </Flexbox>
      </Flexbox>
    );
  },
);

export default AssistantCard;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/plugins/features/Card.tsx
================================================================================

import { Avatar, Tag } from '@lobehub/ui';
import { Skeleton, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { startCase } from 'lodash-es';
import dynamic from 'next/dynamic';
import qs from 'query-string';
import { memo } from 'react';
import { Center, Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverPlugintem } from '@/types/discover';

import CardBanner from '../../../components/CardBanner';
import { useCategoryItem } from './useCategory';

const Link = dynamic(() => import('next/link'), {
  loading: () => <Skeleton.Button size={'small'} style={{ height: 22 }} />,
  ssr: false,
});

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;

    position: relative;

    overflow: hidden;

    height: 100%;
    min-height: 162px;

    background: ${token.colorBgContainer};
    border-radius: ${token.borderRadiusLG}px;
    box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillQuaternary : token.colorFillSecondary}
      inset;

    transition: box-shadow 0.2s ${token.motionEaseInOut};

    &:hover {
      box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillSecondary : token.colorFill} inset;
    }
  `,
  desc: css`
    min-height: 44px;
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  inner: css`
    padding: 16px;
  `,
  time: css`
    color: ${token.colorTextDescription};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 18px !important;
    font-weight: bold;
  `,
}));

interface PluginCardProps
  extends Omit<DiscoverPlugintem, 'manifest' | 'suggestions' | 'socialData'>,
    Omit<FlexboxProps, 'children'> {
  showCategory?: boolean;
  variant?: 'default' | 'compact';
}

const PluginCard = memo<PluginCardProps>(
  ({ className, showCategory, meta, createdAt, author, variant, ...rest }) => {
    const { avatar, title, description, tags = [], category } = meta;
    const categoryItem = useCategoryItem(category, 12);
    const { cx, styles, theme } = useStyles();
    const isCompact = variant === 'compact';

    return (
      <Flexbox className={cx(styles.container, className)} gap={24} {...rest}>
        {!isCompact && <CardBanner avatar={avatar} />}
        <Flexbox className={styles.inner} gap={12}>
          <Flexbox align={'flex-end'} gap={16} horizontal justify={'space-between'} width={'100%'}>
            <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
              {title}
            </Title>
            {isCompact ? (
              <Avatar avatar={avatar} size={40} style={{ display: 'block' }} title={title} />
            ) : (
              <Center
                flex={'none'}
                height={64}
                style={{
                  background: theme.colorBgContainer,
                  borderRadius: '50%',
                  marginTop: -6,
                  overflow: 'hidden',
                  zIndex: 2,
                }}
                width={64}
              >
                <Avatar
                  alt={title}
                  avatar={avatar}
                  size={56}
                  style={{ display: 'block' }}
                  title={title}
                />
              </Center>
            )}
          </Flexbox>
          <Flexbox gap={8} horizontal style={{ fontSize: 12 }}>
            <div style={{ color: theme.colorTextSecondary }}>@{author}</div>
            {!isCompact && (
              <time className={styles.time} dateTime={new Date(createdAt).toISOString()}>
                {createdAt}
              </time>
            )}
          </Flexbox>
          <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
            {description}
          </Paragraph>
          <Flexbox gap={6} horizontal style={{ flexWrap: 'wrap' }}>
            {showCategory && categoryItem ? (
              <Link href={urlJoin('/discover/plugins', categoryItem.key)}>
                <Tag icon={categoryItem.icon} style={{ margin: 0 }}>
                  {categoryItem.label}
                </Tag>
              </Link>
            ) : (
              tags
                .slice(0, 4)
                .filter(Boolean)
                .map((tag: string, index) => {
                  const url = qs.stringifyUrl({
                    query: { q: tag, type: 'plugins' },
                    url: '/discover/search',
                  });
                  return (
                    <Link href={url} key={index}>
                      <Tag style={{ margin: 0 }}>{startCase(tag).trim()}</Tag>
                    </Link>
                  );
                })
            )}
          </Flexbox>
        </Flexbox>
      </Flexbox>
    );
  },
);

export default PluginCard;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/providers/features/Card.tsx
================================================================================

import { ModelTag, ProviderCombine } from '@lobehub/icons';
import { Tag } from '@lobehub/ui';
import { Skeleton, Typography } from 'antd';
import { createStyles } from 'antd-style';
import dynamic from 'next/dynamic';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverProviderItem } from '@/types/discover';

const Link = dynamic(() => import('next/link'), {
  loading: () => <Skeleton.Button size={'small'} style={{ height: 22 }} />,
  ssr: false,
});

const { Paragraph } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;

    position: relative;

    overflow: hidden;

    height: 100%;
    min-height: 162px;

    background: ${token.colorBgContainer};
    border-radius: ${token.borderRadiusLG}px;
    box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillQuaternary : token.colorFillSecondary}
      inset;

    transition: box-shadow 0.2s ${token.motionEaseInOut};

    &:hover {
      box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillSecondary : token.colorFill} inset;
    }
  `,
  desc: css`
    min-height: 44px;
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  tagBlue: css`
    color: ${token.geekblue};
    background: ${token.geekblue1};
  `,
  tagGreen: css`
    color: ${token.green};
    background: ${token.green1};
  `,
  time: css`
    color: ${token.colorTextDescription};
  `,
  title: css`
    zoom: 1.2;
    margin-block-end: 0 !important;
    font-size: 18px !important;
    font-weight: bold;
  `,
  token: css`
    font-family: ${token.fontFamilyCode};
  `,
}));

export interface ProviderCardProps extends DiscoverProviderItem, FlexboxProps {
  mobile?: boolean;
}

const ProviderCard = memo<ProviderCardProps>(({ models, className, meta, identifier, ...rest }) => {
  const { description } = meta;
  const { t } = useTranslation(['discover', 'providers']);
  const { cx, styles, theme } = useStyles();

  return (
    <Flexbox className={cx(styles.container, className)} gap={24} {...rest}>
      <Flexbox gap={12} padding={16} width={'100%'}>
        <ProviderCombine
          provider={identifier}
          size={28}
          style={{ color: theme.colorText }}
          title={meta.title}
        />
        <Flexbox gap={8} horizontal style={{ fontSize: 12, marginTop: -8 }}>
          <div style={{ color: theme.colorTextSecondary }}>@{meta.title}</div>
          <div style={{ color: theme.colorTextDescription }}>
            {t('providers.modelCount', { count: models.length })}
          </div>
        </Flexbox>
        {description && (
          <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
            {t(`${identifier}.description`, { ns: 'providers' })}
          </Paragraph>
        )}
        <Flexbox gap={6} horizontal style={{ flexWrap: 'wrap' }}>
          {models
            .slice(0, 3)
            .filter(Boolean)
            .map((tag: string) => (
              <Link href={urlJoin('/discover/model', tag)} key={tag}>
                <ModelTag model={tag} style={{ margin: 0 }} />
              </Link>
            ))}
          {models.length > 3 && <Tag>...</Tag>}
        </Flexbox>
      </Flexbox>
    </Flexbox>
  );
});

export default ProviderCard;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/models/features/const.ts
================================================================================

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';

const providerMap: { [key: string]: string } = {};

DEFAULT_MODEL_PROVIDER_LIST.filter((item) => item.chatModels.length > 0).forEach((item) => {
  providerMap[item.id] = item.name;
});

export { providerMap };


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(list)/models/features/Card.tsx
================================================================================

import { ModelIcon } from '@lobehub/icons';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverModelItem } from '@/types/discover';

import ModelFeatureTags from '../../../features/ModelFeatureTags';

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;

    position: relative;

    overflow: hidden;

    height: 100%;
    min-height: 162px;

    background: ${token.colorBgContainer};
    border-radius: ${token.borderRadiusLG}px;
    box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillQuaternary : token.colorFillSecondary}
      inset;

    transition: box-shadow 0.2s ${token.motionEaseInOut};

    &:hover {
      box-shadow: 0 0 1px 1px ${isDarkMode ? token.colorFillSecondary : token.colorFill} inset;
    }
  `,
  desc: css`
    min-height: 44px;
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  id: css`
    margin-block-end: 0 !important;
    font-size: 12px;
    color: ${token.colorTextSecondary};
  `,
  tagBlue: css`
    color: ${token.geekblue};
    background: ${token.geekblue1};
  `,
  tagGreen: css`
    color: ${token.green};
    background: ${token.green1};
  `,
  time: css`
    color: ${token.colorTextDescription};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 18px !important;
    font-weight: bold;
  `,
  token: css`
    font-family: ${token.fontFamilyCode};
  `,
}));

export interface ModelCardProps extends DiscoverModelItem, FlexboxProps {
  showCategory?: boolean;
}

const ModelCard = memo<ModelCardProps>(({ className, meta, identifier, ...rest }) => {
  const { description, title, functionCall, vision, contextWindowTokens } = meta;
  const { t } = useTranslation('models');
  const { cx, styles } = useStyles();

  return (
    <Flexbox className={cx(styles.container, className)} gap={24} key={identifier} {...rest}>
      <Flexbox
        gap={12}
        padding={16}
        style={{ overflow: 'hidden', position: 'relative' }}
        width={'100%'}
      >
        <Flexbox
          align={'center'}
          gap={12}
          horizontal
          style={{ overflow: 'hidden', position: 'relative' }}
          width={'100%'}
        >
          <ModelIcon model={identifier} size={32} type={'avatar'} />
          <Flexbox style={{ overflow: 'hidden', position: 'relative' }}>
            <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
              {title}
            </Title>
            <Paragraph className={styles.id} ellipsis={{ rows: 1 }}>
              {identifier}
            </Paragraph>
          </Flexbox>
        </Flexbox>
        {description && (
          <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
            {t(`${identifier}.description`)}
          </Paragraph>
        )}

        <ModelFeatureTags
          functionCall={functionCall}
          tokens={contextWindowTokens}
          vision={vision}
        />
      </Flexbox>
    </Flexbox>
  );
});

export default ModelCard;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/features/CreateButton/index.tsx
================================================================================

import { ActionIcon, Icon, Modal } from '@lobehub/ui';
import { Button, Skeleton } from 'antd';
import { useResponsive } from 'antd-style';
import { Brush } from 'lucide-react';
import dynamic from 'next/dynamic';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';

import { MOBILE_HEADER_ICON_SIZE } from '@/const/layoutTokens';

const Inner = dynamic(() => import('./Inner'), {
  loading: () => <Skeleton paragraph={{ rows: 8 }} title={false} />,
});

const CreateButton = memo<{ mobile?: boolean }>(({ mobile }) => {
  const { mobile: resMobile } = useResponsive();
  const { t } = useTranslation('discover');
  const [isModalOpen, setIsModalOpen] = useState(false);

  const buttonContent =
    mobile || resMobile ? (
      <ActionIcon
        icon={Brush}
        onClick={() => setIsModalOpen(true)}
        size={MOBILE_HEADER_ICON_SIZE}
        title={t('create')}
      />
    ) : (
      <Button icon={<Icon icon={Brush} />} onClick={() => setIsModalOpen(true)}>
        {t('create')}
      </Button>
    );

  return (
    <>
      {buttonContent}
      <Modal
        allowFullscreen
        footer={null}
        onCancel={() => setIsModalOpen(false)}
        open={isModalOpen}
        title={t('create')}
      >
        <Inner />
      </Modal>
    </>
  );
});

export default CreateButton;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/components/Statistic.tsx
================================================================================

import { Icon, Tooltip } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { HelpCircleIcon } from 'lucide-react';
import { CSSProperties, ReactNode, memo } from 'react';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

export const useStyles = createStyles(({ css, token }) => ({
  container: css`
    overflow: hidden;
    min-width: 64px;
  `,
  number: css`
    margin: 0 !important;

    font-size: 16px;
    font-weight: 500;
    line-height: 1.4;
    text-align: center;
  `,
  title: css`
    margin: 0 !important;

    font-size: 12px;
    line-height: 1.2;
    color: ${token.colorTextSecondary};
    text-align: center;
  `,
}));

export interface StatisticProps extends Omit<FlexboxProps, 'children' | 'title'> {
  title: ReactNode;
  titleStyle?: CSSProperties;
  tooltip?: string;
  value: ReactNode;
  valuePlacement?: 'top' | 'bottom';
  valueStyle?: CSSProperties;
}

const Statistic = memo<StatisticProps>(
  ({
    className,
    valueStyle,
    titleStyle,
    valuePlacement = 'top',
    tooltip,
    title,
    value,
    ...rest
  }) => {
    const { cx, styles } = useStyles();
    const isTop = valuePlacement === 'top';
    const valueContent = (
      <Typography.Paragraph className={styles.number} ellipsis={{ rows: 1 }} style={valueStyle}>
        {value}
      </Typography.Paragraph>
    );
    const titleContent = (
      <Typography.Paragraph className={styles.title} ellipsis={{ rows: 1 }} style={titleStyle}>
        {title}
        {tooltip && <Icon icon={HelpCircleIcon} style={{ marginLeft: '0.4em' }} />}
      </Typography.Paragraph>
    );
    const content = (
      <Flexbox
        align={'center'}
        className={cx(styles.container, className)}
        flex={1}
        justify={'center'}
        {...rest}
      >
        {isTop ? (
          <>
            {valueContent}
            {titleContent}
          </>
        ) : (
          <>
            {titleContent}
            {valueContent}
          </>
        )}
      </Flexbox>
    );

    if (!tooltip) return content;

    return <Tooltip title={tooltip}>{content}</Tooltip>;
  },
);

export default Statistic;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/components/Loading.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { memo } from 'react';

import Title from './Title';

const Loading = memo<{ title: string }>(({ title }) => {
  return (
    <>
      <Title>{title}</Title>
      <Skeleton active paragraph={{ rows: 8 }} title={false} />
    </>
  );
});

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/loading.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { useServerConfigStore } from '@/store/serverConfig';

import DetailLayout from './features/DetailLayout';

const Loading = memo(() => {
  const mobile = useServerConfigStore((s) => s.isMobile);
  return (
    <DetailLayout
      actions={<Skeleton.Button block />}
      header={
        <Flexbox gap={12} width={'100%'}>
          <Flexbox align={'center'} gap={8} horizontal justify={'space-between'} width={'100%'}>
            <Flexbox align={'center'} gap={12} horizontal justify={'flex-start'}>
              <Skeleton.Avatar active shape={'circle'} size={48} />
              <Skeleton.Button active />
            </Flexbox>
            <Flexbox align={'center'} gap={4} horizontal justify={'flex-end'}>
              <Skeleton.Button active />
            </Flexbox>
          </Flexbox>
          <Skeleton active title={false} />
        </Flexbox>
      }
      mobile={mobile}
      sidebar={<Skeleton paragraph={{ rows: 5 }} />}
    >
      <Skeleton paragraph={{ rows: 16 }} title={false} />
    </DetailLayout>
  );
});

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/assistant/[slug]/features/SystemRole.tsx
================================================================================

'use client';

import { Markdown } from '@lobehub/ui';
import { Skeleton } from 'antd';
import { useTheme } from 'antd-style';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';

import Block from '../../../features/Block';

const SystemRole = memo<{ children?: string }>(({ children }) => {
  const { t } = useTranslation('discover');
  const theme = useTheme();

  return (
    <Block title={t('assistants.systemRole')}>
      {children ? (
        <Markdown
          fontSize={theme.fontSize}
          style={{
            border: `1px solid ${theme.colorBorderSecondary}`,
            borderRadius: theme.borderRadiusLG,
            paddingInline: 16,
          }}
        >
          {children}
        </Markdown>
      ) : (
        <Skeleton paragraph={{ rows: 4 }} title={false} />
      )}
    </Block>
  );
});

export default SystemRole;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/assistant/[slug]/features/Temp.tsx
================================================================================

'use client';

import { Markdown } from '@lobehub/ui';
import { Skeleton } from 'antd';
import { useTheme } from 'antd-style';
import { BotMessageSquare } from 'lucide-react';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverAssistantItem } from '@/types/discover';

import HighlightBlock from '../../../features/HighlightBlock';

interface ConversationExampleProps extends FlexboxProps {
  data: DiscoverAssistantItem;
  identifier: string;
  mobile?: boolean;
}

const ConversationExample = memo<ConversationExampleProps>(({ data }) => {
  const { t } = useTranslation('discover');
  const theme = useTheme();

  return (
    <HighlightBlock
      avatar={data?.meta.avatar}
      icon={BotMessageSquare}
      justify={'space-between'}
      style={{ background: theme.colorBgContainer }}
      title={t('assistants.systemRole')}
    >
      <Flexbox paddingInline={16}>
        {data.config.systemRole ? (
          <Markdown fontSize={theme.fontSize}>{data.config.systemRole}</Markdown>
        ) : (
          <Skeleton paragraph={{ rows: 4 }} title={false} />
        )}
      </Flexbox>
    </HighlightBlock>
  );
});

export default ConversationExample;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/assistant/[slug]/features/InfoSidebar/index.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import Link from 'next/link';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverAssistantItem, DiscoverPlugintem } from '@/types/discover';

import Block from '../../../../features/Block';
import SuggestionItem from './SuggestionItem';
import ToolItem from './ToolItem';

interface InfoSidebarProps extends FlexboxProps {
  data: DiscoverAssistantItem;
  identifier: string;
  mobile?: boolean;
  pluginData?: DiscoverPlugintem[];
}

const InfoSidebar = memo<InfoSidebarProps>(({ pluginData, data, ...rest }) => {
  const { t } = useTranslation('discover');

  return (
    <Flexbox gap={48} style={{ position: 'relative' }} width={'100%'} {...rest}>
      {pluginData && pluginData?.length > 0 && (
        <Block gap={12} title={t('assistants.plugins')}>
          {pluginData.map((item) => (
            <Link
              href={urlJoin('/discover/plugin', item.identifier)}
              key={item.identifier}
              style={{ color: 'inherit' }}
            >
              <ToolItem {...item} />
            </Link>
          ))}
        </Block>
      )}
      <Block
        gap={24}
        more={t('assistants.more')}
        moreLink={urlJoin('/discover/assistants', data.meta?.category || '')}
        title={t('assistants.suggestions')}
      >
        {data?.suggestions?.length > 0 ? (
          data?.suggestions.map((item) => (
            <Link href={urlJoin('/discover/assistant', item.identifier)} key={item.identifier}>
              <SuggestionItem {...item} />
            </Link>
          ))
        ) : (
          <Skeleton active paragraph={{ rows: 5 }} title={false} />
        )}
      </Block>
    </Flexbox>
  );
});

export default InfoSidebar;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/assistant/[slug]/features/InfoSidebar/SuggestionItem.tsx
================================================================================

import { Avatar } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverAssistantItem } from '@/types/discover';

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;
    position: relative;
    overflow: hidden;
    height: 100%;
  `,
  desc: css`
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 16px !important;
    font-weight: 500 !important;
  `,
}));

export interface SuggestionItemProps
  extends Omit<DiscoverAssistantItem, 'suggestions' | 'socialData' | 'category' | 'config'>,
    FlexboxProps {}

const SuggestionItem = memo<SuggestionItemProps>(({ className, meta, identifier, ...rest }) => {
  const { avatar, title, description, backgroundColor } = meta;

  const { cx, styles, theme } = useStyles();

  return (
    <Flexbox className={cx(styles.container, className)} gap={12} key={identifier} {...rest}>
      <Flexbox align={'center'} gap={12} horizontal width={'100%'}>
        <Avatar
          alt={title}
          avatar={avatar}
          background={backgroundColor || theme.colorFillTertiary}
          size={36}
          title={title}
        />
        <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
          {title}
        </Title>
      </Flexbox>
      <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
        {description}
      </Paragraph>
    </Flexbox>
  );
});

export default SuggestionItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/features/ShareButton.tsx
================================================================================

import { ActionIcon, Avatar, CopyButton, Icon, Input, Modal, Tag } from '@lobehub/ui';
import { Button, ButtonProps, Skeleton, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { startCase } from 'lodash-es';
import { LinkIcon, Share2Icon } from 'lucide-react';
import Link from 'next/link';
import { ReactNode, memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import { useShare } from '@/hooks/useShare';

import CardBanner from '../../components/CardBanner';

const useStyles = createStyles(({ css, token }) => {
  return {
    banner: css`
      overflow: hidden;

      background: ${token.colorBgContainer};
      border: 1px solid ${token.colorBorderSecondary};
      border-radius: ${token.borderRadiusLG}px;
      box-shadow: ${token.boxShadowTertiary};
    `,
    copy: css`
      background: ${token.colorPrimary};

      &:hover {
        background: ${token.colorPrimaryHover};
      }
    `,
    icon: css`
      border: 1px solid ${token.colorFillSecondary};

      svg {
        fill: ${token.colorTextSecondary};
      }

      &:hover {
        border: 1px solid ${token.colorBorderSecondary};

        svg {
          fill: ${token.colorText};
        }
      }
    `,
  };
});

interface ShareButtonProps extends ButtonProps {
  meta?: {
    avatar?: string | ReactNode;
    desc?: string;
    hashtags?: string[];
    tags?: ReactNode;
    title?: string;
    url: string;
  };
}

const ShareButton = memo<ShareButtonProps>(({ meta, ...rest }) => {
  const { x, reddit, telegram, whatsapp, mastodon, linkedin, weibo } = useShare({
    avatar: '',
    desc: '',
    hashtags: [],
    title: '',
    url: '',
    ...meta,
  });
  const { t } = useTranslation('common');
  const { styles, theme } = useStyles();
  const [open, setOpen] = useState(false);

  let content;

  if (meta) {
    content = (
      <Center gap={16} style={{ position: 'relative' }} width={'100%'}>
        <Flexbox align={'center'} className={styles.banner} width={'100%'}>
          <CardBanner avatar={meta.avatar} size={640} style={{ height: 72, marginBottom: -36 }} />
          <Center
            flex={'none'}
            height={72}
            style={{
              backgroundColor: theme.colorBgContainer,
              borderRadius: '50%',
              overflow: 'hidden',
              zIndex: 2,
            }}
            width={72}
          >
            <Avatar animation avatar={meta.avatar} shape={'circle'} size={64} />
          </Center>
          <Center padding={12} width={'100%'}>
            <h3 style={{ fontWeight: 'bold', textAlign: 'center' }}>{meta.title}</h3>
            <Typography.Paragraph style={{ color: theme.colorTextSecondary, textAlign: 'center' }}>
              {meta.desc}
            </Typography.Paragraph>
            {meta.hashtags && (
              <Flexbox align={'center'} gap={4} horizontal justify={'center'} wrap={'wrap'}>
                {meta.hashtags.map((tag, index) => (
                  <Tag key={index} style={{ margin: 0 }}>
                    {startCase(tag).trim()}
                  </Tag>
                ))}
              </Flexbox>
            )}
            {meta.tags}
          </Center>
        </Flexbox>
        <Flexbox align={'center'} gap={8} horizontal justify={'center'} wrap={'wrap'}>
          {[x, reddit, telegram, whatsapp, mastodon, linkedin, weibo].map((item) => (
            <Link href={item.link} key={item.title} target={'_blank'}>
              <ActionIcon
                className={styles.icon}
                icon={item.icon as any}
                size={{ blockSize: 36, borderRadius: 18, fontSize: 16 }}
                title={item.title}
              />
            </Link>
          ))}
        </Flexbox>
        <Flexbox align={'center'} gap={8} horizontal width={'100%'}>
          <Input type={'block'} value={meta.url} />
          <CopyButton
            className={styles.copy}
            color={theme.colorBgLayout}
            content={meta.url}
            icon={LinkIcon}
            size={{ blockSize: 36, fontSize: 16 }}
          />
        </Flexbox>
      </Center>
    );
  } else {
    content = <Skeleton active paragraph={{ rows: 4 }} title={false} />;
  }

  return (
    <>
      <Button
        icon={<Icon icon={Share2Icon} />}
        onClick={() => setOpen(true)}
        size={'large'}
        {...rest}
      />
      <Modal
        footer={null}
        onCancel={() => setOpen(false)}
        open={open}
        title={t('share')}
        width={360}
      >
        {content}
      </Modal>
    </>
  );
});

export default ShareButton;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/plugin/[slug]/features/Schema.tsx
================================================================================

'use client';

import { Highlighter } from '@lobehub/ui';
import { Skeleton } from 'antd';
import { memo } from 'react';

import Block from '../../../features/Block';

const Schema = memo<{ children?: string }>(({ children }) => {
  return (
    <Block title={'JSON Schema'}>
      {children ? (
        <Highlighter allowChangeLanguage={false} fullFeatured language={'json'}>
          {children}
        </Highlighter>
      ) : (
        <Skeleton paragraph={{ rows: 4 }} title={false} />
      )}
    </Block>
  );
});

export default Schema;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/plugin/[slug]/features/InfoSidebar/index.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import Link from 'next/link';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverPlugintem } from '@/types/discover';

import Block from '../../../../features/Block';
import SuggestionItem from './SuggestionItem';

interface InfoSidebarProps extends FlexboxProps {
  data: DiscoverPlugintem;
  identifier: string;
  mobile?: boolean;
}

const InfoSidebar = memo<InfoSidebarProps>(({ data, ...rest }) => {
  const { t } = useTranslation('discover');

  return (
    <Flexbox gap={48} style={{ position: 'relative' }} width={'100%'} {...rest}>
      <Block
        gap={24}
        more={t('assistants.more')}
        moreLink={urlJoin('/discover/plugins', data.meta?.category || '')}
        title={t('assistants.suggestions')}
      >
        {data?.suggestions?.length > 0 ? (
          data?.suggestions.map((item) => (
            <Link href={urlJoin('/discover/plugin', item.identifier)} key={item.identifier}>
              <SuggestionItem {...item} />
            </Link>
          ))
        ) : (
          <Skeleton active paragraph={{ rows: 5 }} title={false} />
        )}
      </Block>
    </Flexbox>
  );
});

export default InfoSidebar;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/plugin/[slug]/features/InfoSidebar/SuggestionItem.tsx
================================================================================

import { Avatar } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverPlugintem } from '@/types/discover';

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;
    position: relative;
    overflow: hidden;
    height: 100%;
  `,
  desc: css`
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 16px !important;
    font-weight: 500 !important;
  `,
}));

export interface SuggestionItemProps
  extends Omit<DiscoverPlugintem, 'suggestions' | 'socialData' | 'category' | 'manifest'>,
    FlexboxProps {}

const SuggestionItem = memo<SuggestionItemProps>(({ className, meta, identifier, ...rest }) => {
  const { avatar, title, description } = meta;

  const { cx, styles, theme } = useStyles();

  return (
    <Flexbox className={cx(styles.container, className)} gap={12} key={identifier} {...rest}>
      <Flexbox align={'center'} gap={12} horizontal width={'100%'}>
        <Avatar
          alt={title}
          avatar={avatar}
          background={theme.colorFillTertiary}
          size={36}
          title={title}
        />
        <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
          {title}
        </Title>
      </Flexbox>
      <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
        {description}
      </Paragraph>
    </Flexbox>
  );
});

export default SuggestionItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/provider/[slug]/features/ModelList/index.tsx
================================================================================

'use client';

import { ProviderIcon } from '@lobehub/icons';
import { Icon } from '@lobehub/ui';
import { Button, Divider } from 'antd';
import { useTheme } from 'antd-style';
import { Brain, ChevronsUpDown } from 'lucide-react';
import { Fragment, memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { DiscoverModelItem } from '@/types/discover';

import HighlightBlock from '../../../../features/HighlightBlock';
import ModelItem from './ModelItem';

const DEFAULT_LENGTH = 4;

interface ModelListProps {
  identifier: string;
  mobile?: boolean;
  modelData: DiscoverModelItem[];
}

const ModelList = memo<ModelListProps>(({ mobile, modelData, identifier }) => {
  const [showAll, setShowAll] = useState(false);
  const { t } = useTranslation('discover');
  const theme = useTheme();

  const list = showAll ? modelData : modelData.slice(0, DEFAULT_LENGTH);

  return (
    <HighlightBlock
      avatar={<ProviderIcon provider={identifier} size={300} type={'avatar'} />}
      icon={Brain}
      justify={'space-between'}
      style={{ background: theme.colorBgContainer }}
      title={t('providers.supportedModels')}
    >
      {list.map((item, index) => (
        <Fragment key={item.identifier}>
          <ModelItem mobile={mobile} {...item} />
          {index < modelData.length - 1 && <Divider style={{ margin: 0 }} />}
        </Fragment>
      ))}
      {modelData.length > DEFAULT_LENGTH && !showAll && (
        <Flexbox padding={16}>
          <Button icon={<Icon icon={ChevronsUpDown} />} onClick={() => setShowAll(true)}>
            {t('providers.showAllModels')}{' '}
            <span style={{ color: theme.colorTextDescription }}>
              (+{modelData.length - DEFAULT_LENGTH})
            </span>
          </Button>
        </Flexbox>
      )}
    </HighlightBlock>
  );
});

export default ModelList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/provider/[slug]/features/ModelList/ModelItem.tsx
================================================================================

import { ModelIcon } from '@lobehub/icons';
import { ActionIcon, Grid } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles, useResponsive } from 'antd-style';
import { ChevronRightIcon } from 'lucide-react';
import Link from 'next/link';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverModelItem } from '@/types/discover';
import { formatPriceByCurrency, formatTokenNumber } from '@/utils/format';

import Statistic, { type StatisticProps } from '../../../../../components/Statistic';
import ModelFeatureTags from '../../../../../features/ModelFeatureTags';

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  desc: css`
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  id: css`
    margin-block-end: 0 !important;
    font-size: 12px;
    color: ${token.colorTextSecondary};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 16px !important;
    font-weight: 500 !important;
  `,
  token: css`
    font-family: ${token.fontFamilyCode};
  `,
}));

export interface SuggestionItemProps
  extends Omit<DiscoverModelItem, 'suggestions' | 'socialData' | 'providers'>,
    FlexboxProps {
  mobile?: boolean;
}

const ModelItem = memo<SuggestionItemProps>(({ mobile, meta, identifier }) => {
  const { title, contextWindowTokens, vision, functionCall } = meta;
  const { xl = true } = useResponsive();
  const { t } = useTranslation('discover');
  const { styles, theme } = useStyles();

  const isMobile = mobile || !xl;

  const items: StatisticProps[] = [
    {
      title: t('models.contentLength'),
      value: meta?.contextWindowTokens ? formatTokenNumber(meta.contextWindowTokens) : '--',
    },
    {
      title: t('models.providerInfo.maxOutput'),
      tooltip: t('models.providerInfo.maxOutputTooltip'),
      value: meta?.maxOutput ? formatTokenNumber(meta.maxOutput) : '--',
    },
    {
      title: t('models.providerInfo.input'),
      tooltip: t('models.providerInfo.inputTooltip'),
      value: meta?.pricing?.input
        ? '$' + formatPriceByCurrency(meta.pricing.input, meta.pricing?.currency)
        : '--',
    },
    {
      title: t('models.providerInfo.output'),
      tooltip: t('models.providerInfo.outputTooltip'),
      value: meta?.pricing?.output
        ? '$' + formatPriceByCurrency(meta.pricing.output, meta.pricing?.currency)
        : '--',
    },
    /* ↓ cloud slot ↓ */

    /* ↑ cloud slot ↑ */
  ];

  const header = (
    <Flexbox gap={12}>
      <Link href={urlJoin('/discover/model', identifier)} style={{ color: 'inherit' }}>
        <Flexbox align={'center'} gap={12} horizontal width={'100%'}>
          <ModelIcon model={identifier} size={36} type={'avatar'} />
          <Flexbox style={{ overflow: 'hidden' }}>
            <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
              {title}
            </Title>
            <Paragraph className={styles.id} ellipsis={{ rows: 1 }}>
              {identifier}
            </Paragraph>
          </Flexbox>
        </Flexbox>
      </Link>
      <ModelFeatureTags functionCall={functionCall} tokens={contextWindowTokens} vision={vision} />
    </Flexbox>
  );

  const button = (
    <Link href={urlJoin('/discover/model', identifier)} style={{ color: 'inherit' }}>
      <ActionIcon color={theme.colorTextDescription} icon={ChevronRightIcon} />
    </Link>
  );

  return (
    <Flexbox
      align={'center'}
      gap={16}
      horizontal
      justify={'space-between'}
      padding={16}
      wrap={'wrap'}
    >
      {isMobile && (
        <Flexbox align={'center'} horizontal justify={'space-between'}>
          {header}
          {button}
        </Flexbox>
      )}
      <Grid
        align={'center'}
        flex={1}
        gap={16}
        horizontal
        maxItemWidth={100}
        rows={isMobile ? 2 : items.length + 1}
        style={{ minWidth: 240 }}
      >
        {!isMobile && header}
        {items.map((item, index) => (
          <Statistic
            align={isMobile ? 'flex-start' : 'center'}
            gap={4}
            key={index}
            valuePlacement={'bottom'}
            valueStyle={{ fontSize: 18 }}
            {...item}
          />
        ))}
      </Grid>
      {!isMobile && button}
    </Flexbox>
  );
});

export default ModelItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/provider/[slug]/features/InfoSidebar/index.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import Link from 'next/link';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverProviderItem } from '@/types/discover';

import Block from '../../../../features/Block';
import SuggestionItem from './SuggestionItem';

interface InfoSidebarProps extends FlexboxProps {
  data: DiscoverProviderItem;
  identifier: string;
  mobile?: boolean;
}

const InfoSidebar = memo<InfoSidebarProps>(({ data, ...rest }) => {
  const { t } = useTranslation('discover');

  return (
    <Flexbox gap={48} style={{ position: 'relative' }} width={'100%'} {...rest}>
      <Block
        gap={24}
        more={t('providers.more')}
        moreLink={'/discover/providers'}
        title={t('providers.suggestions')}
      >
        {data?.suggestions?.length > 0 ? (
          data?.suggestions.map((item) => (
            <Link href={urlJoin('/discover/provider', item.identifier)} key={item.identifier}>
              <SuggestionItem {...item} />
            </Link>
          ))
        ) : (
          <Skeleton active paragraph={{ rows: 5 }} title={false} />
        )}
      </Block>
    </Flexbox>
  );
});

export default InfoSidebar;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/provider/[slug]/features/InfoSidebar/SuggestionItem.tsx
================================================================================

import { ProviderCombine } from '@lobehub/icons';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverProviderItem } from '@/types/discover';

const { Paragraph } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;

    position: relative;

    overflow: hidden;

    height: 100%;

    color: ${token.colorText};
  `,
  desc: css`
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  id: css`
    margin-block-end: 0 !important;
    font-size: 12px;
    color: ${token.colorTextSecondary};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 16px !important;
    font-weight: 500 !important;
  `,
  token: css`
    font-family: ${token.fontFamilyCode};
  `,
}));

export interface SuggestionItemProps
  extends Omit<DiscoverProviderItem, 'suggestions' | 'socialData'>,
    FlexboxProps {}

const SuggestionItem = memo<SuggestionItemProps>(
  ({ className, meta, identifier, models, ...rest }) => {
    const { title, description } = meta;
    const { t } = useTranslation(['discover', 'providers']);
    const { cx, styles, theme } = useStyles();

    return (
      <Flexbox className={cx(styles.container, className)} gap={12} key={identifier} {...rest}>
        <ProviderCombine provider={identifier} size={24} title={title} />
        <Flexbox gap={8} horizontal style={{ fontSize: 12, marginTop: -8 }}>
          <div style={{ color: theme.colorTextSecondary }}>@{meta.title}</div>
          <div style={{ color: theme.colorTextDescription }}>
            {t('providers.modelCount', { count: models.length })}
          </div>
        </Flexbox>
        {description && (
          <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
            {t(`${identifier}.description`, { ns: 'providers' })}
          </Paragraph>
        )}
      </Flexbox>
    );
  },
);

export default SuggestionItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/model/[...slugs]/features/ProviderList/ProviderItem.tsx
================================================================================

import { ModelTag, ProviderCombine } from '@lobehub/icons';
import { ActionIcon, Grid, Icon, Tooltip } from '@lobehub/ui';
import { Tag } from 'antd';
import { createStyles, useResponsive } from 'antd-style';
import { BadgeCheck, BookIcon, ChevronRightIcon, KeyIcon } from 'lucide-react';
import Link from 'next/link';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';
import { BASE_PROVIDER_DOC_URL } from '@/const/url';
import { DiscoverProviderItem } from '@/types/discover';
import { formatPriceByCurrency, formatTokenNumber } from '@/utils/format';

import Statistic, { type StatisticProps } from '../../../../../components/Statistic';

const useStyles = createStyles(({ css, token }) => ({
  tagGreen: css`
    color: ${token.colorSuccess};
    background: ${token.colorSuccessBgHover};
  `,
}));

interface ProviderItemProps extends DiscoverProviderItem {
  mobile?: boolean;
  modelId: string;
}

const ProviderItem = memo<ProviderItemProps>(({ mobile, modelId, identifier }) => {
  const { t } = useTranslation('discover');
  const { xl = true } = useResponsive();
  const { styles, theme } = useStyles();
  const isLobeHub = identifier === 'lobehub';

  const isMobile = mobile || !xl;

  const model = useMemo(() => {
    const prividerItem = DEFAULT_MODEL_PROVIDER_LIST.find((v) => v.id === identifier);
    if (!prividerItem) return;
    return prividerItem.chatModels.find((m) => m.id.toLowerCase().includes(modelId.toLowerCase()));
  }, [identifier, modelId]);

  const items: StatisticProps[] = [
    {
      title: t('models.contentLength'),
      value: model?.contextWindowTokens ? formatTokenNumber(model.contextWindowTokens) : '--',
    },
    {
      title: t('models.providerInfo.maxOutput'),
      tooltip: t('models.providerInfo.maxOutputTooltip'),
      value: model?.maxOutput ? formatTokenNumber(model.maxOutput) : '--',
    },
    {
      title: t('models.providerInfo.input'),
      tooltip: t('models.providerInfo.inputTooltip'),
      value: model?.pricing?.input
        ? '$' + formatPriceByCurrency(model.pricing.input, model.pricing?.currency)
        : '--',
    },
    {
      title: t('models.providerInfo.output'),
      tooltip: t('models.providerInfo.outputTooltip'),
      value: model?.pricing?.output
        ? '$' + formatPriceByCurrency(model.pricing.output, model.pricing?.currency)
        : '--',
    },
    /* ↓ cloud slot ↓ */

    /* ↑ cloud slot ↑ */
  ];

  const header = (
    <Flexbox gap={4} style={{ minWidth: 240 }}>
      <Link href={urlJoin('/discover/provider', identifier)} style={{ color: 'inherit' }}>
        <ProviderCombine provider={identifier} size={24} />
      </Link>
      <Flexbox align={'center'} gap={6} horizontal wrap={'wrap'}>
        <ModelTag model={modelId} style={{ background: theme.colorFillQuaternary, margin: 0 }} />
        {isLobeHub && (
          <Tooltip title={t('models.providerInfo.officialTooltip')}>
            <Tag
              bordered={false}
              className={styles.tagGreen}
              icon={<Icon icon={BadgeCheck} />}
              style={{ margin: 0 }}
            />
          </Tooltip>
        )}
        {!isLobeHub && (
          <Tooltip title={t('models.providerInfo.apiTooltip')}>
            <Tag bordered={false} icon={<Icon icon={KeyIcon} />} style={{ margin: 0 }} />
          </Tooltip>
        )}
        <Tooltip title={t('models.guide')}>
          <Link href={urlJoin(BASE_PROVIDER_DOC_URL, identifier)} target={'_blank'}>
            <Tag bordered={false} icon={<Icon icon={BookIcon} />} style={{ margin: 0 }} />
          </Link>
        </Tooltip>
      </Flexbox>
    </Flexbox>
  );

  const button = (
    <Link href={urlJoin('/discover/provider', identifier)} style={{ color: 'inherit' }}>
      <ActionIcon color={theme.colorTextDescription} icon={ChevronRightIcon} />
    </Link>
  );

  return (
    <Flexbox
      align={'center'}
      gap={16}
      horizontal
      justify={'space-between'}
      padding={16}
      wrap={'wrap'}
    >
      {isMobile && (
        <Flexbox align={'center'} horizontal justify={'space-between'}>
          {header}
          {button}
        </Flexbox>
      )}
      <Grid
        align={'center'}
        flex={1}
        gap={16}
        horizontal
        maxItemWidth={100}
        rows={isMobile ? 2 : items.length + 1}
        style={{ minWidth: 240 }}
      >
        {!isMobile && header}
        {items.map((item, index) => (
          <Statistic
            align={isMobile ? 'flex-start' : 'center'}
            gap={4}
            key={index}
            valuePlacement={'bottom'}
            valueStyle={{ fontSize: 18 }}
            {...item}
          />
        ))}
      </Grid>
      {!isMobile && button}
    </Flexbox>
  );
});

export default ProviderItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/model/[...slugs]/features/InfoSidebar/index.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import Link from 'next/link';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';
import urlJoin from 'url-join';

import { DiscoverModelItem } from '@/types/discover';

import Block from '../../../../features/Block';
import SuggestionItem from './SuggestionItem';

interface InfoSidebarProps extends FlexboxProps {
  data: DiscoverModelItem;
  identifier: string;
  mobile?: boolean;
}

const InfoSidebar = memo<InfoSidebarProps>(({ data, ...rest }) => {
  const { t } = useTranslation('discover');

  return (
    <Flexbox gap={48} style={{ position: 'relative' }} width={'100%'} {...rest}>
      <Block
        gap={24}
        more={t('models.more')}
        moreLink={urlJoin('/discover/models', data.meta?.category || '')}
        title={t('models.suggestions')}
      >
        {data?.suggestions?.length > 0 ? (
          data?.suggestions.map((item) => (
            <Link href={urlJoin('/discover/model', item.identifier)} key={item.identifier}>
              <SuggestionItem {...item} />
            </Link>
          ))
        ) : (
          <Skeleton active paragraph={{ rows: 5 }} title={false} />
        )}
      </Block>
    </Flexbox>
  );
});

export default InfoSidebar;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/discover/(detail)/model/[...slugs]/features/InfoSidebar/SuggestionItem.tsx
================================================================================

import { ModelIcon } from '@lobehub/icons';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox, FlexboxProps } from 'react-layout-kit';

import { DiscoverModelItem } from '@/types/discover';

import ModelFeatureTags from '../../../../../features/ModelFeatureTags';

const { Paragraph, Title } = Typography;

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  banner: css`
    opacity: ${isDarkMode ? 0.9 : 0.4};
  `,
  container: css`
    cursor: pointer;
    position: relative;
    overflow: hidden;
    height: 100%;
  `,
  desc: css`
    margin-block-end: 0 !important;
    color: ${token.colorTextDescription};
  `,
  id: css`
    margin-block-end: 0 !important;
    font-size: 12px;
    color: ${token.colorTextSecondary};
  `,
  title: css`
    margin-block-end: 0 !important;
    font-size: 16px !important;
    font-weight: 500 !important;
  `,
  token: css`
    font-family: ${token.fontFamilyCode};
  `,
}));

export interface SuggestionItemProps
  extends Omit<DiscoverModelItem, 'suggestions' | 'socialData' | 'providers'>,
    FlexboxProps {}

const SuggestionItem = memo<SuggestionItemProps>(({ className, meta, identifier, ...rest }) => {
  const { title, description, contextWindowTokens, vision, functionCall } = meta;
  const { t } = useTranslation('models');
  const { cx, styles } = useStyles();

  return (
    <Flexbox className={cx(styles.container, className)} gap={12} key={identifier} {...rest}>
      <Flexbox align={'center'} gap={12} horizontal width={'100%'}>
        <ModelIcon model={identifier} size={36} type={'avatar'} />
        <Flexbox style={{ overflow: 'hidden' }}>
          <Title className={styles.title} ellipsis={{ rows: 1, tooltip: title }} level={3}>
            {title}
          </Title>
          <Paragraph className={styles.id} ellipsis={{ rows: 1 }}>
            {identifier}
          </Paragraph>
        </Flexbox>
      </Flexbox>
      {description && (
        <Paragraph className={styles.desc} ellipsis={{ rows: 2 }}>
          {t(`${identifier}.description`)}
        </Paragraph>
      )}
      <ModelFeatureTags functionCall={functionCall} tokens={contextWindowTokens} vision={vision} />
    </Flexbox>
  );
});

export default SuggestionItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/@topic/features/SkeletonList.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css, prefixCls }) => ({
  container: css`
    display: flex;
    flex-direction: column;
    justify-content: center;

    height: 44px;
    padding-block: 8px;
    padding-inline: 12px;

    .${prefixCls}-skeleton-content {
      display: flex;
      flex-direction: column;
    }
  `,

  paragraph: css`
    > li {
      height: 24px !important;
    }
  `,
}));

export const Placeholder = memo(() => {
  const { styles } = useStyles();

  return (
    <Skeleton
      active
      avatar={false}
      className={styles.container}
      paragraph={{
        className: styles.paragraph,
        rows: 1,
        style: { marginBottom: 0 },
        width: '100%',
      }}
      title={false}
    />
  );
});

export const SkeletonList = memo(() => (
  <Flexbox style={{ paddingTop: 6 }}>
    {Array.from({ length: 6 }).map((_, i) => (
      <Placeholder key={i} />
    ))}
  </Flexbox>
));

export default SkeletonList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/@topic/features/TopicListContent/ThreadItem/Content.tsx
================================================================================

import { ActionIcon, EditableText, Icon } from '@lobehub/ui';
import { App, Dropdown, type MenuProps, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { MoreVertical, PencilLine, Trash } from 'lucide-react';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import BubblesLoading from '@/components/BubblesLoading';
import { LOADING_FLAT } from '@/const/message';
import { useIsMobile } from '@/hooks/useIsMobile';
import { useChatStore } from '@/store/chat';

const useStyles = createStyles(({ css, token }) => ({
  active: css`
    color: ${token.colorText};
  `,
  content: css`
    position: relative;
    overflow: hidden;
    flex: 1;
  `,
  title: css`
    flex: 1;

    height: 28px;

    line-height: 28px;
    color: ${token.colorTextSecondary};
    text-align: start;
  `,
}));
const { Paragraph } = Typography;

interface TopicContentProps {
  active?: boolean;
  id: string;
  showMore?: boolean;
  title: string;
}

const Content = memo<TopicContentProps>(({ id, title, active, showMore }) => {
  const { t } = useTranslation(['thread', 'common']);

  const mobile = useIsMobile();

  const [editing, updateThreadTitle, removeThread] = useChatStore((s) => [
    s.threadRenamingId === id,
    s.updateThreadTitle,
    s.removeThread,
  ]);
  const { styles, cx } = useStyles();

  const toggleEditing = (visible?: boolean) => {
    useChatStore.setState({ threadRenamingId: visible ? id : '' });
  };

  const { modal } = App.useApp();

  const items = useMemo<MenuProps['items']>(
    () => [
      {
        icon: <Icon icon={PencilLine} />,
        key: 'rename',
        label: t('rename', { ns: 'common' }),
        onClick: () => {
          toggleEditing(true);
        },
      },
      {
        type: 'divider',
      },
      {
        danger: true,
        icon: <Icon icon={Trash} />,
        key: 'delete',
        label: t('delete', { ns: 'common' }),
        onClick: () => {
          if (!id) return;

          modal.confirm({
            centered: true,
            okButtonProps: { danger: true },
            onOk: async () => {
              await removeThread(id);
            },
            title: t('actions.confirmRemoveThread'),
          });
        },
      },
    ],
    [],
  );

  return (
    <Flexbox
      align={'center'}
      gap={8}
      horizontal
      justify={'space-between'}
      onDoubleClick={(e) => {
        if (!id) return;
        if (e.altKey) toggleEditing(true);
      }}
    >
      {!editing ? (
        title === LOADING_FLAT ? (
          <Flexbox flex={1} height={28} justify={'center'}>
            <BubblesLoading />
          </Flexbox>
        ) : (
          <Paragraph
            className={cx(styles.title, active && styles.active)}
            ellipsis={{ rows: 1, tooltip: { placement: 'left', title } }}
            style={{ margin: 0 }}
          >
            {title}
          </Paragraph>
        )
      ) : (
        <EditableText
          editing={editing}
          onChangeEnd={(v) => {
            if (title !== v) {
              updateThreadTitle(id, v);
            }
            toggleEditing(false);
          }}
          onEditingChange={toggleEditing}
          showEditIcon={false}
          size={'small'}
          style={{
            height: 28,
          }}
          type={'pure'}
          value={title}
        />
      )}
      {(showMore || mobile) && !editing && (
        <Dropdown
          arrow={false}
          menu={{
            items: items,
            onClick: ({ domEvent }) => {
              domEvent.stopPropagation();
            },
          }}
          trigger={['click']}
        >
          <ActionIcon
            className="topic-more"
            icon={MoreVertical}
            onClick={(e) => {
              e.stopPropagation();
            }}
            size={'small'}
          />
        </Dropdown>
      )}
    </Flexbox>
  );
});

export default Content;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/@topic/features/TopicListContent/TopicItem/TopicContent.tsx
================================================================================

import { ActionIcon, EditableText, Icon } from '@lobehub/ui';
import { App, Dropdown, type MenuProps, Typography } from 'antd';
import { createStyles } from 'antd-style';
import {
  LucideCopy,
  LucideLoader2,
  MoreVertical,
  PencilLine,
  Star,
  Trash,
  Wand2,
} from 'lucide-react';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import BubblesLoading from '@/components/BubblesLoading';
import { LOADING_FLAT } from '@/const/message';
import { useIsMobile } from '@/hooks/useIsMobile';
import { useChatStore } from '@/store/chat';

const useStyles = createStyles(({ css }) => ({
  content: css`
    position: relative;
    overflow: hidden;
    flex: 1;
  `,
  title: css`
    flex: 1;
    height: 28px;
    line-height: 28px;
    text-align: start;
  `,
}));
const { Paragraph } = Typography;

interface TopicContentProps {
  fav?: boolean;
  id: string;
  showMore?: boolean;
  title: string;
}

const TopicContent = memo<TopicContentProps>(({ id, title, fav, showMore }) => {
  const { t } = useTranslation(['topic', 'common']);

  const mobile = useIsMobile();

  const [
    editing,
    favoriteTopic,
    updateTopicTitle,
    removeTopic,
    autoRenameTopicTitle,
    duplicateTopic,
    isLoading,
  ] = useChatStore((s) => [
    s.topicRenamingId === id,
    s.favoriteTopic,
    s.updateTopicTitle,
    s.removeTopic,
    s.autoRenameTopicTitle,
    s.duplicateTopic,
    s.topicLoadingIds.includes(id),
  ]);
  const { styles, theme } = useStyles();

  const toggleEditing = (visible?: boolean) => {
    useChatStore.setState({ topicRenamingId: visible ? id : '' });
  };

  const { modal } = App.useApp();

  const items = useMemo<MenuProps['items']>(
    () => [
      {
        icon: <Icon icon={Wand2} />,
        key: 'autoRename',
        label: t('actions.autoRename'),
        onClick: () => {
          autoRenameTopicTitle(id);
        },
      },
      {
        icon: <Icon icon={PencilLine} />,
        key: 'rename',
        label: t('rename', { ns: 'common' }),
        onClick: () => {
          toggleEditing(true);
        },
      },
      {
        type: 'divider',
      },
      {
        icon: <Icon icon={LucideCopy} />,
        key: 'duplicate',
        label: t('actions.duplicate'),
        onClick: () => {
          duplicateTopic(id);
        },
      },
      // {
      //   icon: <Icon icon={LucideDownload} />,
      //   key: 'export',
      //   label: t('topic.actions.export'),
      //   onClick: () => {
      //     configService.exportSingleTopic(sessionId, id);
      //   },
      // },
      {
        type: 'divider',
      },
      // {
      //   icon: <Icon icon={Share2} />,
      //   key: 'share',
      //   label: t('share'),
      // },
      {
        danger: true,
        icon: <Icon icon={Trash} />,
        key: 'delete',
        label: t('delete', { ns: 'common' }),
        onClick: () => {
          if (!id) return;

          modal.confirm({
            centered: true,
            okButtonProps: { danger: true },
            onOk: async () => {
              await removeTopic(id);
            },
            title: t('actions.confirmRemoveTopic'),
          });
        },
      },
    ],
    [],
  );

  return (
    <Flexbox
      align={'center'}
      gap={8}
      horizontal
      justify={'space-between'}
      onDoubleClick={(e) => {
        if (!id) return;
        if (e.altKey) toggleEditing(true);
      }}
    >
      <ActionIcon
        color={fav && !isLoading ? theme.colorWarning : undefined}
        fill={fav && !isLoading ? theme.colorWarning : 'transparent'}
        icon={isLoading ? LucideLoader2 : Star}
        onClick={(e) => {
          e.stopPropagation();
          if (!id) return;
          favoriteTopic(id, !fav);
        }}
        size={'small'}
        spin={isLoading}
      />
      {!editing ? (
        title === LOADING_FLAT ? (
          <Flexbox flex={1} height={28} justify={'center'}>
            <BubblesLoading />
          </Flexbox>
        ) : (
          <Paragraph
            className={styles.title}
            ellipsis={{ rows: 1, tooltip: { placement: 'left', title } }}
            style={{ margin: 0 }}
          >
            {title}
          </Paragraph>
        )
      ) : (
        <EditableText
          editing={editing}
          onChangeEnd={(v) => {
            if (title !== v) {
              updateTopicTitle(id, v);
            }
            toggleEditing(false);
          }}
          onEditingChange={toggleEditing}
          showEditIcon={false}
          size={'small'}
          style={{
            height: 28,
          }}
          type={'pure'}
          value={title}
        />
      )}
      {(showMore || mobile) && !editing && (
        <Dropdown
          arrow={false}
          menu={{
            items: items,
            onClick: ({ domEvent }) => {
              domEvent.stopPropagation();
            },
          }}
          trigger={['click']}
        >
          <ActionIcon
            className="topic-more"
            icon={MoreVertical}
            onClick={(e) => {
              e.stopPropagation();
            }}
            size={'small'}
          />
        </Dropdown>
      )}
    </Flexbox>
  );
});

export default TopicContent;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/@topic/features/TopicListContent/TopicItem/DefaultContent.tsx
================================================================================

import { Icon, Tag } from '@lobehub/ui';
import { Typography } from 'antd';
import { useTheme } from 'antd-style';
import { MessageSquareDashed } from 'lucide-react';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

const { Paragraph } = Typography;

const DefaultContent = memo(() => {
  const { t } = useTranslation('topic');

  const theme = useTheme();

  return (
    <Flexbox align={'center'} gap={8} horizontal>
      <Flexbox align={'center'} height={24} justify={'center'} width={24}>
        <Icon color={theme.colorTextDescription} icon={MessageSquareDashed} />
      </Flexbox>
      <Paragraph ellipsis={{ rows: 1 }} style={{ margin: 0 }}>
        {t('defaultTitle')}
      </Paragraph>
      <Tag>{t('temp')}</Tag>
    </Flexbox>
  );
});

export default DefaultContent;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/_layout/Desktop/TopicPanel.tsx
================================================================================

'use client';

import { DraggablePanel, DraggablePanelContainer } from '@lobehub/ui';
import { createStyles, useResponsive } from 'antd-style';
import isEqual from 'fast-deep-equal';
import { PropsWithChildren, memo, useEffect, useState } from 'react';

import { CHAT_SIDEBAR_WIDTH } from '@/const/layoutTokens';
import { useChatStore } from '@/store/chat';
import { chatPortalSelectors } from '@/store/chat/slices/portal/selectors';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';

const useStyles = createStyles(({ css, token }) => ({
  content: css`
    display: flex;
    flex-direction: column;
    height: 100% !important;
  `,
  drawer: css`
    z-index: 10;
    background: ${token.colorBgLayout};
  `,
  header: css`
    border-block-end: 1px solid ${token.colorBorder};
  `,
}));

const TopicPanel = memo(({ children }: PropsWithChildren) => {
  const { styles } = useStyles();
  const { md = true, lg = true } = useResponsive();
  const [showTopic, toggleConfig] = useGlobalStore((s) => [
    systemStatusSelectors.showChatSideBar(s),
    s.toggleChatSideBar,
  ]);
  const showPortal = useChatStore(chatPortalSelectors.showPortal);

  const [cacheExpand, setCacheExpand] = useState<boolean>(Boolean(showTopic));

  const handleExpand = (expand: boolean) => {
    if (isEqual(expand, Boolean(showTopic))) return;
    toggleConfig(expand);
    setCacheExpand(expand);
  };

  useEffect(() => {
    if (lg && cacheExpand) toggleConfig(true);
    if (!lg) toggleConfig(false);
  }, [lg, cacheExpand]);

  return (
    !showPortal && (
      <DraggablePanel
        className={styles.drawer}
        classNames={{
          content: styles.content,
        }}
        expand={showTopic}
        minWidth={CHAT_SIDEBAR_WIDTH}
        mode={md ? 'fixed' : 'float'}
        onExpandChange={handleExpand}
        placement={'right'}
        showHandlerWideArea={false}
      >
        <DraggablePanelContainer
          style={{
            flex: 'none',
            height: '100%',
            maxHeight: '100vh',
            minWidth: CHAT_SIDEBAR_WIDTH,
          }}
        >
          {children}
        </DraggablePanelContainer>
      </DraggablePanel>
    )
  );
});

export default TopicPanel;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/_layout/Desktop/Portal.tsx
================================================================================

'use client';

import { DraggablePanel, DraggablePanelContainer } from '@lobehub/ui';
import { createStyles, useResponsive } from 'antd-style';
import { rgba } from 'polished';
import { PropsWithChildren, memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import {
  CHAT_PORTAL_MAX_WIDTH,
  CHAT_PORTAL_TOOL_UI_WIDTH,
  CHAT_PORTAL_WIDTH,
} from '@/const/layoutTokens';
import { useChatStore } from '@/store/chat';
import { chatPortalSelectors, portalThreadSelectors } from '@/store/chat/selectors';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  content: css`
    display: flex;
    flex-direction: column;
    height: 100% !important;
  `,
  drawer: css`
    z-index: 10;
    background: ${token.colorBgLayout};
  `,
  header: css`
    border-block-end: 1px solid ${token.colorBorder};
  `,
  panel: css`
    overflow: hidden;
    height: 100%;
    background: ${isDarkMode ? rgba(token.colorBgElevated, 0.8) : token.colorBgElevated};
  `,

  thread: css`
    background: ${token.colorBgLayout};
  `,
}));

const PortalPanel = memo(({ children }: PropsWithChildren) => {
  const { styles, cx } = useStyles();
  const { md = true } = useResponsive();

  const [showInspector, showToolUI, showArtifactUI, showThread] = useChatStore((s) => [
    chatPortalSelectors.showPortal(s),
    chatPortalSelectors.showPluginUI(s),
    chatPortalSelectors.showArtifactUI(s),
    portalThreadSelectors.showThread(s),
  ]);

  return (
    showInspector && (
      <DraggablePanel
        className={styles.drawer}
        classNames={{
          content: styles.content,
        }}
        expand
        hanlderStyle={{ display: 'none' }}
        maxWidth={CHAT_PORTAL_MAX_WIDTH}
        minWidth={
          showArtifactUI || showToolUI || showThread ? CHAT_PORTAL_TOOL_UI_WIDTH : CHAT_PORTAL_WIDTH
        }
        mode={md ? 'fixed' : 'float'}
        placement={'right'}
        showHandlerWhenUnexpand={false}
        showHandlerWideArea={false}
      >
        <DraggablePanelContainer
          style={{
            flex: 'none',
            height: '100%',
            maxHeight: '100vh',
            minWidth: CHAT_PORTAL_WIDTH,
          }}
        >
          <Flexbox className={cx(styles.panel, showThread && styles.thread)}>{children}</Flexbox>
        </DraggablePanelContainer>
      </DraggablePanel>
    )
  );
});

export default PortalPanel;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/_layout/Desktop/ChatHeader/Main.tsx
================================================================================

'use client';

import { ActionIcon, Avatar, ChatHeaderTitle } from '@lobehub/ui';
import { Skeleton } from 'antd';
import { PanelLeftClose, PanelLeftOpen } from 'lucide-react';
import { parseAsBoolean, useQueryState } from 'nuqs';
import { Suspense, memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { DESKTOP_HEADER_ICON_SIZE } from '@/const/layoutTokens';
import { useOpenChatSettings } from '@/hooks/useInterceptingRoutes';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';
import { useSessionStore } from '@/store/session';
import { sessionMetaSelectors, sessionSelectors } from '@/store/session/selectors';

import { useInitAgentConfig } from '../../useInitAgentConfig';
import Tags from './Tags';

const Main = memo(() => {
  const { t } = useTranslation('chat');

  useInitAgentConfig();
  const [isPinned] = useQueryState('pinned', parseAsBoolean);

  const [init, isInbox, title, description, avatar, backgroundColor] = useSessionStore((s) => [
    sessionSelectors.isSomeSessionActive(s),
    sessionSelectors.isInboxSession(s),
    sessionMetaSelectors.currentAgentTitle(s),
    sessionMetaSelectors.currentAgentDescription(s),
    sessionMetaSelectors.currentAgentAvatar(s),
    sessionMetaSelectors.currentAgentBackgroundColor(s),
  ]);

  const openChatSettings = useOpenChatSettings();

  const displayTitle = isInbox ? t('inbox.title') : title;
  const displayDesc = isInbox ? t('inbox.desc') : description;
  const showSessionPanel = useGlobalStore(systemStatusSelectors.showSessionPanel);
  const updateSystemStatus = useGlobalStore((s) => s.updateSystemStatus);

  return !init ? (
    <Flexbox horizontal>
      <Skeleton
        active
        avatar={{ shape: 'circle', size: 'default' }}
        paragraph={false}
        title={{ style: { margin: 0, marginTop: 8 }, width: 200 }}
      />
    </Flexbox>
  ) : (
    <Flexbox align={'center'} gap={4} horizontal>
      {!isPinned && (
        <ActionIcon
          aria-label={t('agents')}
          icon={showSessionPanel ? PanelLeftClose : PanelLeftOpen}
          onClick={() => {
            updateSystemStatus({
              sessionsWidth: showSessionPanel ? 0 : 320,
              showSessionPanel: !showSessionPanel,
            });
          }}
          size={DESKTOP_HEADER_ICON_SIZE}
          title={t('agents')}
        />
      )}
      <Avatar
        avatar={avatar}
        background={backgroundColor}
        onClick={() => openChatSettings()}
        size={40}
        title={title}
      />
      <ChatHeaderTitle desc={displayDesc} tag={<Tags />} title={displayTitle} />
    </Flexbox>
  );
});

export default () => (
  <Suspense
    fallback={
      <Skeleton
        active
        avatar={{ shape: 'circle', size: 'default' }}
        paragraph={false}
        title={{ style: { margin: 0, marginTop: 8 }, width: 200 }}
      />
    }
  >
    <Main />
  </Suspense>
);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/(workspace)/@conversation/features/ChatList/WelcomeChatItem/InboxWelcome/AgentsSuggest.tsx
================================================================================

'use client';

import { ActionIcon, Avatar, Grid } from '@lobehub/ui';
import { Skeleton, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { RefreshCw } from 'lucide-react';
import Link from 'next/link';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';
import useSWR from 'swr';
import urlJoin from 'url-join';

import { assistantService } from '@/services/assistant';
import { useUserStore } from '@/store/user';
import { userGeneralSettingsSelectors } from '@/store/user/selectors';
import { DiscoverAssistantItem } from '@/types/discover';

const { Paragraph } = Typography;

const useStyles = createStyles(({ css, token, responsive }) => ({
  card: css`
    position: relative;

    overflow: hidden;

    height: 100%;
    min-height: 110px;
    padding: 16px;

    color: ${token.colorText};

    background: ${token.colorBgContainer};
    border-radius: ${token.borderRadius}px;

    &:hover {
      background: ${token.colorBgElevated};
    }

    ${responsive.mobile} {
      min-height: 72px;
    }
  `,
  cardDesc: css`
    margin-block: 0 !important;
    color: ${token.colorTextDescription};
  `,
  cardTitle: css`
    margin-block: 0 !important;
    font-size: 16px;
    font-weight: bold;
  `,
  icon: css`
    color: ${token.colorTextSecondary};
  `,
  title: css`
    color: ${token.colorTextDescription};
  `,
}));

const AgentsSuggest = memo<{ mobile?: boolean }>(({ mobile }) => {
  const { t } = useTranslation('welcome');
  const locale = useUserStore(userGeneralSettingsSelectors.currentLanguage);
  const [sliceStart, setSliceStart] = useState(0);

  const { data: assistantList, isLoading } = useSWR(
    ['assistant-list', locale].join('-'),
    async () => await assistantService.getAssistantList(),
    {
      refreshWhenOffline: false,
      revalidateOnFocus: false,
      revalidateOnReconnect: false,
    },
  );

  const { styles } = useStyles();

  const agentLength = mobile ? 2 : 4;

  const loadingCards = Array.from({ length: agentLength }).map((_, index) => (
    <Flexbox className={styles.card} key={index}>
      <Skeleton active avatar paragraph={{ rows: 2 }} title={false} />
    </Flexbox>
  ));

  const handleRefresh = () => {
    if (!assistantList) return;
    setSliceStart(Math.floor((Math.random() * assistantList.length) / 2));
  };

  return (
    <Flexbox gap={8} width={'100%'}>
      <Flexbox align={'center'} horizontal justify={'space-between'}>
        <div className={styles.title}>{t('guide.agents.title')}</div>
        <ActionIcon
          icon={RefreshCw}
          onClick={handleRefresh}
          size={{ blockSize: 24, fontSize: 14 }}
          title={t('guide.agents.replaceBtn')}
        />
      </Flexbox>
      <Grid gap={8} rows={2}>
        {isLoading || !assistantList
          ? loadingCards
          : assistantList
              .slice(sliceStart, sliceStart + agentLength)
              .map((item: DiscoverAssistantItem) => (
                <Link href={urlJoin('/discover/assistant/', item.identifier)} key={item.identifier}>
                  <Flexbox className={styles.card} gap={8} horizontal>
                    <Avatar avatar={item.meta.avatar} style={{ flex: 'none' }} />
                    <Flexbox gap={mobile ? 2 : 8} style={{ overflow: 'hidden', width: '100%' }}>
                      <Paragraph className={styles.cardTitle} ellipsis={{ rows: 1 }}>
                        {item.meta.title}
                      </Paragraph>
                      <Paragraph className={styles.cardDesc} ellipsis={{ rows: mobile ? 1 : 2 }}>
                        {item.meta.description}
                      </Paragraph>
                    </Flexbox>
                  </Flexbox>
                </Link>
              ))}
      </Grid>
    </Flexbox>
  );
});

export default AgentsSuggest;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/settings/loading.tsx
================================================================================

import SkeletonLoading from '@/components/Loading/SkeletonLoading';

export default () => <SkeletonLoading paragraph={{ rows: 8 }} />;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/features/Migration/UpgradeButton.tsx
================================================================================

import { Button } from 'antd';
import { ReactNode, memo } from 'react';
import { useTranslation } from 'react-i18next';

import { configService } from '@/services/config';
import { useChatStore } from '@/store/chat';
import { useSessionStore } from '@/store/session';

import { MigrationError, UpgradeStatus } from './const';

export interface UpgradeButtonProps {
  children?: ReactNode;
  primary?: boolean;
  setError: (error: MigrationError) => void;
  setUpgradeStatus: (status: UpgradeStatus) => void;
  state: any;
  upgradeStatus: UpgradeStatus;
}

const UpgradeButton = memo<UpgradeButtonProps>(
  ({ setUpgradeStatus, upgradeStatus, state, setError, primary = true, children }) => {
    const { t } = useTranslation('migration');

    const refreshSession = useSessionStore((s) => s.refreshSessions);
    const [refreshMessages, refreshTopic] = useChatStore((s) => [
      s.refreshMessages,
      s.refreshTopic,
    ]);

    const upgrade = async () => {
      try {
        setUpgradeStatus(UpgradeStatus.UPGRADING);

        await configService.importConfigState({
          exportType: 'sessions',
          state: state,
          version: 7,
        });

        await refreshSession();
        await refreshMessages();
        await refreshTopic();

        localStorage.setItem('V2DB_IS_MIGRATED', '1');

        setUpgradeStatus(UpgradeStatus.UPGRADED);

        return { success: true };
      } catch (error) {
        setUpgradeStatus(UpgradeStatus.UPGRADE_FAILED);
        const err = error as { message: string; stack: string };

        setError({ message: err.message, stack: err.stack });
      }
    };

    return (
      <Button
        loading={upgradeStatus === UpgradeStatus.UPGRADING}
        onClick={upgrade}
        size={'large'}
        type={primary ? 'primary' : undefined}
      >
        {children ?? t('dbV1.action.upgrade')}
      </Button>
    );
  },
);

export default UpgradeButton;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/features/Migration/index.tsx
================================================================================

'use client';

import { Spin } from 'antd';
import dynamic from 'next/dynamic';
import { memo, useEffect, useState } from 'react';

import { isServerMode } from '@/const/version';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';

import { V2DBReader } from './DBReader';

const Modal = dynamic(() => import('./Modal'), { loading: () => <Spin fullscreen />, ssr: false });

const Migration = memo(() => {
  const [dbState, setDbState] = useState(null);
  const [open, setOpen] = useState(false);

  const isPgliteInited = useGlobalStore(systemStatusSelectors.isPgliteInited);

  const checkMigration = async () => {
    const isMigrated = localStorage.getItem('V2DB_IS_MIGRATED');
    // if db have migrated already, don't show modal
    if (isMigrated || isServerMode) return;

    const dbReader = new V2DBReader([
      'messages',
      'files',
      'plugins',
      'sessionGroups',
      'sessions',
      'topics',
      'users',
    ]);
    const data = await dbReader.readAllData();
    console.log('migration data:', data);
    const state = await dbReader.convertToImportData(data);
    console.log('import state', state);
    setDbState(state as any);
    setOpen(true);
  };

  useEffect(() => {
    if (isPgliteInited) checkMigration();
  }, [isPgliteInited]);

  return open && <Modal open={open} setOpen={setOpen} state={dbState} />;
});

export default Migration;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/_layout/Desktop/SessionPanel.tsx
================================================================================

'use client';

import { DraggablePanel, DraggablePanelContainer, type DraggablePanelProps } from '@lobehub/ui';
import { createStyles, useResponsive } from 'antd-style';
import isEqual from 'fast-deep-equal';
import { parseAsBoolean, useQueryState } from 'nuqs';
import { PropsWithChildren, memo, useEffect, useState } from 'react';

import { FOLDER_WIDTH } from '@/const/layoutTokens';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';

export const useStyles = createStyles(({ css, token }) => ({
  panel: css`
    height: 100%;
    color: ${token.colorTextSecondary};
    background: ${token.colorBgContainer};
  `,
}));

const SessionPanel = memo<PropsWithChildren>(({ children }) => {
  const { md = true } = useResponsive();

  const [isPinned] = useQueryState('pinned', parseAsBoolean);

  const { styles } = useStyles();
  const [sessionsWidth, sessionExpandable, updatePreference] = useGlobalStore((s) => [
    systemStatusSelectors.sessionWidth(s),
    systemStatusSelectors.showSessionPanel(s),
    s.updateSystemStatus,
  ]);

  const [cacheExpand, setCacheExpand] = useState<boolean>(Boolean(sessionExpandable));
  const [tmpWidth, setWidth] = useState(sessionsWidth);
  if (tmpWidth !== sessionsWidth) setWidth(sessionsWidth);

  const handleExpand = (expand: boolean) => {
    if (isEqual(expand, sessionExpandable)) return;
    updatePreference({ showSessionPanel: expand });
    setCacheExpand(expand);
  };

  const handleSizeChange: DraggablePanelProps['onSizeChange'] = (_, size) => {
    if (!size) return;
    const nextWidth = typeof size.width === 'string' ? Number.parseInt(size.width) : size.width;
    if (!nextWidth) return;

    if (isEqual(nextWidth, sessionsWidth)) return;
    setWidth(nextWidth);
    updatePreference({ sessionsWidth: nextWidth });
  };

  useEffect(() => {
    if (md && cacheExpand) updatePreference({ showSessionPanel: true });
    if (!md) updatePreference({ showSessionPanel: false });
  }, [md, cacheExpand]);

  return (
    <DraggablePanel
      className={styles.panel}
      defaultSize={{ width: tmpWidth }}
      // 当进入 pin 模式下，不可展开
      expand={!isPinned && sessionExpandable}
      expandable={!isPinned}
      maxWidth={400}
      minWidth={FOLDER_WIDTH}
      mode={md ? 'fixed' : 'float'}
      onExpandChange={handleExpand}
      onSizeChange={handleSizeChange}
      placement="left"
      size={{ height: '100%', width: sessionsWidth }}
    >
      <DraggablePanelContainer style={{ flex: 'none', height: '100%', minWidth: FOLDER_WIDTH }}>
        {children}
      </DraggablePanelContainer>
    </DraggablePanel>
  );
});

export default SessionPanel;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/@session/features/SkeletonList.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css }) => ({
  paragraph: css`
    height: 12px !important;
    margin-block-start: 12px !important;

    > li {
      height: 12px !important;
    }
  `,
  title: css`
    height: 14px !important;
    margin-block: 4px 12px !important;

    > li {
      height: 14px !important;
    }
  `,
}));

interface SkeletonListProps {
  count?: number;
}

const SkeletonList = memo<SkeletonListProps>(({ count = 4 }) => {
  const { styles } = useStyles();

  const list = Array.from({ length: count }).fill('');

  return (
    <Flexbox gap={8} paddingInline={16}>
      {list.map((_, index) => (
        <Skeleton
          active
          avatar
          key={index}
          paragraph={{ className: styles.paragraph, rows: 1 }}
          title={{ className: styles.title }}
        />
      ))}
    </Flexbox>
  );
});
export default SkeletonList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/@session/features/SessionListContent/Modals/ConfigGroupModal/GroupItem.tsx
================================================================================

import { ActionIcon, EditableText, SortableList } from '@lobehub/ui';
import { App } from 'antd';
import { createStyles } from 'antd-style';
import { PencilLine, Trash } from 'lucide-react';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';

import { useSessionStore } from '@/store/session';
import { SessionGroupItem } from '@/types/session';

const useStyles = createStyles(({ css }) => ({
  content: css`
    position: relative;
    overflow: hidden;
    flex: 1;
  `,
  title: css`
    flex: 1;
    height: 28px;
    line-height: 28px;
    text-align: start;
  `,
}));

const GroupItem = memo<SessionGroupItem>(({ id, name }) => {
  const { t } = useTranslation('chat');
  const { styles } = useStyles();
  const { message, modal } = App.useApp();

  const [editing, setEditing] = useState(false);
  const [updateSessionGroupName, removeSessionGroup] = useSessionStore((s) => [
    s.updateSessionGroupName,
    s.removeSessionGroup,
  ]);

  return (
    <>
      <SortableList.DragHandle />
      {!editing ? (
        <>
          <span className={styles.title}>{name}</span>
          <ActionIcon icon={PencilLine} onClick={() => setEditing(true)} size={'small'} />
          <ActionIcon
            icon={Trash}
            onClick={() => {
              modal.confirm({
                centered: true,
                okButtonProps: {
                  danger: true,
                  type: 'primary',
                },
                onOk: async () => {
                  await removeSessionGroup(id);
                },
                title: t('sessionGroup.confirmRemoveGroupAlert'),
              });
            }}
            size={'small'}
          />
        </>
      ) : (
        <EditableText
          editing={editing}
          onChangeEnd={async (input) => {
            if (name !== input) {
              if (!input) return;
              if (input.length === 0 || input.length > 20)
                return message.warning(t('sessionGroup.tooLong'));

              await updateSessionGroupName(id, input);
              message.success(t('sessionGroup.renameSuccess'));
            }
            setEditing(false);
          }}
          onEditingChange={(e) => setEditing(e)}
          showEditIcon={false}
          size={'small'}
          style={{
            height: 28,
          }}
          type={'pure'}
          value={name}
        />
      )}
    </>
  );
});

export default GroupItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/chat/@session/_layout/Desktop/PanelBody.tsx
================================================================================

'use client';

import { DraggablePanelBody } from '@lobehub/ui';
import { createStyles } from 'antd-style';
import { PropsWithChildren, memo } from 'react';

const useStyles = createStyles(
  ({ css }) => css`
    display: flex;
    flex-direction: column;
    gap: 2px;

    padding-block: 8px 0;
    padding-inline: 8px;
  `,
);

const PanelBody = memo<PropsWithChildren>(({ children }) => {
  const { styles } = useStyles();

  return <DraggablePanelBody className={styles}>{children}</DraggablePanelBody>;
});

export default PanelBody;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/changelog/loading.tsx
================================================================================

import { Skeleton } from 'antd';

export default () => <Skeleton active paragraph={{ rows: 5 }} title={false} />;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/changelog/page.tsx
================================================================================

import { Divider, Skeleton } from 'antd';
import { notFound } from 'next/navigation';
import { Fragment, Suspense } from 'react';
import { Flexbox } from 'react-layout-kit';
import urlJoin from 'url-join';

import Pagination from '@/app/@modal/(.)changelog/modal/features/Pagination';
import StructuredData from '@/components/StructuredData';
import { serverFeatureFlags } from '@/config/featureFlags';
import { BRANDING_NAME } from '@/const/branding';
import { OFFICIAL_SITE } from '@/const/url';
import { ldModule } from '@/server/ld';
import { metadataModule } from '@/server/metadata';
import { ChangelogService } from '@/server/services/changelog';
import { translation } from '@/server/translation';
import { isMobileDevice } from '@/utils/server/responsive';

import GridLayout from './features/GridLayout';
import Post from './features/Post';

export const generateMetadata = async () => {
  const { t } = await translation('metadata');
  return metadataModule.generate({
    canonical: urlJoin(OFFICIAL_SITE, 'changelog'),
    description: t('changelog.description', { appName: BRANDING_NAME }),
    title: t('changelog.title'),
    url: '/changelog',
  });
};

const Page = async () => {
  const hideDocs = serverFeatureFlags().hideDocs;

  if (hideDocs) return notFound();

  const mobile = await isMobileDevice();
  const { t, locale } = await translation('metadata');
  const changelogService = new ChangelogService();
  const data = await changelogService.getChangelogIndex();

  if (!data) return notFound();

  const ld = ldModule.generate({
    description: t('changelog.description', { appName: BRANDING_NAME }),
    title: t('changelog.title', { appName: BRANDING_NAME }),
    url: '/changelog',
  });

  return (
    <>
      <StructuredData ld={ld} />
      <Flexbox gap={mobile ? 16 : 48}>
        {data?.map((item) => (
          <Fragment key={item.id}>
            <Suspense
              fallback={
                <GridLayout>
                  <Divider />
                  <Skeleton active paragraph={{ rows: 5 }} />
                </GridLayout>
              }
            >
              <Post locale={locale} mobile={mobile} {...item} />
            </Suspense>
          </Fragment>
        ))}
      </Flexbox>
      <GridLayout>
        <Pagination />
      </GridLayout>
    </>
  );
};

export default Page;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/profile/loading.tsx
================================================================================

import { Flexbox } from 'react-layout-kit';

import SkeletonLoading from '@/components/Loading/SkeletonLoading';
import { isMobileDevice } from '@/utils/server/responsive';

const Loading = async () => {
  const mobile = await isMobileDevice();
  if (mobile) return <SkeletonLoading paragraph={{ rows: 8 }} />;
  return (
    <Flexbox horizontal style={{ position: 'relative' }} width={'100%'}>
      <Flexbox padding={24} width={256}>
        <SkeletonLoading paragraph={{ rows: 8 }} />;
      </Flexbox>
      <Flexbox align={'center'} flex={1}>
        <Flexbox padding={24} style={{ maxWidth: 1024 }} width={'100%'}>
          <SkeletonLoading paragraph={{ rows: 8 }} />;
        </Flexbox>
      </Flexbox>
    </Flexbox>
  );
};

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/files/features/FileDetail.tsx
================================================================================

'use client';

import { Icon } from '@lobehub/ui';
import { Descriptions, Divider, Tag } from 'antd';
import { useTheme } from 'antd-style';
import dayjs from 'dayjs';
import { BoltIcon } from 'lucide-react';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { FileListItem } from '@/types/files';
import { formatSize } from '@/utils/format';

export const DETAIL_PANEL_WIDTH = 300;

const FileDetail = memo<FileListItem>((props) => {
  const { name, embeddingStatus, size, createdAt, updatedAt, chunkCount } = props || {};
  const { t } = useTranslation('file');
  const theme = useTheme();

  if (!props) return null;

  const items = [
    { children: name, key: 'name', label: t('detail.basic.filename') },
    { children: formatSize(size), key: 'size', label: t('detail.basic.size') },
    {
      children: name.split('.').pop()?.toUpperCase(),
      key: 'type',
      label: t('detail.basic.type'),
    },

    {
      children: dayjs(createdAt).format('YYYY-MM-DD HH:mm'),
      key: 'createdAt',
      label: t('detail.basic.createdAt'),
    },
    {
      children: dayjs(updatedAt).format('YYYY-MM-DD HH:mm'),
      key: 'updatedAt',
      label: t('detail.basic.updatedAt'),
    },
  ];

  const dataItems = [
    {
      children: (
        <Tag bordered={false} icon={<Icon icon={BoltIcon} />}>
          {' '}
          {chunkCount}
        </Tag>
      ),
      key: 'chunkCount',
      label: t('detail.data.chunkCount'),
    },
    {
      children: (
        <Tag bordered={false} color={embeddingStatus || 'default'}>
          {t(`detail.data.embedding.${embeddingStatus || 'default'}`)}
        </Tag>
      ),
      key: 'embeddingStatus',
      label: t('detail.data.embeddingStatus'),
    },
  ];

  return (
    <Flexbox
      padding={16}
      style={{ borderInlineStart: `1px solid ${theme.colorSplit}` }}
      width={DETAIL_PANEL_WIDTH}
    >
      <Descriptions
        colon={false}
        column={1}
        items={items}
        labelStyle={{ width: 120 }}
        size={'small'}
        title={t('detail.basic.title')}
      />
      <Divider />
      <Descriptions
        colon={false}
        column={1}
        items={dataItems}
        labelStyle={{ width: 120 }}
        size={'small'}
      />
    </Flexbox>
  );
});

export default FileDetail;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/files/(content)/@menu/features/KnowledgeBase/SkeletonList.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css, prefixCls }) => ({
  container: css`
    display: flex;
    flex-direction: column;
    justify-content: center;

    height: 36px;
    padding: 8px;

    .${prefixCls}-skeleton-content {
      display: flex;
      flex-direction: column;
    }
  `,

  paragraph: css`
    > li {
      height: 20px !important;
    }
  `,
}));

export const Placeholder = memo(() => {
  const { styles } = useStyles();

  return (
    <Skeleton
      active
      avatar={false}
      className={styles.container}
      paragraph={{
        className: styles.paragraph,
        rows: 1,
        style: { marginBottom: 0 },
        width: '100%',
      }}
      title={false}
    />
  );
});

export const SkeletonList = memo(() => (
  <Flexbox paddingInline={24}>
    {Array.from({ length: 3 }).map((_, i) => (
      <Placeholder key={i} />
    ))}
  </Flexbox>
));

export default SkeletonList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/files/(content)/@menu/features/KnowledgeBase/EmptyStatus.tsx
================================================================================

import { createStyles } from 'antd-style';
import React from 'react';
import { Trans } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css, token }) => ({
  container: css`
    font-size: 12px;
    color: ${token.colorTextTertiary};
  `,

  paragraph: css`
    justify-content: center;
    width: 100%;

    kbd {
      margin-inline: 2px;
      padding-inline: 6px;
      background: ${token.colorFillTertiary};
      border-radius: 4px;
    }
  `,
}));
const EmptyStatus = () => {
  const { styles } = useStyles();
  return (
    <Flexbox
      align={'flex-end'}
      className={styles.container}
      gap={12}
      paddingInline={20}
      width={'100%'}
    >
      <svg
        fill="currentColor"
        fillRule="evenodd"
        style={{ flex: 'none', height: 'fit-content', lineHeight: 1 }}
        viewBox="0 0 126 64"
        width={130}
        xmlns="http://www.w3.org/2000/svg"
      >
        <path d="M.5 63a.5.5 0 001 0h-1zM122 1l-2.887 5h5.774L122 1zM1.5 62.042a.5.5 0 10-1 0h1zm-1-1.917a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.916a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 101 0h-1zm1-1.916a.5.5 0 10-1 0h1zm-1-1.917a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.916a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 101 0h-1zm1.063-1.938a.5.5 0 10-.991-.13l.991.13zm-.418-2.274a.5.5 0 00.924.383l-.924-.383zm1.904-1.312a.5.5 0 10-.793-.609l.793.61zm.776-2.178a.5.5 0 00.61.793l-.61-.793zm2.304-.187a.5.5 0 00-.383-.924l.383.924zm1.761-1.497a.5.5 0 00.13.991l-.13-.991zm2.12.928a.5.5 0 100-1v1zm2.019-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 000-1v1zm2.018-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 100-1v1zm2.019-1a.5.5 0 100 1v-1zm2.019 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.018 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 100-1v1zm2.02-1a.5.5 0 100 1v-1zm2.018 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 100-1v1zm2.02-1a.5.5 0 100 1v-1zm2.018 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 100-1v1zm2.018-1a.5.5 0 100 1v-1zm2.02 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 100-1v1zm2.018-1a.5.5 0 100 1v-1zm2.02 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 000-1v1zm2.018-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 100-1v1zm2.019-1a.5.5 0 100 1v-1zm2.019 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 100 1v-1zm2.019 1a.5.5 0 100-1v1zm2.02-1a.5.5 0 100 1v-1zm2.018 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 000-1v1zm2.02-1a.5.5 0 100 1v-1zm2.018 1a.5.5 0 100-1v1zm2.02-1a.5.5 0 100 1v-1zm2.019 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.02 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.019 1a.5.5 0 000-1v1zm2.019-1a.5.5 0 000 1v-1zm2.12.928a.501.501 0 00-.13-.991l.13.991zm1.761-1.497a.501.501 0 00.383.924l-.383-.924zm2.304-.187a.5.5 0 00-.609-.793l.609.793zm.776-2.178a.5.5 0 10.793.609l-.793-.61zm1.904-1.312a.5.5 0 10-.924-.383l.924.383zm-.418-2.274a.5.5 0 10.991.13l-.991-.13zm1.063-1.938a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.916a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 001 0h-1zm1-1.916a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.916a.5.5 0 001 0h-1zm1-1.917a.5.5 0 00-1 0h1zm-1-1.917a.5.5 0 001 0h-1zM1.5 63v-.958h-1V63h1zm0-2.875v-1.917h-1v1.917h1zm0-3.833v-1.917h-1v1.917h1zm0-3.834v-1.916h-1v1.916h1zm0-3.833v-1.917h-1v1.917h1zm0-3.833v-1.917h-1v1.917h1zm0-3.834V40h-1v.958h1zm0-.958c0-.333.022-.66.063-.98l-.991-.13A8.574 8.574 0 00.5 40h1zm.569-2.87c.253-.61.584-1.18.98-1.696l-.793-.609a8.49 8.49 0 00-1.11 1.921l.923.383zm2.365-3.08a7.487 7.487 0 011.695-.981l-.383-.924a8.495 8.495 0 00-1.92 1.111l.608.793zm3.586-1.487c.32-.041.648-.063.98-.063v-1c-.376 0-.746.024-1.11.072l.13.991zM9 32.5h1.01v-1H9v1zm3.029 0h2.02v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.02v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.02v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.02v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.02v1zm4.038 0h2.019v-1h-2.019v1zm4.038 0h2.02v-1h-2.02v1zm4.039 0h2.019v-1h-2.019v1zm4.038 0H114v-1h-1.01v1zm1.01 0c.376 0 .746-.024 1.11-.072l-.13-.991c-.32.041-.648.063-.98.063v1zm3.254-.645a8.506 8.506 0 001.921-1.111l-.609-.793a7.519 7.519 0 01-1.695.98l.383.924zm3.49-2.68a8.516 8.516 0 001.111-1.921l-.924-.383a7.527 7.527 0 01-.98 1.695l.793.609zm1.684-4.066c.048-.363.072-.733.072-1.109h-1c0 .332-.022.66-.063.98l.991.13zM122.5 24v-.958h-1V24h1zm0-2.875v-1.917h-1v1.917h1zm0-3.833v-1.917h-1v1.917h1zm0-3.834v-1.916h-1v1.916h1zm0-3.833V7.708h-1v1.917h1zm0-3.833V3.875h-1v1.917h1z"></path>
      </svg>
      <Flexbox align={'center'} className={styles.paragraph} horizontal>
        <Trans i18nKey={'knowledgeBase.list.empty'} ns={'file'}>
          点击 <kbd>+</kbd> 开始创建知识库
        </Trans>
      </Flexbox>
    </Flexbox>
  );
};

export default EmptyStatus;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(main)/files/(content)/@menu/features/KnowledgeBase/Item/Content.tsx
================================================================================

import { ActionIcon, EditableText, Icon } from '@lobehub/ui';
import { App, Dropdown, type MenuProps, Typography } from 'antd';
import { createStyles } from 'antd-style';
import { LucideLoader2, MoreVertical, PencilLine, Trash } from 'lucide-react';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import BubblesLoading from '@/components/BubblesLoading';
import RepoIcon from '@/components/RepoIcon';
import { LOADING_FLAT } from '@/const/message';
import { useKnowledgeBaseStore } from '@/store/knowledgeBase';

export const knowledgeItemClass = 'knowledge-base-item';

const useStyles = createStyles(({ css }) => ({
  content: css`
    position: relative;
    overflow: hidden;
    flex: 1;
  `,
  icon: css`
    min-width: 24px;
    border-radius: 4px;
  `,
  title: css`
    flex: 1;
    height: 28px;
    line-height: 28px;
    text-align: start;
  `,
}));

const { Paragraph } = Typography;

interface KnowledgeBaseItemProps {
  id: string;
  name: string;
  showMore: boolean;
}

const Content = memo<KnowledgeBaseItemProps>(({ id, name, showMore }) => {
  const { t } = useTranslation(['file', 'common']);

  const [editing, updateKnowledgeBase, removeKnowledgeBase, isLoading] = useKnowledgeBaseStore(
    (s) => [
      s.knowledgeBaseRenamingId === id,
      s.updateKnowledgeBase,
      s.removeKnowledgeBase,
      s.knowledgeBaseLoadingIds.includes(id),
    ],
  );

  const { styles } = useStyles();

  const toggleEditing = (visible?: boolean) => {
    useKnowledgeBaseStore.setState(
      { knowledgeBaseRenamingId: visible ? id : null },
      false,
      'toggleEditing',
    );
  };

  const { modal } = App.useApp();

  const items = useMemo<MenuProps['items']>(
    () => [
      {
        icon: <Icon icon={PencilLine} />,
        key: 'rename',
        label: t('rename', { ns: 'common' }),
        onClick: () => {
          toggleEditing(true);
        },
      },
      {
        type: 'divider',
      },
      {
        danger: true,
        icon: <Icon icon={Trash} />,
        key: 'delete',
        label: t('delete', { ns: 'common' }),
        onClick: () => {
          if (!id) return;

          modal.confirm({
            centered: true,
            okButtonProps: { danger: true },
            onOk: async () => {
              await removeKnowledgeBase(id);
            },
            title: t('knowledgeBase.list.confirmRemoveKnowledgeBase'),
          });
        },
      },
    ],
    [],
  );

  return (
    <Flexbox
      align={'center'}
      gap={8}
      horizontal
      justify={'space-between'}
      onDoubleClick={(e) => {
        if (!id) return;
        if (e.altKey) toggleEditing(true);
      }}
    >
      <Center className={isLoading ? '' : styles.icon} height={24} width={24}>
        {isLoading ? <Icon icon={LucideLoader2} spin /> : <RepoIcon />}
      </Center>
      {!editing ? (
        name === LOADING_FLAT ? (
          <Flexbox flex={1} height={28} justify={'center'}>
            <BubblesLoading />
          </Flexbox>
        ) : (
          <Paragraph
            className={styles.title}
            ellipsis={{ rows: 1, tooltip: { placement: 'left', title: name } }}
            style={{ margin: 0, opacity: isLoading ? 0.6 : undefined }}
          >
            {name}
          </Paragraph>
        )
      ) : (
        <EditableText
          editing={editing}
          onChangeEnd={(v) => {
            if (name !== v) {
              updateKnowledgeBase(id, { name: v });
            }
            toggleEditing(false);
          }}
          onClick={(e) => {
            e.preventDefault();
          }}
          onEditingChange={toggleEditing}
          showEditIcon={false}
          size={'small'}
          style={{ height: 28 }}
          type={'pure'}
          value={name}
        />
      )}

      {showMore && !editing && (
        <div
          onClick={(e) => {
            e.preventDefault();
            e.stopPropagation();
          }}
        >
          <Dropdown
            arrow={false}
            menu={{
              items: items,
              onClick: ({ domEvent }) => {
                domEvent.stopPropagation();
              },
            }}
            trigger={['click']}
          >
            <ActionIcon className={knowledgeItemClass} icon={MoreVertical} size={'small'} />
          </Dropdown>
        </div>
      )}
    </Flexbox>
  );
});

export default Content;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/webapi/chat/[provider]/route.ts
================================================================================

import { checkAuth } from '@/app/(backend)/middleware/auth';
import {
  AGENT_RUNTIME_ERROR_SET,
  AgentRuntime,
  ChatCompletionErrorPayload,
} from '@/libs/agent-runtime';
import { createTraceOptions, initAgentRuntimeWithUserPayload } from '@/server/modules/AgentRuntime';
import { ChatErrorType } from '@/types/fetch';
import { ChatStreamPayload } from '@/types/openai/chat';
import { createErrorResponse } from '@/utils/errorResponse';
import { getTracePayload } from '@/utils/trace';

export const runtime = 'edge';

export const POST = checkAuth(async (req: Request, { params, jwtPayload, createRuntime }) => {
  const { provider } = await params;

  try {
    // ============  1. init chat model   ============ //
    let agentRuntime: AgentRuntime;
    if (createRuntime) {
      agentRuntime = createRuntime(jwtPayload);
    } else {
      agentRuntime = await initAgentRuntimeWithUserPayload(provider, jwtPayload);
    }

    // ============  2. create chat completion   ============ //

    const data = (await req.json()) as ChatStreamPayload;

    const tracePayload = getTracePayload(req);

    let traceOptions = {};
    // If user enable trace
    if (tracePayload?.enabled) {
      traceOptions = createTraceOptions(data, {
        provider,
        trace: tracePayload,
      });
    }

    return await agentRuntime.chat(data, { user: jwtPayload.userId, ...traceOptions });
  } catch (e) {
    const {
      errorType = ChatErrorType.InternalServerError,
      error: errorContent,
      ...res
    } = e as ChatCompletionErrorPayload;

    const error = errorContent || e;

    const logMethod = AGENT_RUNTIME_ERROR_SET.has(errorType as string) ? 'warn' : 'error';
    // track the error at server side
    console[logMethod](`Route: [${provider}] ${errorType}:`, error);

    return createErrorResponse(errorType, { error, ...res, provider });
  }
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/webapi/chat/openai/route.ts
================================================================================

import { POST as UniverseRoute } from '../[provider]/route';

export const runtime = 'edge';

export const preferredRegion = [
  'arn1',
  'bom1',
  'cdg1',
  'cle1',
  'cpt1',
  'dub1',
  'fra1',
  'gru1',
  'hnd1',
  'iad1',
  'icn1',
  'kix1',
  'lhr1',
  'pdx1',
  'sfo1',
  'sin1',
  'syd1',
];

export const POST = async (req: Request) =>
  UniverseRoute(req, { params: Promise.resolve({ provider: 'openai' }) });


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/webapi/text-to-image/[provider]/route.ts
================================================================================

import { NextResponse } from 'next/server';

import { checkAuth } from '@/app/(backend)/middleware/auth';
import { ChatCompletionErrorPayload } from '@/libs/agent-runtime';
import { TextToImagePayload } from '@/libs/agent-runtime/types';
import { initAgentRuntimeWithUserPayload } from '@/server/modules/AgentRuntime';
import { ChatErrorType } from '@/types/fetch';
import { createErrorResponse } from '@/utils/errorResponse';

export const runtime = 'edge';

export const preferredRegion = [
  'arn1',
  'bom1',
  'cdg1',
  'cle1',
  'cpt1',
  'dub1',
  'fra1',
  'gru1',
  'hnd1',
  'iad1',
  'icn1',
  'kix1',
  'lhr1',
  'pdx1',
  'sfo1',
  'sin1',
  'syd1',
];

// return NextResponse.json(
//   {
//     body: {
//       endpoint: 'https://ai****ix.com/v1',
//       error: {
//         code: 'content_policy_violation',
//         message:
//           'Your request was rejected as a result of our safety system. Image descriptions generated from your prompt may contain text that is not allowed by our safety system. If you believe this was done in error, your request may succeed if retried, or by adjusting your prompt.',
//         param: null,
//         type: 'invalid_request_error',
//       },
//       provider: 'openai',
//     },
//     errorType: 'OpenAIBizError',
//   },
//   { status: 400 },
// );

export const POST = checkAuth(async (req: Request, { params, jwtPayload }) => {
  const { provider } = await params;

  try {
    // ============  1. init chat model   ============ //
    const agentRuntime = await initAgentRuntimeWithUserPayload(provider, jwtPayload);

    // ============  2. create chat completion   ============ //

    const data = (await req.json()) as TextToImagePayload;

    const images = await agentRuntime.textToImage(data);

    return NextResponse.json(images);
  } catch (e) {
    const {
      errorType = ChatErrorType.InternalServerError,
      error: errorContent,
      ...res
    } = e as ChatCompletionErrorPayload;

    const error = errorContent || e;
    // track the error at server side
    console.error(`Route: [${provider}] ${errorType}:`, error);

    return createErrorResponse(errorType, { error, ...res, provider });
  }
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/webapi/tts/openai/route.ts
================================================================================

import { OpenAITTSPayload } from '@lobehub/tts';
import { createOpenaiAudioSpeech } from '@lobehub/tts/server';

import { createBizOpenAI } from '@/app/(backend)/_deprecated/createBizOpenAI';

export const runtime = 'edge';

export const preferredRegion = [
  'arn1',
  'bom1',
  'cdg1',
  'cle1',
  'cpt1',
  'dub1',
  'fra1',
  'gru1',
  'hnd1',
  'iad1',
  'icn1',
  'kix1',
  'lhr1',
  'pdx1',
  'sfo1',
  'sin1',
  'syd1',
];

export const POST = async (req: Request) => {
  const payload = (await req.json()) as OpenAITTSPayload;

  // need to be refactored with jwt auth mode
  const openaiOrErrResponse = createBizOpenAI(req);

  // if resOrOpenAI is a Response, it means there is an error,just return it
  if (openaiOrErrResponse instanceof Response) return openaiOrErrResponse;

  return await createOpenaiAudioSpeech({ openai: openaiOrErrResponse, payload });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/webapi/stt/openai/route.ts
================================================================================

import { OpenAISTTPayload } from '@lobehub/tts';
import { createOpenaiAudioTranscriptions } from '@lobehub/tts/server';

import { createBizOpenAI } from '@/app/(backend)/_deprecated/createBizOpenAI';

export const runtime = 'edge';

export const preferredRegion = [
  'arn1',
  'bom1',
  'cdg1',
  'cle1',
  'cpt1',
  'dub1',
  'fra1',
  'gru1',
  'hnd1',
  'iad1',
  'icn1',
  'kix1',
  'lhr1',
  'pdx1',
  'sfo1',
  'sin1',
  'syd1',
];

export const POST = async (req: Request) => {
  const formData = await req.formData();
  const speechBlob = formData.get('speech') as Blob;
  const optionsString = formData.get('options') as string;
  const payload = {
    options: JSON.parse(optionsString),
    speech: speechBlob,
  } as OpenAISTTPayload;

  const openaiOrErrResponse = createBizOpenAI(req);

  // if resOrOpenAI is a Response, it means there is an error,just return it
  if (openaiOrErrResponse instanceof Response) return openaiOrErrResponse;

  const res = await createOpenaiAudioTranscriptions({ openai: openaiOrErrResponse, payload });

  return new Response(JSON.stringify(res), {
    headers: {
      'content-type': 'application/json;charset=UTF-8',
    },
  });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/_deprecated/createBizOpenAI/createAzureOpenai.ts
================================================================================

import OpenAI, { ClientOptions } from 'openai';
import urlJoin from 'url-join';

import { getLLMConfig } from '@/config/llm';
import { ChatErrorType } from '@/types/fetch';

// create Azure OpenAI Instance
export const createAzureOpenai = (params: {
  apiVersion?: string | null;
  endpoint?: string | null;
  model: string;
  userApiKey?: string | null;
}) => {
  const { AZURE_API_VERSION, AZURE_API_KEY } = getLLMConfig();
  const OPENAI_PROXY_URL = process.env.OPENAI_PROXY_URL || '';

  const endpoint = !params.endpoint ? OPENAI_PROXY_URL : params.endpoint;
  const baseURL = urlJoin(endpoint, `/openai/deployments/${params.model.replace('.', '')}`); // refs: https://test-001.openai.azure.com/openai/deployments/gpt-35-turbo

  const defaultApiVersion = AZURE_API_VERSION || '2023-08-01-preview';
  const apiVersion = !params.apiVersion ? defaultApiVersion : params.apiVersion;
  const apiKey = !params.userApiKey ? AZURE_API_KEY : params.userApiKey;

  if (!apiKey) throw new Error('AZURE_API_KEY is empty', { cause: ChatErrorType.NoOpenAIAPIKey });

  const config: ClientOptions = {
    apiKey,
    baseURL,
    defaultHeaders: { 'api-key': apiKey },
    defaultQuery: { 'api-version': apiVersion },
  };

  return new OpenAI(config);
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/_deprecated/createBizOpenAI/createOpenai.ts
================================================================================

import OpenAI from 'openai';

import { getLLMConfig } from '@/config/llm';
import { ChatErrorType } from '@/types/fetch';

// create OpenAI instance
export const createOpenai = (userApiKey: string | null, endpoint?: string | null) => {
  const { OPENAI_API_KEY } = getLLMConfig();
  const OPENAI_PROXY_URL = process.env.OPENAI_PROXY_URL;

  const baseURL = endpoint ? endpoint : OPENAI_PROXY_URL ? OPENAI_PROXY_URL : undefined;

  const apiKey = !userApiKey ? OPENAI_API_KEY : userApiKey;

  if (!apiKey) throw new Error('OPENAI_API_KEY is empty', { cause: ChatErrorType.NoOpenAIAPIKey });

  return new OpenAI({ apiKey, baseURL });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/(backend)/_deprecated/createBizOpenAI/index.ts
================================================================================

import OpenAI from 'openai';

import { getOpenAIAuthFromRequest } from '@/const/fetch';
import { ChatErrorType, ErrorType } from '@/types/fetch';
import { createErrorResponse } from '@/utils/errorResponse';

import { checkAuth } from './auth';
import { createOpenai } from './createOpenai';

/**
 * @deprecated
 * createOpenAI Instance with Auth and azure openai support
 * if auth not pass ,just return error response
 */
export const createBizOpenAI = (req: Request): Response | OpenAI => {
  const { apiKey, accessCode, endpoint, oauthAuthorized } = getOpenAIAuthFromRequest(req);

  const result = checkAuth({ accessCode, apiKey, oauthAuthorized });

  if (!result.auth) {
    return createErrorResponse(result.error as ErrorType);
  }

  let openai: OpenAI;

  try {
    openai = createOpenai(apiKey, endpoint);
  } catch (error) {
    if ((error as Error).cause === ChatErrorType.NoOpenAIAPIKey) {
      return createErrorResponse(ChatErrorType.NoOpenAIAPIKey);
    }

    console.error(error); // log error to trace it
    return createErrorResponse(ChatErrorType.InternalServerError);
  }

  return openai;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/@modal/loading.tsx
================================================================================

import { Skeleton } from 'antd';

export default () => {
  return <Skeleton paragraph={{ rows: 6 }} style={{ padding: 56 }} />;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/@modal/(.)changelog/modal/loading.tsx
================================================================================

import { Skeleton } from 'antd';
import { Flexbox } from 'react-layout-kit';

export default () => {
  return (
    <Flexbox paddingBlock={12} paddingInline={24}>
      <Skeleton active paragraph={{ rows: 5 }} title={false} />
    </Flexbox>
  );
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/@modal/chat/(.)settings/modal/layout.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import isEqual from 'fast-deep-equal';
import dynamic from 'next/dynamic';
import { PropsWithChildren, memo } from 'react';
import { useTranslation } from 'react-i18next';

import ModalLayout from '@/app/@modal/_layout/ModalLayout';
import StoreUpdater from '@/features/AgentSetting/StoreUpdater';
import { Provider, createStore } from '@/features/AgentSetting/store';
import { useQuery } from '@/hooks/useQuery';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/slices/chat';
import { ChatSettingsTabs } from '@/store/global/initialState';
import { useSessionStore } from '@/store/session';
import { sessionMetaSelectors } from '@/store/session/selectors';

import SettingModalLayout from '../../../_layout/SettingModalLayout';

const CategoryContent = dynamic(() => import('./features/CategoryContent'), {
  loading: () => <Skeleton paragraph={{ rows: 6 }} title={false} />,
  ssr: false,
});

const Layout = memo<PropsWithChildren>(({ children }) => {
  const { tab = ChatSettingsTabs.Meta } = useQuery();
  const { t } = useTranslation('setting');
  const id = useSessionStore((s) => s.activeId);
  const config = useAgentStore(agentSelectors.currentAgentConfig, isEqual);
  const meta = useSessionStore(sessionMetaSelectors.currentAgentMeta, isEqual);
  const [updateAgentConfig] = useAgentStore((s) => [s.updateAgentConfig]);
  const [updateAgentMeta] = useSessionStore((s) => [
    s.updateSessionMeta,
    sessionMetaSelectors.currentAgentTitle(s),
  ]);

  return (
    <ModalLayout>
      <SettingModalLayout
        activeTitle={t(`agentTab.${tab as ChatSettingsTabs}`)}
        category={<CategoryContent />}
        desc={t('header.sessionDesc')}
        title={t('header.session')}
      >
        <Provider createStore={createStore}>
          <StoreUpdater
            config={config}
            id={id}
            meta={meta}
            onConfigChange={updateAgentConfig}
            onMetaChange={updateAgentMeta}
          />
          {children}
        </Provider>
      </SettingModalLayout>
    </ModalLayout>
  );
});

export default Layout;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/app/@modal/chat/(.)settings/modal/loading.tsx
================================================================================

import { Skeleton } from 'antd';

export default () => {
  return <Skeleton paragraph={{ rows: 6 }} style={{ paddingBlock: 16 }} />;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/file.ts
================================================================================

import { createEnv } from '@t3-oss/env-nextjs';
import { z } from 'zod';

const DEFAULT_S3_FILE_PATH = 'files';

export const getFileConfig = () => {
  if (!!process.env.NEXT_PUBLIC_S3_DOMAIN) {
    console.warn(
      '⚠️ `NEXT_PUBLIC_S3_DOMAIN` will be de deprecated in the next major version, please replace it with `S3_PUBLIC_DOMAIN` in your env',
    );
  }

  const S3_PUBLIC_DOMAIN = process.env.S3_PUBLIC_DOMAIN || process.env.NEXT_PUBLIC_S3_DOMAIN;

  return createEnv({
    client: {
      /**
       * @deprecated
       */
      NEXT_PUBLIC_S3_DOMAIN: z.string().optional(),
      NEXT_PUBLIC_S3_FILE_PATH: z.string().optional(),
    },
    runtimeEnv: {
      CHUNKS_AUTO_EMBEDDING: process.env.CHUNKS_AUTO_EMBEDDING !== '0',
      CHUNKS_AUTO_GEN_METADATA: process.env.CHUNKS_AUTO_GEN_METADATA !== '0',

      NEXT_PUBLIC_S3_DOMAIN: process.env.NEXT_PUBLIC_S3_DOMAIN,
      NEXT_PUBLIC_S3_FILE_PATH: process.env.NEXT_PUBLIC_S3_FILE_PATH || DEFAULT_S3_FILE_PATH,

      S3_ACCESS_KEY_ID: process.env.S3_ACCESS_KEY_ID,
      S3_BUCKET: process.env.S3_BUCKET,
      S3_ENABLE_PATH_STYLE: process.env.S3_ENABLE_PATH_STYLE === '1',
      S3_ENDPOINT: process.env.S3_ENDPOINT,
      S3_PREVIEW_URL_EXPIRE_IN: parseInt(process.env.S3_PREVIEW_URL_EXPIRE_IN || '7200'),
      S3_PUBLIC_DOMAIN,
      S3_REGION: process.env.S3_REGION,
      S3_SECRET_ACCESS_KEY: process.env.S3_SECRET_ACCESS_KEY,
      S3_SET_ACL: process.env.S3_SET_ACL !== '0',
    },
    server: {
      CHUNKS_AUTO_EMBEDDING: z.boolean(),
      CHUNKS_AUTO_GEN_METADATA: z.boolean(),

      // S3
      S3_ACCESS_KEY_ID: z.string().optional(),
      S3_BUCKET: z.string().optional(),
      S3_ENABLE_PATH_STYLE: z.boolean(),

      S3_ENDPOINT: z.string().url().optional(),
      S3_PREVIEW_URL_EXPIRE_IN: z.number(),
      S3_PUBLIC_DOMAIN: z.string().url().optional(),
      S3_REGION: z.string().optional(),
      S3_SECRET_ACCESS_KEY: z.string().optional(),
      S3_SET_ACL: z.boolean(),
    },
  });
};

export const fileEnv = getFileConfig();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/llm.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix , typescript-sort-keys/interface */
import { createEnv } from '@t3-oss/env-nextjs';
import { z } from 'zod';

export const getLLMConfig = () => {
  return createEnv({
    server: {
      API_KEY_SELECT_MODE: z.string().optional(),

      ENABLED_OPENAI: z.boolean(),
      OPENAI_API_KEY: z.string().optional(),

      ENABLED_AZURE_OPENAI: z.boolean(),
      AZURE_API_KEY: z.string().optional(),
      AZURE_API_VERSION: z.string().optional(),
      AZURE_ENDPOINT: z.string().optional(),

      ENABLED_ZHIPU: z.boolean(),
      ZHIPU_API_KEY: z.string().optional(),

      ENABLED_DEEPSEEK: z.boolean(),
      DEEPSEEK_API_KEY: z.string().optional(),

      ENABLED_GOOGLE: z.boolean(),
      GOOGLE_API_KEY: z.string().optional(),

      ENABLED_MOONSHOT: z.boolean(),
      MOONSHOT_API_KEY: z.string().optional(),

      ENABLED_PERPLEXITY: z.boolean(),
      PERPLEXITY_API_KEY: z.string().optional(),

      ENABLED_ANTHROPIC: z.boolean(),
      ANTHROPIC_API_KEY: z.string().optional(),

      ENABLED_MINIMAX: z.boolean(),
      MINIMAX_API_KEY: z.string().optional(),

      ENABLED_MISTRAL: z.boolean(),
      MISTRAL_API_KEY: z.string().optional(),

      ENABLED_GROQ: z.boolean(),
      GROQ_API_KEY: z.string().optional(),

      ENABLED_GITHUB: z.boolean(),
      GITHUB_TOKEN: z.string().optional(),

      ENABLED_OPENROUTER: z.boolean(),
      OPENROUTER_API_KEY: z.string().optional(),

      ENABLED_ZEROONE: z.boolean(),
      ZEROONE_API_KEY: z.string().optional(),

      ENABLED_TOGETHERAI: z.boolean(),
      TOGETHERAI_API_KEY: z.string().optional(),

      ENABLED_FIREWORKSAI: z.boolean(),
      FIREWORKSAI_API_KEY: z.string().optional(),

      ENABLED_AWS_BEDROCK: z.boolean(),
      AWS_REGION: z.string().optional(),
      AWS_ACCESS_KEY_ID: z.string().optional(),
      AWS_SECRET_ACCESS_KEY: z.string().optional(),
      AWS_SESSION_TOKEN: z.string().optional(),

      ENABLED_WENXIN: z.boolean(),
      WENXIN_ACCESS_KEY: z.string().optional(),
      WENXIN_SECRET_KEY: z.string().optional(),

      ENABLED_OLLAMA: z.boolean(),

      ENABLED_QWEN: z.boolean(),
      QWEN_API_KEY: z.string().optional(),

      ENABLED_STEPFUN: z.boolean(),
      STEPFUN_API_KEY: z.string().optional(),

      ENABLED_NOVITA: z.boolean(),
      NOVITA_API_KEY: z.string().optional(),

      ENABLED_BAICHUAN: z.boolean(),
      BAICHUAN_API_KEY: z.string().optional(),

      ENABLED_TAICHU: z.boolean(),
      TAICHU_API_KEY: z.string().optional(),

      ENABLED_CLOUDFLARE: z.boolean(),
      CLOUDFLARE_API_KEY: z.string().optional(),
      CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID: z.string().optional(),

      ENABLED_AI360: z.boolean(),
      AI360_API_KEY: z.string().optional(),

      ENABLED_SILICONCLOUD: z.boolean(),
      SILICONCLOUD_API_KEY: z.string().optional(),

      ENABLED_GITEE_AI: z.boolean(),
      GITEE_AI_API_KEY: z.string().optional(),

      ENABLED_UPSTAGE: z.boolean(),
      UPSTAGE_API_KEY: z.string().optional(),

      ENABLED_SPARK: z.boolean(),
      SPARK_API_KEY: z.string().optional(),

      ENABLED_AI21: z.boolean(),
      AI21_API_KEY: z.string().optional(),

      ENABLED_HUNYUAN: z.boolean(),
      HUNYUAN_API_KEY: z.string().optional(),

      ENABLED_HUGGINGFACE: z.boolean(),
      HUGGINGFACE_API_KEY: z.string().optional(),

      ENABLED_SENSENOVA: z.boolean(),
      SENSENOVA_API_KEY: z.string().optional(),

      ENABLED_XAI: z.boolean(),
      XAI_API_KEY: z.string().optional(),

      ENABLED_INTERNLM: z.boolean(),
      INTERNLM_API_KEY: z.string().optional(),

      ENABLED_HIGRESS: z.boolean(),
      HIGRESS_API_KEY: z.string().optional(),
    },
    runtimeEnv: {
      API_KEY_SELECT_MODE: process.env.API_KEY_SELECT_MODE,

      ENABLED_OPENAI: process.env.ENABLED_OPENAI !== '0',
      OPENAI_API_KEY: process.env.OPENAI_API_KEY,

      ENABLED_AZURE_OPENAI: !!process.env.AZURE_API_KEY,
      AZURE_API_KEY: process.env.AZURE_API_KEY,
      AZURE_API_VERSION: process.env.AZURE_API_VERSION,
      AZURE_ENDPOINT: process.env.AZURE_ENDPOINT,

      ENABLED_ZHIPU: !!process.env.ZHIPU_API_KEY,
      ZHIPU_API_KEY: process.env.ZHIPU_API_KEY,

      ENABLED_DEEPSEEK: !!process.env.DEEPSEEK_API_KEY,
      DEEPSEEK_API_KEY: process.env.DEEPSEEK_API_KEY,

      ENABLED_GOOGLE: !!process.env.GOOGLE_API_KEY,
      GOOGLE_API_KEY: process.env.GOOGLE_API_KEY,

      ENABLED_PERPLEXITY: !!process.env.PERPLEXITY_API_KEY,
      PERPLEXITY_API_KEY: process.env.PERPLEXITY_API_KEY,

      ENABLED_ANTHROPIC: !!process.env.ANTHROPIC_API_KEY,
      ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY,

      ENABLED_MINIMAX: !!process.env.MINIMAX_API_KEY,
      MINIMAX_API_KEY: process.env.MINIMAX_API_KEY,

      ENABLED_MISTRAL: !!process.env.MISTRAL_API_KEY,
      MISTRAL_API_KEY: process.env.MISTRAL_API_KEY,

      ENABLED_OPENROUTER: !!process.env.OPENROUTER_API_KEY,
      OPENROUTER_API_KEY: process.env.OPENROUTER_API_KEY,

      ENABLED_TOGETHERAI: !!process.env.TOGETHERAI_API_KEY,
      TOGETHERAI_API_KEY: process.env.TOGETHERAI_API_KEY,

      ENABLED_FIREWORKSAI: !!process.env.FIREWORKSAI_API_KEY,
      FIREWORKSAI_API_KEY: process.env.FIREWORKSAI_API_KEY,

      ENABLED_MOONSHOT: !!process.env.MOONSHOT_API_KEY,
      MOONSHOT_API_KEY: process.env.MOONSHOT_API_KEY,

      ENABLED_GROQ: !!process.env.GROQ_API_KEY,
      GROQ_API_KEY: process.env.GROQ_API_KEY,

      ENABLED_GITHUB: !!process.env.GITHUB_TOKEN,
      GITHUB_TOKEN: process.env.GITHUB_TOKEN,

      ENABLED_ZEROONE: !!process.env.ZEROONE_API_KEY,
      ZEROONE_API_KEY: process.env.ZEROONE_API_KEY,

      ENABLED_AWS_BEDROCK: process.env.ENABLED_AWS_BEDROCK === '1',
      AWS_REGION: process.env.AWS_REGION,
      AWS_ACCESS_KEY_ID: process.env.AWS_ACCESS_KEY_ID,
      AWS_SECRET_ACCESS_KEY: process.env.AWS_SECRET_ACCESS_KEY,
      AWS_SESSION_TOKEN: process.env.AWS_SESSION_TOKEN,

      ENABLED_WENXIN: !!process.env.WENXIN_ACCESS_KEY && !!process.env.WENXIN_SECRET_KEY,
      WENXIN_ACCESS_KEY: process.env.WENXIN_ACCESS_KEY,
      WENXIN_SECRET_KEY: process.env.WENXIN_SECRET_KEY,

      ENABLED_OLLAMA: process.env.ENABLED_OLLAMA !== '0',

      ENABLED_QWEN: !!process.env.QWEN_API_KEY,
      QWEN_API_KEY: process.env.QWEN_API_KEY,

      ENABLED_STEPFUN: !!process.env.STEPFUN_API_KEY,
      STEPFUN_API_KEY: process.env.STEPFUN_API_KEY,

      ENABLED_NOVITA: !!process.env.NOVITA_API_KEY,
      NOVITA_API_KEY: process.env.NOVITA_API_KEY,

      ENABLED_BAICHUAN: !!process.env.BAICHUAN_API_KEY,
      BAICHUAN_API_KEY: process.env.BAICHUAN_API_KEY,

      ENABLED_TAICHU: !!process.env.TAICHU_API_KEY,
      TAICHU_API_KEY: process.env.TAICHU_API_KEY,

      ENABLED_CLOUDFLARE:
        !!process.env.CLOUDFLARE_API_KEY && !!process.env.CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID,
      CLOUDFLARE_API_KEY: process.env.CLOUDFLARE_API_KEY,
      CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID: process.env.CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID,

      ENABLED_AI360: !!process.env.AI360_API_KEY,
      AI360_API_KEY: process.env.AI360_API_KEY,

      ENABLED_SILICONCLOUD: !!process.env.SILICONCLOUD_API_KEY,
      SILICONCLOUD_API_KEY: process.env.SILICONCLOUD_API_KEY,

      ENABLED_GITEE_AI: !!process.env.GITEE_AI_API_KEY,
      GITEE_AI_API_KEY: process.env.GITEE_AI_API_KEY,

      ENABLED_UPSTAGE: !!process.env.UPSTAGE_API_KEY,
      UPSTAGE_API_KEY: process.env.UPSTAGE_API_KEY,

      ENABLED_SPARK: !!process.env.SPARK_API_KEY,
      SPARK_API_KEY: process.env.SPARK_API_KEY,

      ENABLED_AI21: !!process.env.AI21_API_KEY,
      AI21_API_KEY: process.env.AI21_API_KEY,

      ENABLED_HUNYUAN: !!process.env.HUNYUAN_API_KEY,
      HUNYUAN_API_KEY: process.env.HUNYUAN_API_KEY,

      ENABLED_HUGGINGFACE: !!process.env.HUGGINGFACE_API_KEY,
      HUGGINGFACE_API_KEY: process.env.HUGGINGFACE_API_KEY,

      ENABLED_SENSENOVA: !!process.env.SENSENOVA_API_KEY,
      SENSENOVA_API_KEY: process.env.SENSENOVA_API_KEY,

      ENABLED_XAI: !!process.env.XAI_API_KEY,
      XAI_API_KEY: process.env.XAI_API_KEY,

      ENABLED_INTERNLM: !!process.env.INTERNLM_API_KEY,
      INTERNLM_API_KEY: process.env.INTERNLM_API_KEY,

      ENABLED_HIGRESS: !!process.env.HIGRESS_API_KEY,
      HIGRESS_API_KEY: process.env.HIGRESS_API_KEY,
    },
  });
};

export const llmEnv = getLLMConfig();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/bedrock.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref :https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html
// ref :https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/models
// ref :https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/models
const Bedrock: ModelProviderCard = {
  chatModels: [
    /*
    // TODO: Not support for now
    {
      description: '亚马逊 Titan Text Lite 是一款轻量级高效模型，非常适合对英语任务进行微调，包括总结和文案编写等，客户希望有一个更小、更经济的模型，同时也非常可定制。',
      displayName: 'Titan Text G1 - Lite',
      id: 'amazon.titan-text-lite-v1',
      tokens: 4000,
    },
    {
      description: '亚马逊 Titan Text Express 的上下文长度可达 8,000 个标记，非常适合广泛的高级通用语言任务，如开放式文本生成和对话聊天，以及在检索增强生成 (RAG) 中的支持。在推出时，该模型针对英语进行了优化，预览版还支持其他 100 多种语言。',
      displayName: 'Titan Text G1 - Express',
      id: 'amazon.titan-text-express-v1',
      tokens: 8000,
    },
    {
      description: 'Titan Text Premier 是 Titan Text 系列中一款强大的先进模型，旨在为广泛的企业应用提供卓越的性能。凭借其尖端能力，它提供了更高的准确性和卓越的结果，是寻求一流文本处理解决方案的组织的绝佳选择。',
      displayName: 'Titan Text G1 - Premier',
      id: 'amazon.titan-text-premier-v1:0',
      tokens: 32_000,
    },
*/
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提升了行业标准，性能超过竞争对手模型和 Claude 3 Opus，在广泛的评估中表现出色，同时具有我们中等层级模型的速度和成本。',
      displayName: 'Claude 3.5 Sonnet',
      enabled: true,
      functionCall: true,
      id: 'anthropic.claude-3-5-sonnet-20241022-v2:0',
      pricing: {
        input: 3,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提升了行业标准，性能超过竞争对手模型和 Claude 3 Opus，在广泛的评估中表现出色，同时具有我们中等层级模型的速度和成本。',
      displayName: 'Claude 3.5 Sonnet v2 (Inference profile)',
      enabled: true,
      functionCall: true,
      id: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
      pricing: {
        input: 3,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提升了行业标准，性能超过竞争对手模型和 Claude 3 Opus，在广泛的评估中表现出色，同时具有我们中等层级模型的速度和成本。',
      displayName: 'Claude 3.5 Sonnet 0620',
      enabled: true,
      functionCall: true,
      id: 'anthropic.claude-3-5-sonnet-20240620-v1:0',
      pricing: {
        input: 3,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Haiku 是 Anthropic 最快、最紧凑的模型，提供近乎即时的响应速度。它可以快速回答简单的查询和请求。客户将能够构建模仿人类互动的无缝 AI 体验。Claude 3 Haiku 可以处理图像并返回文本输出，具有 200K 的上下文窗口。',
      displayName: 'Claude 3 Haiku',
      enabled: true,
      functionCall: true,
      id: 'anthropic.claude-3-haiku-20240307-v1:0',
      pricing: {
        input: 0.25,
        output: 1.25,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Anthropic 的 Claude 3 Sonnet 在智能和速度之间达到了理想的平衡——特别适合企业工作负载。它以低于竞争对手的价格提供最大的效用，并被设计成为可靠的、高耐用的主力机，适用于规模化的 AI 部署。Claude 3 Sonnet 可以处理图像并返回文本输出，具有 200K 的上下文窗口。',
      displayName: 'Claude 3 Sonnet',
      enabled: true,
      functionCall: true,
      id: 'anthropic.claude-3-sonnet-20240229-v1:0',
      pricing: {
        input: 3,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Opus 是 Anthropic 最强大的 AI 模型，具有在高度复杂任务上的最先进性能。它可以处理开放式提示和未见过的场景，具有出色的流畅性和类人的理解能力。Claude 3 Opus 展示了生成 AI 可能性的前沿。Claude 3 Opus 可以处理图像并返回文本输出，具有 200K 的上下文窗口。',
      displayName: 'Claude 3 Opus',
      enabled: true,
      functionCall: true,
      id: 'anthropic.claude-3-opus-20240229-v1:0',
      pricing: {
        input: 15,
        output: 75,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 2 的更新版，具有双倍的上下文窗口，以及在长文档和 RAG 上下文中的可靠性、幻觉率和基于证据的准确性的改进。',
      displayName: 'Claude 2.1',
      id: 'anthropic.claude-v2:1',
      pricing: {
        input: 8,
        output: 24,
      },
    },
    {
      contextWindowTokens: 100_000,
      description:
        'Anthropic 在从复杂对话和创意内容生成到详细指令跟随的广泛任务中都表现出高度能力的模型。',
      displayName: 'Claude 2.0',
      id: 'anthropic.claude-v2',
      pricing: {
        input: 8,
        output: 24,
      },
    },
    {
      contextWindowTokens: 100_000,
      description:
        '一款快速、经济且仍然非常有能力的模型，可以处理包括日常对话、文本分析、总结和文档问答在内的一系列任务。',
      displayName: 'Claude Instant',
      id: 'anthropic.claude-instant-v1',
      pricing: {
        input: 0.8,
        output: 2.4,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Meta Llama 3.1 8B Instruct 的更新版，包括扩展的 128K 上下文长度、多语言性和改进的推理能力。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。',
      displayName: 'Llama 3.1 8B Instruct',
      enabled: true,
      functionCall: true,
      id: 'meta.llama3-1-8b-instruct-v1:0',
      pricing: {
        input: 0.22,
        output: 0.22,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Meta Llama 3.1 70B Instruct 的更新版，包括扩展的 128K 上下文长度、多语言性和改进的推理能力。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。',
      displayName: 'Llama 3.1 70B Instruct',
      enabled: true,
      functionCall: true,
      id: 'meta.llama3-1-70b-instruct-v1:0',
      pricing: {
        input: 0.99,
        output: 0.99,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Meta Llama 3.1 405B Instruct 是 Llama 3.1 Instruct 模型中最大、最强大的模型，是一款高度先进的对话推理和合成数据生成模型，也可以用作在特定领域进行专业持续预训练或微调的基础。Llama 3.1 提供的多语言大型语言模型 (LLMs) 是一组预训练的、指令调整的生成模型，包括 8B、70B 和 405B 大小 (文本输入/输出)。Llama 3.1 指令调整的文本模型 (8B、70B、405B) 专为多语言对话用例进行了优化，并在常见的行业基准测试中超过了许多可用的开源聊天模型。Llama 3.1 旨在用于多种语言的商业和研究用途。指令调整的文本模型适用于类似助手的聊天，而预训练模型可以适应各种自然语言生成任务。Llama 3.1 模型还支持利用其模型的输出来改进其他模型，包括合成数据生成和精炼。Llama 3.1 是使用优化的变压器架构的自回归语言模型。调整版本使用监督微调 (SFT) 和带有人类反馈的强化学习 (RLHF) 来符合人类对帮助性和安全性的偏好。',
      displayName: 'Llama 3.1 405B Instruct',
      enabled: true,
      functionCall: true,
      id: 'meta.llama3-1-405b-instruct-v1:0',
      pricing: {
        input: 5.32,
        output: 16,
      },
    },
    {
      contextWindowTokens: 8000,
      description:
        'Meta Llama 3 是一款面向开发者、研究人员和企业的开放大型语言模型 (LLM)，旨在帮助他们构建、实验并负责任地扩展他们的生成 AI 想法。作为全球社区创新的基础系统的一部分，它非常适合计算能力和资源有限、边缘设备和更快的训练时间。',
      displayName: 'Llama 3 8B Instruct',
      id: 'meta.llama3-8b-instruct-v1:0',
      pricing: {
        input: 0.3,
        output: 0.6,
      },
    },
    {
      contextWindowTokens: 8000,
      description:
        'Meta Llama 3 是一款面向开发者、研究人员和企业的开放大型语言模型 (LLM)，旨在帮助他们构建、实验并负责任地扩展他们的生成 AI 想法。作为全球社区创新的基础系统的一部分，它非常适合内容创建、对话 AI、语言理解、研发和企业应用。',
      displayName: 'Llama 3 70B Instruct',
      id: 'meta.llama3-70b-instruct-v1:0',
      pricing: {
        input: 2.65,
        output: 3.5,
      },
    },
    /*
    // TODO: Not support for now
    {
      description: 'A 7B dense Transformer, fast-deployed and easily customisable. Small, yet powerful for a variety of use cases. Supports English and code, and a 32k context window.',
      displayName: 'Mistral 7B Instruct',
      enabled: true,
      id: 'mistral.mistral-7b-instruct-v0:2',
      tokens: 32_000,
    },
    {
      description: 'A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Uses 12B active parameters out of 45B total. Supports multiple languages, code and 32k context window.',
      displayName: 'Mixtral 8X7B Instruct',
      enabled: true,
      id: 'mistral.mixtral-8x7b-instruct-v0:1',
      tokens: 32_000,
    },
    {
      description: 'Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation. It provides outstanding performance at a cost-effective price point.',
      displayName: 'Mistral Small',
      functionCall: true,
      id: 'mistral.mistral-small-2402-v1:0',
      tokens: 32_000,
    },
    {
      description: 'Mistral Large 2407 is an advanced Large Language Model (LLM) that supports dozens of languages and is trained on 80+ coding languages. It has best-in-class agentic capabilities with native function calling JSON outputting and reasoning capabilities.',
      displayName: 'Mistral Large 2 (24.07)',
      enabled: true,
      functionCall: true,
      id: 'mistral.mistral-large-2407-v1:0',
      tokens: 128_000,
    },
    {
      description: 'The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.',
      displayName: 'Mistral Large',
      enabled: true,
      functionCall: true,
      id: 'mistral.mistral-large-2402-v1:0',
      tokens: 32_000,
    },
*/
    /*
    // TODO: Not support for now
    {
      description: 'Command R+ is a highly performant generative language model optimized for large scale production workloads.',
      displayName: 'Command R+',
      enabled: true,
      functionCall: true,
      id: 'cohere.command-r-plus-v1:0',
      tokens: 128_000,
    },
    {
      description: 'Command R is a generative language model optimized for long-context tasks and large scale production workloads.',
      displayName: 'Command R',
      enabled: true,
      functionCall: true,
      id: 'cohere.command-r-v1:0',
      tokens: 128_000,
    },
*/
    /*
    // Cohere Command (Text) and AI21 Labs Jurassic-2 (Text) don't support chat with the Converse API
    {
      description: 'Command is Cohere flagship text generation model. It is trained to follow user commands and to be instantly useful in practical business applications.',
      displayName: 'Command',
      id: 'cohere.command-text-v14',
      tokens: 4000,
    },
    {
      description: 'Cohere Command-Light is a generative model that responds well with instruction-like prompts. This model provides customers with an unbeatable balance of quality, cost-effectiveness, and low-latency inference.',
      displayName: 'Command Light',
      id: 'cohere.command-light-text-v14',
      tokens: 4000,
    },
*/
    /*
    // TODO: Not support for now
    {
      description: 'The latest Foundation Model from AI21 Labs, Jamba-Instruct offers an impressive 256K context window and delivers the best value per price on core text generation, summarization, and question answering tasks for the enterprise.',
      displayName: 'Jamba-Instruct',
      id: 'ai21.jamba-instruct-v1:0',
      tokens: 256_000,
    },
*/
    /*
    // Cohere Command (Text) and AI21 Labs Jurassic-2 (Text) don't support chat with the Converse API
    {
      description: 'Jurassic-2 Mid is less powerful than Ultra, yet carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others.',
      displayName: 'Jurassic-2 Mid',
      id: 'ai21.j2-mid-v1',
      tokens: 8191,
    },
    {
      description: 'Jurassic-2 Ultra is AI21’s most powerful model for complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more.',
      displayName: 'Jurassic-2 Ultra',
      id: 'ai21.j2-ultra-v1',
      tokens: 8191,
    },
*/
  ],
  checkModel: 'anthropic.claude-instant-v1',
  description:
    'Bedrock 是亚马逊 AWS 提供的一项服务，专注于为企业提供先进的 AI 语言模型和视觉模型。其模型家族包括 Anthropic 的 Claude 系列、Meta 的 Llama 3.1 系列等，涵盖从轻量级到高性能的多种选择，支持文本生成、对话、图像处理等多种任务，适用于不同规模和需求的企业应用。',
  id: 'bedrock',
  modelsUrl: 'https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html',
  name: 'Bedrock',
  url: 'https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html',
};

export default Bedrock;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/higress.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

const Higress: ModelProviderCard = {
  chatModels: [
    //qwen
    {
      contextWindowTokens: 131_072,
      description: '通义千问超大规模语言模型，支持中文、英文等不同语言输入。',
      displayName: 'Qwen Turbo',
      enabled: true,
      functionCall: true,
      id: 'qwen-turbo',
      pricing: {
        currency: 'CNY',
        input: 0.3,
        output: 0.6,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问超大规模语言模型增强版，支持中文、英文等不同语言输入。',
      displayName: 'Qwen Plus',
      enabled: true,
      functionCall: true,
      id: 'qwen-plus',
      pricing: {
        currency: 'CNY',
        input: 0.8,
        output: 2,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        '通义千问千亿级别超大规模语言模型，支持中文、英文等不同语言输入，当前通义千问2.5产品版本背后的API模型。',
      displayName: 'Qwen Max',
      enabled: true,
      functionCall: true,
      id: 'qwen-max',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 60,
      },
    },
    {
      contextWindowTokens: 1_000_000,
      description:
        '通义千问超大规模语言模型，支持长文本上下文，以及基于长文档、多文档等多个场景的对话功能。',
      displayName: 'Qwen Long',
      id: 'qwen-long',
      pricing: {
        currency: 'CNY',
        input: 0.5,
        output: 2,
      },
    },
    //后面几个qwen未知支持
    {
      contextWindowTokens: 32_000,
      description:
        '通义千问大规模视觉语言模型增强版。大幅提升细节识别能力和文字识别能力，支持超百万像素分辨率和任意长宽比规格的图像。',
      displayName: 'Qwen VL Plus',
      enabled: true,
      id: 'qwen-vl-plus-latest',
      pricing: {
        currency: 'CNY',
        input: 8,
        output: 8,
      },
      vision: true,
    },
    {
      contextWindowTokens: 32_000,
      description:
        '通义千问超大规模视觉语言模型。相比增强版，再次提升视觉推理能力和指令遵循能力，提供更高的视觉感知和认知水平。',
      displayName: 'Qwen VL Max',
      enabled: true,
      id: 'qwen-vl-max-latest',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
      vision: true,
    },
    {
      contextWindowTokens: 4096,
      description: '通义千问数学模型是专门用于数学解题的语言模型。',
      displayName: 'Qwen Math Turbo',
      id: 'qwen-math-turbo-latest',
      pricing: {
        currency: 'CNY',
        input: 2,
        output: 6,
      },
    },
    {
      contextWindowTokens: 4096,
      description: '通义千问数学模型是专门用于数学解题的语言模型。',
      displayName: 'Qwen Math Plus',
      id: 'qwen-math-plus-latest',
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 12,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问代码模型。',
      displayName: 'Qwen Coder Turbo',
      id: 'qwen-coder-turbo-latest',
      pricing: {
        currency: 'CNY',
        input: 2,
        output: 6,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问2.5对外开源的7B规模的模型。',
      displayName: 'Qwen2.5 7B',
      functionCall: true,
      id: 'qwen2.5-7b-instruct',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 2,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问2.5对外开源的14B规模的模型。',
      displayName: 'Qwen2.5 14B',
      functionCall: true,
      id: 'qwen2.5-14b-instruct',
      pricing: {
        currency: 'CNY',
        input: 2,
        output: 6,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问2.5对外开源的32B规模的模型。',
      displayName: 'Qwen2.5 32B',
      functionCall: true,
      id: 'qwen2.5-32b-instruct',
      pricing: {
        currency: 'CNY',
        input: 3.5,
        output: 7,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问2.5对外开源的72B规模的模型。',
      displayName: 'Qwen2.5 72B',
      functionCall: true,
      id: 'qwen2.5-72b-instruct',
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 12,
      },
    },
    {
      contextWindowTokens: 4096,
      description: 'Qwen-Math 模型具有强大的数学解题能力。',
      displayName: 'Qwen2.5 Math 1.5B',
      id: 'qwen2.5-math-1.5b-instruct',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
    },
    {
      contextWindowTokens: 4096,
      description: 'Qwen-Math 模型具有强大的数学解题能力。',
      displayName: 'Qwen2.5 Math 7B',
      id: 'qwen2.5-math-7b-instruct',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 2,
      },
    },
    {
      contextWindowTokens: 4096,
      description: 'Qwen-Math 模型具有强大的数学解题能力。',
      displayName: 'Qwen2.5 Math 72B',
      id: 'qwen2.5-math-72b-instruct',
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 12,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问代码模型开源版。',
      displayName: 'Qwen2.5 Coder 1.5B',
      id: 'qwen2.5-coder-1.5b-instruct',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
    },
    {
      contextWindowTokens: 131_072,
      description: '通义千问代码模型开源版。',
      displayName: 'Qwen2.5 Coder 7B',
      id: 'qwen2.5-coder-7b-instruct',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 2,
      },
    },
    {
      contextWindowTokens: 8000,
      description: '以 Qwen-7B 语言模型初始化，添加图像模型，图像输入分辨率为448的预训练模型。',
      displayName: 'Qwen VL',
      id: 'qwen-vl-v1',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
      vision: true,
    },
    {
      contextWindowTokens: 8000,
      description: '通义千问VL支持灵活的交互方式，包括多图、多轮问答、创作等能力的模型。',
      displayName: 'Qwen VL Chat',
      id: 'qwen-vl-chat-v1',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
      vision: true,
    },

    //moonshot
    {
      contextWindowTokens: 8192,
      description:
        'Moonshot V1 8K 专为生成短文本任务设计，具有高效的处理性能，能够处理8,192个tokens，非常适合简短对话、速记和快速内容生成。',
      displayName: 'Moonshot V1 8K',
      enabled: true,
      functionCall: true,
      id: 'moonshot-v1-8k',
    },
    {
      contextWindowTokens: 32_768,
      description:
        'Moonshot V1 32K 提供中等长度的上下文处理能力，能够处理32,768个tokens，特别适合生成各种长文档和复杂对话，应用于内容创作、报告生成和对话系统等领域。',
      displayName: 'Moonshot V1 32K',
      enabled: true,
      functionCall: true,
      id: 'moonshot-v1-32k',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Moonshot V1 128K 是一款拥有超长上下文处理能力的模型，适用于生成超长文本，满足复杂的生成任务需求，能够处理多达128,000个tokens的内容，非常适合科研、学术和大型文档生成等应用场景。',
      displayName: 'Moonshot V1 128K',
      enabled: true,
      functionCall: true,
      id: 'moonshot-v1-128k',
    },
    //百川智能
    {
      contextWindowTokens: 32_768,
      description:
        '模型能力国内第一，在知识百科、长文本、生成创作等中文任务上超越国外主流模型。还具备行业领先的多模态能力，多项权威评测基准表现优异。',
      displayName: 'Baichuan 4',
      enabled: true,
      functionCall: true,
      id: 'Baichuan4',
      maxOutput: 4096,
      pricing: {
        currency: 'CNY',
        input: 100,
        output: 100,
      },
    },
    {
      description: '',
      displayName: 'Baichuan 4 Turbo',
      enabled: true,
      functionCall: true,
      id: 'Baichuan4-Turbo',
      // maxOutput: 4096,
      // pricing: {
      //   currency: 'CNY',
      //   input: 100,
      //   output: 100,
      // },
      // tokens: 32_768,
    },
    {
      description: '',
      displayName: 'Baichuan 4 Air',
      enabled: true,
      functionCall: true,
      id: 'Baichuan4-Air',
      // maxOutput: 4096,
      // pricing: {
      //   currency: 'CNY',
      //   input: 100,
      //   output: 100,
      // },
      // tokens: 32_768,
    },
    {
      contextWindowTokens: 32_768,
      description:
        '针对企业高频场景优化，效果大幅提升，高性价比。相对于Baichuan2模型，内容创作提升20%，知识问答提升17%， 角色扮演能力提升40%。整体效果比GPT3.5更优。',
      displayName: 'Baichuan 3 Turbo',
      enabled: true,
      functionCall: true,
      id: 'Baichuan3-Turbo',
      maxOutput: 8192,
      pricing: {
        currency: 'CNY',
        input: 12,
        output: 12,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '具备 128K 超长上下文窗口，针对企业高频场景优化，效果大幅提升，高性价比。相对于Baichuan2模型，内容创作提升20%，知识问答提升17%， 角色扮演能力提升40%。整体效果比GPT3.5更优。',
      displayName: 'Baichuan 3 Turbo 128k',
      enabled: true,
      id: 'Baichuan3-Turbo-128k',
      maxOutput: 4096,
      pricing: {
        currency: 'CNY',
        input: 24,
        output: 24,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        '采用搜索增强技术实现大模型与领域知识、全网知识的全面链接。支持PDF、Word等多种文档上传及网址输入，信息获取及时、全面，输出结果准确、专业。',
      displayName: 'Baichuan 2 Turbo',
      id: 'Baichuan2-Turbo',
      maxOutput: 8192,
      pricing: {
        currency: 'CNY',
        input: 8,
        output: 8,
      },
    },
    //零一万物
    {
      contextWindowTokens: 16_384,
      description: '最新高性能模型，保证高质量输出同时，推理速度大幅提升。',
      displayName: 'Yi Lightning',
      enabled: true,
      id: 'yi-lightning',
      pricing: {
        currency: 'CNY',
        input: 0.99,
        output: 0.99,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '小而精悍，轻量极速模型。提供强化数学运算和代码编写能力。',
      displayName: 'Yi Spark',
      enabled: true,
      id: 'yi-spark',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 1,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '中型尺寸模型升级微调，能力均衡，性价比高。深度优化指令遵循能力。',
      displayName: 'Yi Medium',
      enabled: true,
      id: 'yi-medium',
      pricing: {
        currency: 'CNY',
        input: 2.5,
        output: 2.5,
      },
    },
    {
      contextWindowTokens: 200_000,
      description: '200K 超长上下文窗口，提供长文本深度理解和生成能力。',
      displayName: 'Yi Medium 200K',
      enabled: true,
      id: 'yi-medium-200k',
      pricing: {
        currency: 'CNY',
        input: 12,
        output: 12,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '超高性价比、卓越性能。根据性能和推理速度、成本，进行平衡性高精度调优。',
      displayName: 'Yi Large Turbo',
      enabled: true,
      id: 'yi-large-turbo',
      pricing: {
        currency: 'CNY',
        input: 12,
        output: 12,
      },
    },
    {
      contextWindowTokens: 16_384,
      description:
        '基于 yi-large 超强模型的高阶服务，结合检索与生成技术提供精准答案，实时全网检索信息服务。',
      displayName: 'Yi Large RAG',
      enabled: true,
      id: 'yi-large-rag',
      pricing: {
        currency: 'CNY',
        input: 25,
        output: 25,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        '在 yi-large 模型的基础上支持并强化了工具调用的能力，适用于各种需要搭建 agent 或 workflow 的业务场景。',
      displayName: 'Yi Large FC',
      enabled: true,
      functionCall: true,
      id: 'yi-large-fc',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 32_768,
      description: '全新千亿参数模型，提供超强问答及文本生成能力。',
      displayName: 'Yi Large',
      id: 'yi-large',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '复杂视觉任务模型，提供高性能图片理解、分析能力。',
      displayName: 'Yi Vision',
      enabled: true,
      id: 'yi-vision',
      pricing: {
        currency: 'CNY',
        input: 6,
        output: 6,
      },
      vision: true,
    },
    {
      contextWindowTokens: 16_384,
      description: '初期版本，推荐使用 yi-large（新版本）。',
      displayName: 'Yi Large Preview',
      id: 'yi-large-preview',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '轻量化版本，推荐使用 yi-lightning。',
      displayName: 'Yi Lightning Lite',
      id: 'yi-lightning-lite',
      pricing: {
        currency: 'CNY',
        input: 0.99,
        output: 0.99,
      },
    },
    //智谱AI
    {
      contextWindowTokens: 128_000,
      description: 'GLM-4-Flash 是处理简单任务的理想选择，速度最快且免费。',
      displayName: 'GLM-4-Flash',
      enabled: true,
      functionCall: true,
      id: 'glm-4-flash',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: 'GLM-4-FlashX 是Flash的增强版本，超快推理速度。',
      displayName: 'GLM-4-FlashX',
      enabled: true,
      functionCall: true,
      id: 'glm-4-flashx',
      pricing: {
        currency: 'CNY',
        input: 0.1,
        output: 0.1,
      },
    },
    {
      contextWindowTokens: 1_024_000,
      description: 'GLM-4-Long 支持超长文本输入，适合记忆型任务与大规模文档处理。',
      displayName: 'GLM-4-Long',
      functionCall: true,
      id: 'glm-4-long',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 1,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: 'GLM-4-Air 是性价比高的版本，性能接近GLM-4，提供快速度和实惠的价格。',
      displayName: 'GLM-4-Air',
      enabled: true,
      functionCall: true,
      id: 'glm-4-air',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 1,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'GLM-4-AirX 提供 GLM-4-Air 的高效版本，推理速度可达其2.6倍。',
      displayName: 'GLM-4-AirX',
      enabled: true,
      functionCall: true,
      id: 'glm-4-airx',
      pricing: {
        currency: 'CNY',
        input: 10,
        output: 10,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'GLM-4-AllTools 是一个多功能智能体模型，优化以支持复杂指令规划与工具调用，如网络浏览、代码解释和文本生成，适用于多任务执行。',
      displayName: 'GLM-4-AllTools',
      functionCall: true,
      id: 'glm-4-alltools',
      pricing: {
        currency: 'CNY',
        input: 100,
        output: 100,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'GLM-4-Plus 作为高智能旗舰，具备强大的处理长文本和复杂任务的能力，性能全面提升。',
      displayName: 'GLM-4-Plus',
      enabled: true,
      functionCall: true,
      id: 'glm-4-plus',
      pricing: {
        currency: 'CNY',
        input: 50,
        output: 50,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: 'GLM-4-0520 是最新模型版本，专为高度复杂和多样化任务设计，表现卓越。',
      displayName: 'GLM-4-0520',
      functionCall: true,
      id: 'glm-4-0520',
      pricing: {
        currency: 'CNY',
        input: 100,
        output: 100,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: 'GLM-4 是发布于2024年1月的旧旗舰版本，目前已被更强的 GLM-4-0520 取代。',
      displayName: 'GLM-4',
      functionCall: true,
      id: 'glm-4',
      pricing: {
        currency: 'CNY',
        input: 100,
        output: 100,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'GLM-4V-Plus 具备对视频内容及多图片的理解能力，适合多模态任务。',
      displayName: 'GLM-4V-Plus',
      enabled: true,
      id: 'glm-4v-plus',
      pricing: {
        currency: 'CNY',
        input: 10,
        output: 10,
      },
      vision: true,
    },
    {
      contextWindowTokens: 2048,
      description: 'GLM-4V 提供强大的图像理解与推理能力，支持多种视觉任务。',
      displayName: 'GLM-4V',
      id: 'glm-4v',
      pricing: {
        currency: 'CNY',
        input: 50,
        output: 50,
      },
      vision: true,
    },
    {
      contextWindowTokens: 4096,
      description: 'CharGLM-3 专为角色扮演与情感陪伴设计，支持超长多轮记忆与个性化对话，应用广泛。',
      displayName: 'CharGLM-3',
      id: 'charglm-3',
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 15,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Emohaa 是心理模型，具备专业咨询能力，帮助用户理解情感问题。',
      displayName: 'Emohaa',
      id: 'emohaa',
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 15,
      },
    },
    //360智脑
    {
      contextWindowTokens: 8192,
      description:
        '360GPT2 Pro 是 360 公司推出的高级自然语言处理模型，具备卓越的文本生成和理解能力，尤其在生成与创作领域表现出色，能够处理复杂的语言转换和角色演绎任务。',
      displayName: '360GPT2 Pro',
      enabled: true,
      id: '360gpt2-pro',
      maxOutput: 7000,
      pricing: {
        currency: 'CNY',
        input: 5,
        output: 5,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '360GPT Pro 作为 360 AI 模型系列的重要成员，以高效的文本处理能力满足多样化的自然语言应用场景，支持长文本理解和多轮对话等功能。',
      displayName: '360GPT Pro',
      enabled: true,
      functionCall: true,
      id: '360gpt-pro',
      maxOutput: 7000,
      pricing: {
        currency: 'CNY',
        input: 5,
        output: 5,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '360GPT Turbo 提供强大的计算和对话能力，具备出色的语义理解和生成效率，是企业和开发者理想的智能助理解决方案。',
      displayName: '360GPT Turbo',
      enabled: true,
      id: '360gpt-turbo',
      maxOutput: 7000,
      pricing: {
        currency: 'CNY',
        input: 2,
        output: 2,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '360GPT Turbo Responsibility 8K 强调语义安全和责任导向，专为对内容安全有高度要求的应用场景设计，确保用户体验的准确性与稳健性。',
      displayName: '360GPT Turbo Responsibility 8K',
      enabled: true,
      id: '360gpt-turbo-responsibility-8k',
      maxOutput: 2048,
      pricing: {
        currency: 'CNY',
        input: 2,
        output: 2,
      },
    },
    //文心一言
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
      displayName: 'ERNIE 3.5 8K',
      enabled: true,
      id: 'ERNIE-3.5-8K',
      pricing: {
        currency: 'CNY',
        input: 0.8,
        output: 2,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
      displayName: 'ERNIE 3.5 8K Preview',
      id: 'ERNIE-3.5-8K-Preview',
      pricing: {
        currency: 'CNY',
        input: 0.8,
        output: 2,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
      displayName: 'ERNIE 3.5 128K',
      enabled: true,
      id: 'ERNIE-3.5-128K',
      pricing: {
        currency: 'CNY',
        input: 0.8,
        output: 2,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
      displayName: 'ERNIE 4.0 8K',
      enabled: true,
      id: 'ERNIE-4.0-8K-Latest',
      pricing: {
        currency: 'CNY',
        input: 30,
        output: 90,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
      displayName: 'ERNIE 4.0 8K Preview',
      id: 'ERNIE-4.0-8K-Preview',
      pricing: {
        currency: 'CNY',
        input: 30,
        output: 90,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
      displayName: 'ERNIE 4.0 Turbo 8K',
      enabled: true,
      id: 'ERNIE-4.0-Turbo-8K-Latest',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 60,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
      displayName: 'ERNIE 4.0 Turbo 8K Preview',
      id: 'ERNIE-4.0-Turbo-8K-Preview',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 60,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '百度自研的轻量级大语言模型，兼顾优异的模型效果与推理性能，效果比ERNIE Lite更优，适合低算力AI加速卡推理使用。',
      displayName: 'ERNIE Lite Pro 128K',
      enabled: true,
      id: 'ERNIE-Lite-Pro-128K',
      pricing: {
        currency: 'CNY',
        input: 0.2,
        output: 0.4,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '百度2024年最新发布的自研高性能大语言模型，通用能力优异，效果比ERNIE Speed更优，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
      displayName: 'ERNIE Speed Pro 128K',
      enabled: true,
      id: 'ERNIE-Speed-Pro-128K',
      pricing: {
        currency: 'CNY',
        input: 0.3,
        output: 0.6,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '百度2024年最新发布的自研高性能大语言模型，通用能力优异，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
      displayName: 'ERNIE Speed 128K',
      id: 'ERNIE-Speed-128K',
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        '百度自研的垂直场景大语言模型，适合游戏NPC、客服对话、对话角色扮演等应用场景，人设风格更为鲜明、一致，指令遵循能力更强，推理性能更优。',
      displayName: 'ERNIE Character 8K',
      id: 'ERNIE-Character-8K',
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 8,
      },
    },
    //混元
    {
      contextWindowTokens: 256_000,
      description:
        '升级为 MOE 结构，上下文窗口为 256k ，在 NLP，代码，数学，行业等多项评测集上领先众多开源模型。',
      displayName: 'Hunyuan Lite',
      enabled: true,
      id: 'hunyuan-lite',
      maxOutput: 6000,
      pricing: {
        currency: 'CNY',
        input: 0,
        output: 0,
      },
    },
    {
      contextWindowTokens: 32_000,
      description:
        '采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-32K 性价比相对更高，在平衡效果、价格的同时，可对实现对长文本输入的处理。',
      displayName: 'Hunyuan Standard',
      enabled: true,
      id: 'hunyuan-standard',
      maxOutput: 2000,
      pricing: {
        currency: 'CNY',
        input: 4.5,
        output: 5,
      },
    },
    {
      contextWindowTokens: 256_000,
      description:
        '采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-256K 在长度和效果上进一步突破，极大的扩展了可输入长度。',
      displayName: 'Hunyuan Standard 256K',
      enabled: true,
      id: 'hunyuan-standard-256K',
      maxOutput: 6000,
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 60,
      },
    },
    {
      contextWindowTokens: 32_000,
      description:
        '混元全新一代大语言模型的预览版，采用全新的混合专家模型（MoE）结构，相比hunyuan-pro推理效率更快，效果表现更强。',
      displayName: 'Hunyuan Turbo',
      enabled: true,
      functionCall: true,
      id: 'hunyuan-turbo',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 50,
      },
    },
    {
      contextWindowTokens: 32_000,
      description:
        '万亿级参数规模 MOE-32K 长文模型。在各种 benchmark 上达到绝对领先的水平，复杂指令和推理，具备复杂数学能力，支持 functioncall，在多语言翻译、金融法律医疗等领域应用重点优化。',
      displayName: 'Hunyuan Pro',
      enabled: true,
      functionCall: true,
      id: 'hunyuan-pro',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 30,
        output: 100,
      },
    },
    {
      description: '',
      displayName: 'Hunyuan Large',
      enabled: true,
      functionCall: true,
      id: 'hunyuan-large',
      // maxOutput: 4000,
      // pricing: {
      //   currency: 'CNY',
      //   input: 30,
      //   output: 100,
      // },
      // tokens: 32_000,
    },
    {
      contextWindowTokens: 8000,
      description: '混元最新多模态模型，支持图片+文本输入生成文本内容。',
      displayName: 'Hunyuan Vision',
      enabled: true,
      id: 'hunyuan-vision',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 18,
        output: 18,
      },
      vision: true,
    },
    {
      contextWindowTokens: 8000,
      description:
        '混元最新代码生成模型，经过 200B 高质量代码数据增训基座模型，迭代半年高质量 SFT 数据训练，上下文长窗口长度增大到 8K，五大语言代码生成自动评测指标上位居前列；五大语言10项考量各方面综合代码任务人工高质量评测上，性能处于第一梯队',
      displayName: 'Hunyuan Code',
      id: 'hunyuan-code',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 8,
      },
    },
    {
      contextWindowTokens: 32_000,
      description:
        '混元最新 MOE 架构 FunctionCall 模型，经过高质量的 FunctionCall 数据训练，上下文窗口达 32K，在多个维度的评测指标上处于领先。',
      displayName: 'Hunyuan FunctionCall',
      functionCall: true,
      id: 'hunyuan-functioncall',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 8,
      },
    },
    {
      contextWindowTokens: 8000,
      description:
        '混元最新版角色扮演模型，混元官方精调训练推出的角色扮演模型，基于混元模型结合角色扮演场景数据集进行增训，在角色扮演场景具有更好的基础效果。',
      displayName: 'Hunyuan Role',
      id: 'hunyuan-role',
      maxOutput: 4000,
      pricing: {
        currency: 'CNY',
        input: 4,
        output: 8,
      },
    },
    //阶跃星辰
    {
      contextWindowTokens: 8000,
      description: '高速模型，适合实时对话。',
      displayName: 'Step 1 Flash',
      enabled: true,
      functionCall: true,
      id: 'step-1-flash',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 4,
      },
    },
    {
      contextWindowTokens: 8000,
      description: '小型模型，适合轻量级任务。',
      displayName: 'Step 1 8K',
      enabled: true,
      functionCall: true,
      id: 'step-1-8k',
      pricing: {
        currency: 'CNY',
        input: 5,
        output: 20,
      },
    },
    {
      contextWindowTokens: 32_000,
      description: '支持中等长度的对话，适用于多种应用场景。',
      displayName: 'Step 1 32K',
      enabled: true,
      functionCall: true,
      id: 'step-1-32k',
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 70,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: '平衡性能与成本，适合一般场景。',
      displayName: 'Step 1 128K',
      enabled: true,
      functionCall: true,
      id: 'step-1-128k',
      pricing: {
        currency: 'CNY',
        input: 40,
        output: 200,
      },
    },
    {
      contextWindowTokens: 256_000,
      description: '具备超长上下文处理能力，尤其适合长文档分析。',
      displayName: 'Step 1 256K',
      functionCall: true,
      id: 'step-1-256k',
      pricing: {
        currency: 'CNY',
        input: 95,
        output: 300,
      },
    },
    {
      contextWindowTokens: 16_000,
      description: '支持大规模上下文交互，适合复杂对话场景。',
      displayName: 'Step 2 16K',
      enabled: true,
      functionCall: true,
      id: 'step-2-16k',
      pricing: {
        currency: 'CNY',
        input: 38,
        output: 120,
      },
    },
    {
      contextWindowTokens: 8000,
      description: '小型视觉模型，适合基本的图文任务。',
      displayName: 'Step 1V 8K',
      enabled: true,
      functionCall: true,
      id: 'step-1v-8k',
      pricing: {
        currency: 'CNY',
        input: 5,
        output: 20,
      },
      vision: true,
    },
    {
      contextWindowTokens: 32_000,
      description: '支持视觉输入，增强多模态交互体验。',
      displayName: 'Step 1V 32K',
      enabled: true,
      functionCall: true,
      id: 'step-1v-32k',
      pricing: {
        currency: 'CNY',
        input: 15,
        output: 70,
      },
      vision: true,
    },
    {
      contextWindowTokens: 32_000,
      description: '该模型拥有强大的视频理解能力。',
      displayName: 'Step 1.5V Mini',
      enabled: true,
      id: 'step-1.5v-mini',
      pricing: {
        currency: 'CNY',
        input: 8,
        output: 35,
      },
      vision: true,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Spark Lite 是一款轻量级大语言模型，具备极低的延迟与高效的处理能力，完全免费开放，支持实时在线搜索功能。其快速响应的特性使其在低算力设备上的推理应用和模型微调中表现出色，为用户带来出色的成本效益和智能体验，尤其在知识问答、内容生成及搜索场景下表现不俗。',
      displayName: 'Spark Lite',
      enabled: true,
      functionCall: false,
      id: 'lite',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Spark Pro 是一款为专业领域优化的高性能大语言模型，专注数学、编程、医疗、教育等多个领域，并支持联网搜索及内置天气、日期等插件。其优化后模型在复杂知识问答、语言理解及高层次文本创作中展现出色表现和高效性能，是适合专业应用场景的理想选择。',
      displayName: 'Spark Pro',
      enabled: true,
      functionCall: false,
      id: 'generalv3',
      maxOutput: 8192,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Spark Pro 128K 配置了特大上下文处理能力，能够处理多达128K的上下文信息，特别适合需通篇分析和长期逻辑关联处理的长文内容，可在复杂文本沟通中提供流畅一致的逻辑与多样的引用支持。',
      displayName: 'Spark Pro 128K',
      enabled: true,
      functionCall: false,
      id: 'pro-128k',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Spark Max 为功能最为全面的版本，支持联网搜索及众多内置插件。其全面优化的核心能力以及系统角色设定和函数调用功能，使其在各种复杂应用场景中的表现极为优异和出色。',
      displayName: 'Spark Max',
      enabled: true,
      functionCall: false,
      id: 'generalv3.5',
      maxOutput: 8192,
    },
    {
      contextWindowTokens: 32_768,
      description:
        'Spark Max 32K 配置了大上下文处理能力，更强的上下文理解和逻辑推理能力，支持32K tokens的文本输入，适用于长文档阅读、私有知识问答等场景',
      displayName: 'Spark Max 32K',
      enabled: true,
      functionCall: false,
      id: 'max-32k',
      maxOutput: 8192,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Spark Ultra 是星火大模型系列中最为强大的版本，在升级联网搜索链路同时，提升对文本内容的理解和总结能力。它是用于提升办公生产力和准确响应需求的全方位解决方案，是引领行业的智能产品。',
      displayName: 'Spark 4.0 Ultra',
      enabled: true,
      functionCall: false,
      id: '4.0Ultra',
      maxOutput: 8192,
    },
    //openai
    {
      contextWindowTokens: 128_000,
      description:
        'o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-mini',
      enabled: true,
      id: 'o1-mini',
      maxOutput: 65_536,
      pricing: {
        input: 3,
        output: 12,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-preview',
      enabled: true,
      id: 'o1-preview',
      maxOutput: 32_768,
      pricing: {
        input: 15,
        output: 60,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'GPT-4o mini是OpenAI在GPT-4 Omni之后推出的最新模型，支持图文输入并输出文本。作为他们最先进的小型模型，它比其他近期的前沿模型便宜很多，并且比GPT-3.5 Turbo便宜超过60%。它保持了最先进的智能，同时具有显著的性价比。GPT-4o mini在MMLU测试中获得了 82% 的得分，目前在聊天偏好上排名高于 GPT-4。',
      displayName: 'GPT-4o mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      maxOutput: 16_385,
      pricing: {
        input: 0.15,
        output: 0.6,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      pricing: {
        input: 2.5,
        output: 10,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o 0806',
      functionCall: true,
      id: 'gpt-4o-2024-08-06',
      pricing: {
        input: 2.5,
        output: 10,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o 0513',
      functionCall: true,
      id: 'gpt-4o-2024-05-13',
      pricing: {
        input: 5,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'ChatGPT-4o',
      enabled: true,
      id: 'chatgpt-4o-latest',
      pricing: {
        input: 5,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo',
      functionCall: true,
      id: 'gpt-4-turbo',
      pricing: {
        input: 10,
        output: 30,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Vision 0409',
      functionCall: true,
      id: 'gpt-4-turbo-2024-04-09',
      pricing: {
        input: 10,
        output: 30,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview',
      functionCall: true,
      id: 'gpt-4-turbo-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview 0125',
      functionCall: true,
      id: 'gpt-4-0125-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview 1106',
      functionCall: true,
      id: 'gpt-4-1106-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4',
      functionCall: true,
      id: 'gpt-4',
      pricing: {
        input: 30,
        output: 60,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4 0613',
      functionCall: true,
      id: 'gpt-4-0613',
      pricing: {
        input: 30,
        output: 60,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      // Will be discontinued on June 6, 2025
      displayName: 'GPT-4 32K',
      functionCall: true,
      id: 'gpt-4-32k',
      pricing: {
        input: 60,
        output: 120,
      },
    },
    {
      contextWindowTokens: 32_768,
      // Will be discontinued on June 6, 2025
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4 32K 0613',
      functionCall: true,
      id: 'gpt-4-32k-0613',
      pricing: {
        input: 60,
        output: 120,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo',
      functionCall: true,
      id: 'gpt-3.5-turbo',
      pricing: {
        input: 0.5,
        output: 1.5,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo 0125',
      functionCall: true,
      id: 'gpt-3.5-turbo-0125',
      pricing: {
        input: 0.5,
        output: 1.5,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo 1106',
      functionCall: true,
      id: 'gpt-3.5-turbo-1106',
      pricing: {
        input: 1,
        output: 2,
      },
    },
    {
      contextWindowTokens: 4096,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo Instruct',
      id: 'gpt-3.5-turbo-instruct',
      pricing: {
        input: 1.5,
        output: 2,
      },
    },
    //azure
    {
      contextWindowTokens: 16_385,
      deploymentName: 'gpt-35-turbo',
      description:
        'GPT 3.5 Turbo，OpenAI提供的高效模型，适用于聊天和文本生成任务，支持并行函数调用。',
      displayName: 'GPT 3.5 Turbo',
      enabled: true,
      functionCall: true,
      id: 'gpt-35-turbo',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 16_384,
      deploymentName: 'gpt-35-turbo-16k',
      description: 'GPT 3.5 Turbo 16k，高容量文本生成模型，适合复杂任务。',
      displayName: 'GPT 3.5 Turbo',
      functionCall: true,
      id: 'gpt-35-turbo-16k',
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4-turbo',
      description: 'GPT 4 Turbo，多模态模型，提供杰出的语言理解和生成能力，同时支持图像输入。',
      displayName: 'GPT 4 Turbo',
      enabled: true,
      functionCall: true,
      id: 'gpt-4',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4-vision',
      description: 'GPT-4 视觉预览版，专为图像分析和处理任务设计。',
      displayName: 'GPT 4 Turbo with Vision Preview',
      id: 'gpt-4-vision-preview',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4o-mini',
      description: 'GPT-4o Mini，小型高效模型，具备与GPT-4o相似的卓越性能。',
      displayName: 'GPT 4o Mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4o',
      description: 'GPT-4o 是最新的多模态模型，结合高级文本和图像处理能力。',
      displayName: 'GPT 4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      vision: true,
    },
    //github
    {
      contextWindowTokens: 128_000,
      description: '比 o1-preview 更小、更快，成本低80%，在代码生成和小上下文操作方面表现良好。',
      displayName: 'OpenAI o1-mini',
      enabled: true,
      functionCall: false,
      id: 'o1-mini',
      maxOutput: 65_536,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深度上下文理解和自主工作流程的应用。',
      displayName: 'OpenAI o1-preview',
      enabled: true,
      functionCall: false,
      id: 'o1-preview',
      maxOutput: 32_768,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: '一种经济高效的AI解决方案，适用于多种文本和图像任务。',
      displayName: 'OpenAI GPT-4o mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: 'OpenAI GPT-4系列中最先进的多模态模型，可以处理文本和图像输入。',
      displayName: 'OpenAI GPT-4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 262_144,
      description:
        '一个52B参数（12B活跃）的多语言模型，提供256K长上下文窗口、函数调用、结构化输出和基于事实的生成。',
      displayName: 'AI21 Jamba 1.5 Mini',
      functionCall: true,
      id: 'ai21-jamba-1.5-mini',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 262_144,
      description:
        '一个398B参数（94B活跃）的多语言模型，提供256K长上下文窗口、函数调用、结构化输出和基于事实的生成。',
      displayName: 'AI21 Jamba 1.5 Large',
      functionCall: true,
      id: 'ai21-jamba-1.5-large',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Command R是一个可扩展的生成模型，旨在针对RAG和工具使用，使企业能够实现生产级AI。',
      displayName: 'Cohere Command R',
      id: 'cohere-command-r',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Command R+是一个最先进的RAG优化模型，旨在应对企业级工作负载。',
      displayName: 'Cohere Command R+',
      id: 'cohere-command-r-plus',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Mistral Nemo是一种尖端的语言模型（LLM），在其尺寸类别中拥有最先进的推理、世界知识和编码能力。',
      displayName: 'Mistral Nemo',
      id: 'mistral-nemo',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Mistral Small可用于任何需要高效率和低延迟的基于语言的任务。',
      displayName: 'Mistral Small',
      id: 'mistral-small',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Mistral的旗舰模型，适合需要大规模推理能力或高度专业化的复杂任务（合成文本生成、代码生成、RAG或代理）。',
      displayName: 'Mistral Large',
      id: 'mistral-large',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '在高分辨率图像上表现出色的图像推理能力，适用于视觉理解应用。',
      displayName: 'Llama 3.2 11B Vision',
      id: 'llama-3.2-11b-vision-instruct',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description: '适用于视觉理解代理应用的高级图像推理能力。',
      displayName: 'Llama 3.2 90B Vision',
      id: 'llama-3.2-90b-vision-instruct',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 8B',
      id: 'meta-llama-3.1-8b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 70B',
      id: 'meta-llama-3.1-70b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 405B',
      id: 'meta-llama-3.1-405b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个多功能的80亿参数模型，针对对话和文本生成任务进行了优化。',
      displayName: 'Meta Llama 3 8B',
      id: 'meta-llama-3-8b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个强大的700亿参数模型，在推理、编码和广泛的语言应用方面表现出色。',
      displayName: 'Meta Llama 3 70B',
      id: 'meta-llama-3-70b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Phi-3-mini模型的更新版。',
      displayName: 'Phi-3.5-mini 128K',
      id: 'Phi-3.5-mini-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Phi-3-vision模型的更新版。',
      displayName: 'Phi-3.5-vision 128K',
      id: 'Phi-3.5-vision-instrust',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 4096,
      description: 'Phi-3家族中最小的成员，针对质量和低延迟进行了优化。',
      displayName: 'Phi-3-mini 4K',
      id: 'Phi-3-mini-4k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-mini模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-mini 128K',
      id: 'Phi-3-mini-128k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个70亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。',
      displayName: 'Phi-3-small 8K',
      id: 'Phi-3-small-8k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-small模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-small 128K',
      id: 'Phi-3-small-128k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 4096,
      description: '一个140亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。',
      displayName: 'Phi-3-medium 4K',
      id: 'Phi-3-medium-4k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-medium模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-medium 128K',
      id: 'Phi-3-medium-128k-instruct',
      maxOutput: 4096,
    },

    //groq
    {
      contextWindowTokens: 8192,
      description:
        'Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 11B Vision (Preview)',
      enabled: true,
      id: 'llama-3.2-11b-vision-preview',
      maxOutput: 8192,
      pricing: {
        input: 0.05,
        output: 0.08,
      },
      vision: true,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 90B Vision (Preview)',
      enabled: true,
      id: 'llama-3.2-90b-vision-preview',
      maxOutput: 8192,
      pricing: {
        input: 0.59,
        output: 0.79,
      },
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1 8B 是一款高效能模型，提供了快速的文本生成能力，非常适合需要大规模效率和成本效益的应用场景。',
      displayName: 'Llama 3.1 8B',
      enabled: true,
      functionCall: true,
      id: 'llama-3.1-8b-instant',
      maxOutput: 8192,
      pricing: {
        input: 0.05,
        output: 0.08,
      },
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1 70B 提供更强大的AI推理能力，适合复杂应用，支持超多的计算处理并保证高效和准确率。',
      displayName: 'Llama 3.1 70B',
      enabled: true,
      functionCall: true,
      id: 'llama-3.1-70b-versatile',
      maxOutput: 8192,
      pricing: {
        input: 0.59,
        output: 0.79,
      },
    },
    /*
        // Offline due to overwhelming demand! Stay tuned for updates.
        {
          displayName: 'Llama 3.1 405B',
          functionCall: true,
          id: 'llama-3.1-405b-reasoning',
          tokens: 8_192,
        },
    */
    {
      contextWindowTokens: 8192,
      description: 'Llama 3 Groq 8B Tool Use 是针对高效工具使用优化的模型，支持快速并行计算。',
      displayName: 'Llama 3 Groq 8B Tool Use (Preview)',
      functionCall: true,
      id: 'llama3-groq-8b-8192-tool-use-preview',
      pricing: {
        input: 0.19,
        output: 0.19,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Llama 3 Groq 70B Tool Use 提供强大的工具调用能力，支持复杂任务的高效处理。',
      displayName: 'Llama 3 Groq 70B Tool Use (Preview)',
      functionCall: true,
      id: 'llama3-groq-70b-8192-tool-use-preview',
      pricing: {
        input: 0.89,
        output: 0.89,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Meta Llama 3 8B 带来优质的推理效能，适合多场景应用需求。',
      displayName: 'Meta Llama 3 8B',
      functionCall: true,
      id: 'llama3-8b-8192',
      pricing: {
        input: 0.05,
        output: 0.08,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Meta Llama 3 70B 提供无与伦比的复杂性处理能力，为高要求项目量身定制。',
      displayName: 'Meta Llama 3 70B',
      functionCall: true,
      id: 'llama3-70b-8192',
      pricing: {
        input: 0.59,
        output: 0.79,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Gemma 2 9B 是一款优化用于特定任务和工具整合的模型。',
      displayName: 'Gemma 2 9B',
      enabled: true,
      functionCall: true,
      id: 'gemma2-9b-it',
      pricing: {
        input: 0.2,
        output: 0.2,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Gemma 7B 适合中小规模任务处理，兼具成本效益。',
      displayName: 'Gemma 7B',
      functionCall: true,
      id: 'gemma-7b-it',
      pricing: {
        input: 0.07,
        output: 0.07,
      },
    },
    {
      contextWindowTokens: 32_768,
      description: 'Mixtral 8x7B 提供高容错的并行计算能力，适合复杂任务。',
      displayName: 'Mixtral 8x7B',
      functionCall: true,
      id: 'mixtral-8x7b-32768',
      pricing: {
        input: 0.24,
        output: 0.24,
      },
    },
    {
      contextWindowTokens: 4096,
      description: 'LLaVA 1.5 7B 提供视觉处理能力融合，通过视觉信息输入生成复杂输出。',
      displayName: 'LLaVA 1.5 7B',
      id: 'llava-v1.5-7b-4096-preview',
      vision: true,
    },
    //deepseek
    {
      contextWindowTokens: 128_000,
      description:
        '融合通用与代码能力的全新开源模型, 不仅保留了原有 Chat 模型的通用对话能力和 Coder 模型的强大代码处理能力，还更好地对齐了人类偏好。此外，DeepSeek-V2.5 在写作任务、指令跟随等多个方面也实现了大幅提升。',
      displayName: 'DeepSeek V2.5',
      enabled: true,
      functionCall: true,
      id: 'deepseek-chat',
      pricing: {
        cachedInput: 0.014,
        input: 0.14,
        output: 0.28,
      },
      releasedAt: '2024-09-05',
    },
    //claude
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Haiku 是 Anthropic 最快的下一代模型。与 Claude 3 Haiku 相比，Claude 3.5 Haiku 在各项技能上都有所提升，并在许多智力基准测试中超越了上一代最大的模型 Claude 3 Opus。',
      displayName: 'Claude 3.5 Haiku',
      enabled: true,
      functionCall: true,
      id: 'claude-3-5-haiku-20241022',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.1,
        input: 1,
        output: 5,
        writeCacheInput: 1.25,
      },
      releasedAt: '2024-11-05',
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提供了超越 Opus 的能力和比 Sonnet 更快的速度，同时保持与 Sonnet 相同的价格。Sonnet 特别擅长编程、数据科学、视觉处理、代理任务。',
      displayName: 'Claude 3.5 Sonnet',
      enabled: true,
      functionCall: true,
      id: 'claude-3-5-sonnet-20241022',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.3,
        input: 3,
        output: 15,
        writeCacheInput: 3.75,
      },
      releasedAt: '2024-10-22',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提供了超越 Opus 的能力和比 Sonnet 更快的速度，同时保持与 Sonnet 相同的价格。Sonnet 特别擅长编程、数据科学、视觉处理、代理任务。',
      displayName: 'Claude 3.5 Sonnet 0620',
      functionCall: true,
      id: 'claude-3-5-sonnet-20240620',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.3,
        input: 3,
        output: 15,
        writeCacheInput: 3.75,
      },
      releasedAt: '2024-06-20',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Haiku 是 Anthropic 的最快且最紧凑的模型，旨在实现近乎即时的响应。它具有快速且准确的定向性能。',
      displayName: 'Claude 3 Haiku',
      functionCall: true,
      id: 'claude-3-haiku-20240307',
      maxOutput: 4096,
      pricing: {
        input: 0.25,
        output: 1.25,
      },
      releasedAt: '2024-03-07',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Sonnet 在智能和速度方面为企业工作负载提供了理想的平衡。它以更低的价格提供最大效用，可靠且适合大规模部署。',
      displayName: 'Claude 3 Sonnet',
      functionCall: true,
      id: 'claude-3-sonnet-20240229',
      maxOutput: 4096,
      pricing: {
        input: 3,
        output: 15,
      },
      releasedAt: '2024-02-29',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Opus 是 Anthropic 用于处理高度复杂任务的最强大模型。它在性能、智能、流畅性和理解力方面表现卓越。',
      displayName: 'Claude 3 Opus',
      enabled: true,
      functionCall: true,
      id: 'claude-3-opus-20240229',
      maxOutput: 4096,
      pricing: {
        input: 15,
        output: 75,
      },
      releasedAt: '2024-02-29',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 2 为企业提供了关键能力的进步，包括业界领先的 200K token 上下文、大幅降低模型幻觉的发生率、系统提示以及一个新的测试功能：工具调用。',
      displayName: 'Claude 2.1',
      id: 'claude-2.1',
      maxOutput: 4096,
      pricing: {
        input: 8,
        output: 24,
      },
      releasedAt: '2023-11-21',
    },
    {
      contextWindowTokens: 100_000,
      description:
        'Claude 2 为企业提供了关键能力的进步，包括业界领先的 200K token 上下文、大幅降低模型幻觉的发生率、系统提示以及一个新的测试功能：工具调用。',
      displayName: 'Claude 2.0',
      id: 'claude-2.0',
      maxOutput: 4096,
      pricing: {
        input: 8,
        output: 24,
      },
      releasedAt: '2023-07-11',
    },
    //gemini
    {
      contextWindowTokens: 1_000_000 + 8192,
      description:
        'Gemini 1.5 Flash 是Google最新的多模态AI模型，具备快速处理能力，支持文本、图像和视频输入，适用于多种任务的高效扩展。',
      displayName: 'Gemini 1.5 Flash',
      enabled: true,
      functionCall: true,
      id: 'gemini-1.5-flash-latest',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.018_75,
        input: 0.075,
        output: 0.3,
      },
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description: 'Gemini 1.5 Flash 002 是一款高效的多模态模型，支持广泛应用的扩展。',
      displayName: 'Gemini 1.5 Flash 002',
      enabled: true,
      functionCall: true,
      id: 'gemini-1.5-flash-002',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.018_75,
        input: 0.075,
        output: 0.3,
      },
      releasedAt: '2024-09-25',
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description: 'Gemini 1.5 Flash 001 是一款高效的多模态模型，支持广泛应用的扩展。',
      displayName: 'Gemini 1.5 Flash 001',
      functionCall: true,
      id: 'gemini-1.5-flash-001',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.018_75,
        input: 0.075,
        output: 0.3,
      },
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description: 'Gemini 1.5 Flash 0827 提供了优化后的多模态处理能力，适用多种复杂任务场景。',
      displayName: 'Gemini 1.5 Flash 0827',
      functionCall: true,
      id: 'gemini-1.5-flash-exp-0827',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.018_75,
        input: 0.075,
        output: 0.3,
      },
      releasedAt: '2024-08-27',
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description: 'Gemini 1.5 Flash 8B 是一款高效的多模态模型，支持广泛应用的扩展。',
      displayName: 'Gemini 1.5 Flash 8B',
      enabled: true,
      functionCall: true,
      id: 'gemini-1.5-flash-8b',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.02,
        input: 0.075,
        output: 0.3,
      },
      releasedAt: '2024-10-03',
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description:
        'Gemini 1.5 Flash 8B 0924 是最新的实验性模型，在文本和多模态用例中都有显著的性能提升。',
      displayName: 'Gemini 1.5 Flash 8B 0924',
      functionCall: true,
      id: 'gemini-1.5-flash-8b-exp-0924',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.018_75,
        input: 0.075,
        output: 0.3,
      },
      releasedAt: '2024-09-24',
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description:
        'Gemini 1.5 Pro 支持高达200万个tokens，是中型多模态模型的理想选择，适用于复杂任务的多方面支持。',
      displayName: 'Gemini 1.5 Pro',
      enabled: true,
      functionCall: true,
      id: 'gemini-1.5-pro-latest',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.875,
        input: 3.5,
        output: 10.5,
      },
      releasedAt: '2024-02-15',
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description:
        'Gemini 1.5 Pro 002 是最新的生产就绪模型，提供更高质量的输出，特别在数学、长上下文和视觉任务方面有显著提升。',
      displayName: 'Gemini 1.5 Pro 002',
      enabled: true,
      functionCall: true,
      id: 'gemini-1.5-pro-002',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.315,
        input: 1.25,
        output: 2.5,
      },
      releasedAt: '2024-09-24',
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description: 'Gemini 1.5 Pro 001 是可扩展的多模态AI解决方案，支持广泛的复杂任务。',
      displayName: 'Gemini 1.5 Pro 001',
      functionCall: true,
      id: 'gemini-1.5-pro-001',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.875,
        input: 3.5,
        output: 10.5,
      },
      releasedAt: '2024-02-15',
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description: 'Gemini 1.5 Pro 0827 结合最新优化技术，带来更高效的多模态数据处理能力。',
      displayName: 'Gemini 1.5 Pro 0827',
      functionCall: true,
      id: 'gemini-1.5-pro-exp-0827',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.875,
        input: 3.5,
        output: 10.5,
      },
      releasedAt: '2024-08-27',
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description: 'Gemini 1.5 Pro 0801 提供出色的多模态处理能力，为应用开发带来更大灵活性。',
      displayName: 'Gemini 1.5 Pro 0801',
      functionCall: true,
      id: 'gemini-1.5-pro-exp-0801',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.875,
        input: 3.5,
        output: 10.5,
      },
      releasedAt: '2024-08-01',
      vision: true,
    },
    {
      contextWindowTokens: 30_720 + 2048,
      description: 'Gemini 1.0 Pro 是Google的高性能AI模型，专为广泛任务扩展而设计。',
      displayName: 'Gemini 1.0 Pro',
      id: 'gemini-1.0-pro-latest',
      maxOutput: 2048,
      pricing: {
        input: 0.5,
        output: 1.5,
      },
      releasedAt: '2023-12-06',
    },
    {
      contextWindowTokens: 30_720 + 2048,
      description:
        'Gemini 1.0 Pro 001 (Tuning) 提供稳定并可调优的性能，是复杂任务解决方案的理想选择。',
      displayName: 'Gemini 1.0 Pro 001 (Tuning)',
      functionCall: true,
      id: 'gemini-1.0-pro-001',
      maxOutput: 2048,
      pricing: {
        input: 0.5,
        output: 1.5,
      },
      releasedAt: '2023-12-06',
    },
    {
      contextWindowTokens: 30_720 + 2048,
      description: 'Gemini 1.0 Pro 002 (Tuning) 提供出色的多模态支持，专注于复杂任务的有效解决。',
      displayName: 'Gemini 1.0 Pro 002 (Tuning)',
      id: 'gemini-1.0-pro-002',
      maxOutput: 2048,
      pricing: {
        input: 0.5,
        output: 1.5,
      },
      releasedAt: '2023-12-06',
    },
    //mistral

    {
      contextWindowTokens: 128_000,
      description:
        'Mistral Nemo是一个与Nvidia合作开发的12B模型，提供出色的推理和编码性能，易于集成和替换。',
      displayName: 'Mistral Nemo',
      enabled: true,
      functionCall: true,
      id: 'open-mistral-nemo',
      pricing: {
        input: 0.15,
        output: 0.15,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Mistral Small是成本效益高、快速且可靠的选项，适用于翻译、摘要和情感分析等用例。',
      displayName: 'Mistral Small',
      enabled: true,
      functionCall: true,
      id: 'mistral-small-latest',
      pricing: {
        input: 0.2,
        output: 0.6,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Mistral Large是旗舰大模型，擅长多语言任务、复杂推理和代码生成，是高端应用的理想选择。',
      displayName: 'Mistral Large',
      enabled: true,
      functionCall: true,
      id: 'mistral-large-latest',
      pricing: {
        input: 2,
        output: 6,
      },
    },
    {
      contextWindowTokens: 32_768,
      description: 'Codestral是专注于代码生成的尖端生成模型，优化了中间填充和代码补全任务。',
      displayName: 'Codestral',
      id: 'codestral-latest',
      pricing: {
        input: 0.2,
        output: 0.6,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        'Pixtral 模型在图表和图理解、文档问答、多模态推理和指令遵循等任务上表现出强大的能力，能够以自然分辨率和宽高比摄入图像，还能够在长达 128K 令牌的长上下文窗口中处理任意数量的图像。',
      displayName: 'Pixtral 12B',
      enabled: true,
      id: 'pixtral-12b-2409',
      pricing: {
        input: 0.15,
        output: 0.15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: 'Ministral 3B 是Mistral的世界顶级边缘模型。',
      displayName: 'Ministral 3B',
      id: 'ministral-3b-latest',
      pricing: {
        input: 0.04,
        output: 0.04,
      },
    },
    {
      contextWindowTokens: 128_000,
      description: 'Ministral 8B 是Mistral的性价比极高的边缘模型。',
      displayName: 'Ministral 8B',
      id: 'ministral-8b-latest',
      pricing: {
        input: 0.1,
        output: 0.1,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        'Mistral 7B是一款紧凑但高性能的模型，擅长批量处理和简单任务，如分类和文本生成，具有良好的推理能力。',
      displayName: 'Mistral 7B',
      id: 'open-mistral-7b',
      pricing: {
        input: 0.25,
        output: 0.25,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        'Mixtral 8x7B是一个稀疏专家模型，利用多个参数提高推理速度，适合处理多语言和代码生成任务。',
      displayName: 'Mixtral 8x7B',
      id: 'open-mixtral-8x7b',
      pricing: {
        input: 0.7,
        output: 0.7,
      },
    },
    {
      contextWindowTokens: 65_536,
      description:
        'Mixtral 8x22B是一个更大的专家模型，专注于复杂任务，提供出色的推理能力和更高的吞吐量。',
      displayName: 'Mixtral 8x22B',
      functionCall: true,
      id: 'open-mixtral-8x22b',
      pricing: {
        input: 2,
        output: 6,
      },
    },
    {
      contextWindowTokens: 256_000,
      description:
        'Codestral Mamba是专注于代码生成的Mamba 2语言模型，为先进的代码和推理任务提供强力支持。',
      displayName: 'Codestral Mamba',
      id: 'open-codestral-mamba',
      pricing: {
        input: 0.15,
        output: 0.15,
      },
    },
    //minimax
    {
      contextWindowTokens: 245_760,
      description: '适用于广泛的自然语言处理任务，包括文本生成、对话系统等。',
      displayName: 'abab6.5s',
      enabled: true,
      functionCall: true,
      id: 'abab6.5s-chat',
    },
    {
      contextWindowTokens: 8192,
      description: '专为多语种人设对话设计，支持英文及其他多种语言的高质量对话生成。',
      displayName: 'abab6.5g',
      enabled: true,
      functionCall: true,
      id: 'abab6.5g-chat',
    },
    {
      contextWindowTokens: 8192,
      description: '针对中文人设对话场景优化，提供流畅且符合中文表达习惯的对话生成能力。',
      displayName: 'abab6.5t',
      enabled: true,
      functionCall: true,
      id: 'abab6.5t-chat',
    },
    {
      contextWindowTokens: 16_384,
      description: '面向生产力场景，支持复杂任务处理和高效文本生成，适用于专业领域应用。',
      displayName: 'abab5.5',
      id: 'abab5.5-chat',
    },
    {
      contextWindowTokens: 8192,
      description: '专为中文人设对话场景设计，提供高质量的中文对话生成能力，适用于多种应用场景。',
      displayName: 'abab5.5s',
      id: 'abab5.5s-chat',
    },
    //cohere
    {
      description: '',
      displayName: 'command-r',
      id: 'command-r',
      // tokens: ,
    },
    {
      description: '',
      displayName: 'command-r-plus',
      id: 'command-r-plus',
      // tokens: ,
    },
    {
      description: '',
      displayName: 'command-light',
      id: 'command-light',
      // tokens: ,
    },
    //doubao
    {
      description:
        'Doubao-lite拥有极致的响应速度，更好的性价比，为客户不同场景提供更灵活的选择。支持4k上下文窗口的推理和精调。',
      displayName: 'Doubao-lite-4k',
      id: 'Doubao-lite-4k',
      // tokens: ,
    },
    {
      description:
        'Doubao-lite拥有极致的响应速度，更好的性价比，为客户不同场景提供更灵活的选择。支持32k上下文窗口的推理和精调。',
      displayName: 'Doubao-lite-32k',
      id: 'Doubao-lite-32k',
      // tokens: ,
    },
    {
      description:
        'Doubao-lite 拥有极致的响应速度，更好的性价比，为客户不同场景提供更灵活的选择。支持128k上下文窗口的推理和精调。',
      displayName: 'Doubao-lite-128k',
      id: 'Doubao-lite-128k',
      // tokens: ,
    },
    {
      description:
        '效果最好的主力模型，适合处理复杂任务，在参考问答、总结摘要、创作、文本分类、角色扮演等场景都有很好的效果。支持4k上下文窗口的推理和精调。',
      displayName: 'Doubao-pro-4k',
      id: 'Doubao-pro-4k',
      // tokens: ,
    },
    {
      description:
        '效果最好的主力模型，适合处理复杂任务，在参考问答、总结摘要、创作、文本分类、角色扮演等场景都有很好的效果。支持32k上下文窗口的推理和精调。',
      displayName: 'Doubao-pro-32k',
      id: 'Doubao-pro-32k',
      // tokens: ,
    },
    {
      description:
        '效果最好的主力模型，适合处理复杂任务，在参考问答、总结摘要、创作、文本分类、角色扮演等场景都有很好的效果。支持128k上下文窗口的推理和精调。',
      displayName: 'Doubao-pro-128k',
      id: 'Doubao-pro-128k',
      // tokens: ,
    },
    {
      description:
        '云雀（Skylark）第二代模型，Skylark2-pro-character模型具有优秀的角色扮演和聊天能力，擅长根据用户prompt要求扮演不同角色与用户展开聊天，角色风格突出，对话内容自然流畅，适用于构建聊天机器人、虚拟助手和在线客服等场景，有较高的响应速度。',
      displayName: 'Skylark2-pro-character-4k',
      id: 'Skylark2-pro-character-4k',
      // tokens: ,
    },
    {
      description:
        '云雀（Skylark）第二代模型，Skylark2-pro版本有较高的模型精度，适用于较为复杂的文本生成场景，如专业领域文案生成、小说创作、高质量翻译等，上下文窗口长度为32k。',
      displayName: 'Skylark2-pro-32k',
      id: 'Skylark2-pro-32k',
      // tokens: ,
    },
    {
      description:
        '云雀（Skylark）第二代模型，Skylark2-pro模型有较高的模型精度，适用于较为复杂的文本生成场景，如专业领域文案生成、小说创作、高质量翻译等，上下文窗口长度为4k。',
      displayName: 'Skylark2-pro-4k',
      id: 'Skylark2-pro-4k',
      // tokens: ,
    },
    {
      description:
        '云雀（Skylark）第二代模型，Skylark2-pro-turbo-8k推理更快，成本更低，上下文窗口长度为8k。',
      displayName: 'Skylark2-pro-turbo-8k',
      id: 'Skylark2-pro-turbo-8k',
      // tokens: ,
    },
    {
      description:
        '云雀（Skylark）第二代模型，Skylark2-lite模型有较高的响应速度，适用于实时性要求高、成本敏感、对模型精度要求不高的场景，上下文窗口长度为8k。',
      displayName: 'Skylark2-lite-8k',
      id: 'Skylark2-lite-8k',
      // tokens: ,
    },
  ],
  checkModel: 'qwen-max',
  description:
    'Higress 是一款云原生 API 网关，在阿里内部为解决 Tengine reload 对长连接业务有损，以及 gRPC/Dubbo 负载均衡能力不足而诞生。',
  id: 'higress',
  modelList: { showModelFetcher: true },
  modelsUrl: 'https://higress.cn/',
  name: 'Higress',
  proxyUrl: {
    desc: '输入Higress AI Gateway的访问地址',
    placeholder: 'https://127.0.0.1:8080/v1',
    title: 'AI Gateway地址',
  },
  url: 'https://apig.console.aliyun.com/',
};

export default Higress;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/github.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref:
// https://github.com/marketplace/models
const Github: ModelProviderCard = {
  chatModels: [
    {
      contextWindowTokens: 200_000,
      description:
        '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。',
      displayName: 'OpenAI o1',
      enabled: true,
      functionCall: false,
      id: 'o1',
      maxOutput: 100_000,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: '比 o1-preview 更小、更快，成本低80%，在代码生成和小上下文操作方面表现良好。',
      displayName: 'OpenAI o1-mini',
      enabled: true,
      functionCall: false,
      id: 'o1-mini',
      maxOutput: 65_536,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深度上下文理解和自主工作流程的应用。',
      displayName: 'OpenAI o1-preview',
      enabled: true,
      functionCall: false,
      id: 'o1-preview',
      maxOutput: 32_768,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: '一种经济高效的AI解决方案，适用于多种文本和图像任务。',
      displayName: 'OpenAI GPT-4o mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description: 'OpenAI GPT-4系列中最先进的多模态模型，可以处理文本和图像输入。',
      displayName: 'OpenAI GPT-4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 262_144,
      description:
        '一个52B参数（12B活跃）的多语言模型，提供256K长上下文窗口、函数调用、结构化输出和基于事实的生成。',
      displayName: 'AI21 Jamba 1.5 Mini',
      functionCall: true,
      id: 'ai21-jamba-1.5-mini',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 262_144,
      description:
        '一个398B参数（94B活跃）的多语言模型，提供256K长上下文窗口、函数调用、结构化输出和基于事实的生成。',
      displayName: 'AI21 Jamba 1.5 Large',
      functionCall: true,
      id: 'ai21-jamba-1.5-large',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Command R是一个可扩展的生成模型，旨在针对RAG和工具使用，使企业能够实现生产级AI。',
      displayName: 'Cohere Command R',
      id: 'cohere-command-r',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Command R+是一个最先进的RAG优化模型，旨在应对企业级工作负载。',
      displayName: 'Cohere Command R+',
      id: 'cohere-command-r-plus',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Mistral Nemo是一种尖端的语言模型（LLM），在其尺寸类别中拥有最先进的推理、世界知识和编码能力。',
      displayName: 'Mistral Nemo',
      id: 'mistral-nemo',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Mistral Small可用于任何需要高效率和低延迟的基于语言的任务。',
      displayName: 'Mistral Small',
      id: 'mistral-small',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Mistral的旗舰模型，适合需要大规模推理能力或高度专业化的复杂任务（合成文本生成、代码生成、RAG或代理）。',
      displayName: 'Mistral Large',
      id: 'mistral-large',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '在高分辨率图像上表现出色的图像推理能力，适用于视觉理解应用。',
      displayName: 'Llama 3.2 11B Vision',
      id: 'llama-3.2-11b-vision-instruct',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description: '适用于视觉理解代理应用的高级图像推理能力。',
      displayName: 'Llama 3.2 90B Vision',
      id: 'llama-3.2-90b-vision-instruct',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 8B',
      id: 'meta-llama-3.1-8b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 70B',
      id: 'meta-llama-3.1-70b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1指令调优的文本模型，针对多语言对话用例进行了优化，在许多可用的开源和封闭聊天模型中，在常见行业基准上表现优异。',
      displayName: 'Meta Llama 3.1 405B',
      id: 'meta-llama-3.1-405b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个多功能的80亿参数模型，针对对话和文本生成任务进行了优化。',
      displayName: 'Meta Llama 3 8B',
      id: 'meta-llama-3-8b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个强大的700亿参数模型，在推理、编码和广泛的语言应用方面表现出色。',
      displayName: 'Meta Llama 3 70B',
      id: 'meta-llama-3-70b-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Phi-3-mini模型的更新版。',
      displayName: 'Phi-3.5-mini 128K',
      id: 'Phi-3.5-mini-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: 'Phi-3-vision模型的更新版。',
      displayName: 'Phi-3.5-vision 128K',
      id: 'Phi-3.5-vision-instrust',
      maxOutput: 4096,
      vision: true,
    },
    {
      contextWindowTokens: 4096,
      description: 'Phi-3家族中最小的成员，针对质量和低延迟进行了优化。',
      displayName: 'Phi-3-mini 4K',
      id: 'Phi-3-mini-4k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-mini模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-mini 128K',
      id: 'Phi-3-mini-128k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 8192,
      description: '一个70亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。',
      displayName: 'Phi-3-small 8K',
      id: 'Phi-3-small-8k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-small模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-small 128K',
      id: 'Phi-3-small-128k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 4096,
      description: '一个140亿参数模型，质量优于Phi-3-mini，重点关注高质量、推理密集型数据。',
      displayName: 'Phi-3-medium 4K',
      id: 'Phi-3-medium-4k-instruct',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 131_072,
      description: '相同的Phi-3-medium模型，但具有更大的上下文大小，适用于RAG或少量提示。',
      displayName: 'Phi-3-medium 128K',
      id: 'Phi-3-medium-128k-instruct',
      maxOutput: 4096,
    },
  ],
  checkModel: 'Phi-3-mini-4k-instruct',
  // Ref: https://github.blog/news-insights/product-news/introducing-github-models/
  description: '通过GitHub模型，开发人员可以成为AI工程师，并使用行业领先的AI模型进行构建。',
  enabled: true,
  id: 'github',
  modelList: { showModelFetcher: true }, // I'm not sure if it is good to show the model fetcher, as remote list is not complete.
  name: 'GitHub',
  url: 'https://github.com/marketplace/models',
};

export default Github;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/openai.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref: https://platform.openai.com/docs/deprecations
const OpenAI: ModelProviderCard = {
  chatModels: [
    {
      contextWindowTokens: 128_000,
      description:
        'o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-mini',
      enabled: true,
      id: 'o1-mini',
      maxOutput: 65_536,
      pricing: {
        input: 3,
        output: 12,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-preview',
      enabled: true,
      id: 'o1-preview',
      maxOutput: 32_768,
      pricing: {
        input: 15,
        output: 60,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'GPT-4o mini是OpenAI在GPT-4 Omni之后推出的最新模型，支持图文输入并输出文本。作为他们最先进的小型模型，它比其他近期的前沿模型便宜很多，并且比GPT-3.5 Turbo便宜超过60%。它保持了最先进的智能，同时具有显著的性价比。GPT-4o mini在MMLU测试中获得了 82% 的得分，目前在聊天偏好上排名高于 GPT-4。',
      displayName: 'GPT-4o mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      maxOutput: 16_385,
      pricing: {
        input: 0.15,
        output: 0.6,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o 1120',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-2024-11-20',
      pricing: {
        input: 2.5,
        output: 10,
      },
      releasedAt: '2024-11-20',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      pricing: {
        input: 2.5,
        output: 10,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o 0806',
      functionCall: true,
      id: 'gpt-4o-2024-08-06',
      pricing: {
        input: 2.5,
        output: 10,
      },
      releasedAt: '2024-08-06',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o 0513',
      functionCall: true,
      id: 'gpt-4o-2024-05-13',
      pricing: {
        input: 5,
        output: 15,
      },
      releasedAt: '2024-05-13',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'ChatGPT-4o',
      enabled: true,
      id: 'chatgpt-4o-latest',
      pricing: {
        input: 5,
        output: 15,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo',
      functionCall: true,
      id: 'gpt-4-turbo',
      pricing: {
        input: 10,
        output: 30,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Vision 0409',
      functionCall: true,
      id: 'gpt-4-turbo-2024-04-09',
      pricing: {
        input: 10,
        output: 30,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview',
      functionCall: true,
      id: 'gpt-4-turbo-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview 0125',
      functionCall: true,
      id: 'gpt-4-0125-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 128_000,
      description:
        '最新的 GPT-4 Turbo 模型具备视觉功能。现在，视觉请求可以使用 JSON 模式和函数调用。 GPT-4 Turbo 是一个增强版本，为多模态任务提供成本效益高的支持。它在准确性和效率之间找到平衡，适合需要进行实时交互的应用程序场景。',
      displayName: 'GPT-4 Turbo Preview 1106',
      functionCall: true,
      id: 'gpt-4-1106-preview',
      pricing: {
        input: 10,
        output: 30,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4',
      functionCall: true,
      id: 'gpt-4',
      pricing: {
        input: 30,
        output: 60,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4 0613',
      functionCall: true,
      id: 'gpt-4-0613',
      pricing: {
        input: 30,
        output: 60,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      // Will be discontinued on June 6, 2025
      displayName: 'GPT-4 32K',
      functionCall: true,
      id: 'gpt-4-32k',
      pricing: {
        input: 60,
        output: 120,
      },
    },
    {
      contextWindowTokens: 32_768,
      // Will be discontinued on June 6, 2025
      description:
        'GPT-4 提供了一个更大的上下文窗口，能够处理更长的文本输入，适用于需要广泛信息整合和数据分析的场景。',
      displayName: 'GPT-4 32K 0613',
      functionCall: true,
      id: 'gpt-4-32k-0613',
      pricing: {
        input: 60,
        output: 120,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo',
      functionCall: true,
      id: 'gpt-3.5-turbo',
      pricing: {
        input: 0.5,
        output: 1.5,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo 0125',
      functionCall: true,
      id: 'gpt-3.5-turbo-0125',
      pricing: {
        input: 0.5,
        output: 1.5,
      },
    },
    {
      contextWindowTokens: 16_385,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo 1106',
      functionCall: true,
      id: 'gpt-3.5-turbo-1106',
      pricing: {
        input: 1,
        output: 2,
      },
    },
    {
      contextWindowTokens: 4096,
      description:
        'GPT 3.5 Turbo，适用于各种文本生成和理解任务，Currently points to gpt-3.5-turbo-0125',
      displayName: 'GPT-3.5 Turbo Instruct',
      id: 'gpt-3.5-turbo-instruct',
      pricing: {
        input: 1.5,
        output: 2,
      },
    },
  ],
  checkModel: 'gpt-4o-mini',
  description:
    'OpenAI 是全球领先的人工智能研究机构，其开发的模型如GPT系列推动了自然语言处理的前沿。OpenAI 致力于通过创新和高效的AI解决方案改变多个行业。他们的产品具有显著的性能和经济性，广泛用于研究、商业和创新应用。',
  enabled: true,
  id: 'openai',
  modelList: { showModelFetcher: true },
  modelsUrl: 'https://platform.openai.com/docs/models',
  name: 'OpenAI',
  url: 'https://openai.com',
};

export default OpenAI;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/groq.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref https://console.groq.com/docs/tool-use
const Groq: ModelProviderCard = {
  chatModels: [
    // TODO: During preview launch, Groq is limiting 3.2 models to max_tokens of 8k.
    {
      contextWindowTokens: 131_072,
      description:
        'Meta Llama 3.3 多语言大语言模型 ( LLM ) 是 70B（文本输入/文本输出）中的预训练和指令调整生成模型。 Llama 3.3 指令调整的纯文本模型针对多语言对话用例进行了优化，并且在常见行业基准上优于许多可用的开源和封闭式聊天模型。',
      displayName: 'Llama 3.3 70B',
      enabled: true,
      functionCall: true,
      id: 'llama-3.3-70b-versatile',
      maxOutput: 8192,
      pricing: {
        input: 0.05,
        output: 0.08,
      },
    },
    {
      contextWindowTokens: 8192,
      description:
        'Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 11B Vision (Preview)',
      enabled: true,
      id: 'llama-3.2-11b-vision-preview',
      maxOutput: 8192,
      pricing: {
        input: 0.05,
        output: 0.08,
      },
      vision: true,
    },
    {
      contextWindowTokens: 8192,
      description:
        'Llama 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 90B Vision (Preview)',
      enabled: true,
      id: 'llama-3.2-90b-vision-preview',
      maxOutput: 8192,
      pricing: {
        input: 0.59,
        output: 0.79,
      },
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1 8B 是一款高效能模型，提供了快速的文本生成能力，非常适合需要大规模效率和成本效益的应用场景。',
      displayName: 'Llama 3.1 8B',
      enabled: true,
      functionCall: true,
      id: 'llama-3.1-8b-instant',
      maxOutput: 8192,
      pricing: {
        input: 0.05,
        output: 0.08,
      },
    },
    {
      contextWindowTokens: 131_072,
      description:
        'Llama 3.1 70B 提供更强大的AI推理能力，适合复杂应用，支持超多的计算处理并保证高效和准确率。',
      displayName: 'Llama 3.1 70B',
      enabled: true,
      functionCall: true,
      id: 'llama-3.1-70b-versatile',
      maxOutput: 8192,
      pricing: {
        input: 0.59,
        output: 0.79,
      },
    },
    /*
    // Offline due to overwhelming demand! Stay tuned for updates.
    {
      displayName: 'Llama 3.1 405B',
      functionCall: true,
      id: 'llama-3.1-405b-reasoning',
      tokens: 8_192,
    },
*/
    {
      contextWindowTokens: 8192,
      description: 'Llama 3 Groq 8B Tool Use 是针对高效工具使用优化的模型，支持快速并行计算。',
      displayName: 'Llama 3 Groq 8B Tool Use (Preview)',
      functionCall: true,
      id: 'llama3-groq-8b-8192-tool-use-preview',
      pricing: {
        input: 0.19,
        output: 0.19,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Llama 3 Groq 70B Tool Use 提供强大的工具调用能力，支持复杂任务的高效处理。',
      displayName: 'Llama 3 Groq 70B Tool Use (Preview)',
      functionCall: true,
      id: 'llama3-groq-70b-8192-tool-use-preview',
      pricing: {
        input: 0.89,
        output: 0.89,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Meta Llama 3 8B 带来优质的推理效能，适合多场景应用需求。',
      displayName: 'Meta Llama 3 8B',
      functionCall: true,
      id: 'llama3-8b-8192',
      pricing: {
        input: 0.05,
        output: 0.08,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Meta Llama 3 70B 提供无与伦比的复杂性处理能力，为高要求项目量身定制。',
      displayName: 'Meta Llama 3 70B',
      functionCall: true,
      id: 'llama3-70b-8192',
      pricing: {
        input: 0.59,
        output: 0.79,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Gemma 2 9B 是一款优化用于特定任务和工具整合的模型。',
      displayName: 'Gemma 2 9B',
      enabled: true,
      functionCall: true,
      id: 'gemma2-9b-it',
      pricing: {
        input: 0.2,
        output: 0.2,
      },
    },
    {
      contextWindowTokens: 8192,
      description: 'Gemma 7B 适合中小规模任务处理，兼具成本效益。',
      displayName: 'Gemma 7B',
      functionCall: true,
      id: 'gemma-7b-it',
      pricing: {
        input: 0.07,
        output: 0.07,
      },
    },
    {
      contextWindowTokens: 32_768,
      description: 'Mixtral 8x7B 提供高容错的并行计算能力，适合复杂任务。',
      displayName: 'Mixtral 8x7B',
      functionCall: true,
      id: 'mixtral-8x7b-32768',
      pricing: {
        input: 0.24,
        output: 0.24,
      },
    },
    {
      contextWindowTokens: 4096,
      description: 'LLaVA 1.5 7B 提供视觉处理能力融合，通过视觉信息输入生成复杂输出。',
      displayName: 'LLaVA 1.5 7B',
      id: 'llava-v1.5-7b-4096-preview',
      vision: true,
    },
  ],
  checkModel: 'llama-3.1-8b-instant',
  description:
    'Groq 的 LPU 推理引擎在最新的独立大语言模型（LLM）基准测试中表现卓越，以其惊人的速度和效率重新定义了 AI 解决方案的标准。Groq 是一种即时推理速度的代表，在基于云的部署中展现了良好的性能。',
  id: 'groq',
  modelList: { showModelFetcher: true },
  modelsUrl: 'https://console.groq.com/docs/models',
  name: 'Groq',
  proxyUrl: {
    placeholder: 'https://api.groq.com/openai/v1',
  },
  url: 'https://groq.com',
};

export default Groq;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/azure.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref: https://learn.microsoft.com/azure/ai-services/openai/concepts/models
const Azure: ModelProviderCard = {
  chatModels: [
    {
      contextWindowTokens: 16_385,
      deploymentName: 'gpt-35-turbo',
      description:
        'GPT 3.5 Turbo，OpenAI提供的高效模型，适用于聊天和文本生成任务，支持并行函数调用。',
      displayName: 'GPT 3.5 Turbo',
      enabled: true,
      functionCall: true,
      id: 'gpt-35-turbo',
      maxOutput: 4096,
    },
    {
      contextWindowTokens: 16_384,
      deploymentName: 'gpt-35-turbo-16k',
      description: 'GPT 3.5 Turbo 16k，高容量文本生成模型，适合复杂任务。',
      displayName: 'GPT 3.5 Turbo',
      functionCall: true,
      id: 'gpt-35-turbo-16k',
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4-turbo',
      description: 'GPT 4 Turbo，多模态模型，提供杰出的语言理解和生成能力，同时支持图像输入。',
      displayName: 'GPT 4 Turbo',
      enabled: true,
      functionCall: true,
      id: 'gpt-4',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4o-mini',
      description: 'GPT-4o Mini，小型高效模型，具备与GPT-4o相似的卓越性能。',
      displayName: 'GPT 4o Mini',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o-mini',
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      deploymentName: 'gpt-4o',
      description: 'GPT-4o 是最新的多模态模型，结合高级文本和图像处理能力。',
      displayName: 'GPT 4o',
      enabled: true,
      functionCall: true,
      id: 'gpt-4o',
      vision: true,
    },
  ],
  defaultShowBrowserRequest: true,
  description:
    'Azure 提供多种先进的AI模型，包括GPT-3.5和最新的GPT-4系列，支持多种数据类型和复杂任务，致力于安全、可靠和可持续的AI解决方案。',
  id: 'azure',
  modelsUrl: 'https://learn.microsoft.com/azure/ai-services/openai/concepts/models',
  name: 'Azure',
  url: 'https://azure.microsoft.com',
};

export default Azure;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/openrouter.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref :https://openrouter.ai/docs#models
const OpenRouter: ModelProviderCard = {
  chatModels: [
    {
      contextWindowTokens: 128_000,
      description:
        '根据上下文长度、主题和复杂性，你的请求将发送到 Llama 3 70B Instruct、Claude 3.5 Sonnet（自我调节）或 GPT-4o。',
      displayName: 'Auto (best for prompt)',
      enabled: true,
      functionCall: false,
      id: 'openrouter/auto',
      vision: false,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-mini',
      enabled: true,
      id: 'openai/o1-mini',
      maxOutput: 65_536,
      pricing: {
        input: 3,
        output: 12,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。',
      displayName: 'OpenAI o1-preview',
      enabled: true,
      id: 'openai/o1-preview',
      maxOutput: 32_768,
      pricing: {
        input: 15,
        output: 60,
      },
      releasedAt: '2024-09-12',
    },
    {
      contextWindowTokens: 128_000,
      description:
        'GPT-4o mini是OpenAI在GPT-4 Omni之后推出的最新模型，支持图文输入并输出文本。作为他们最先进的小型模型，它比其他近期的前沿模型便宜很多，并且比GPT-3.5 Turbo便宜超过60%。它保持了最先进的智能，同时具有显著的性价比。GPT-4o mini在MMLU测试中获得了 82% 的得分，目前在聊天偏好上排名高于 GPT-4。',
      displayName: 'GPT-4o mini',
      enabled: true,
      functionCall: true,
      id: 'openai/gpt-4o-mini',
      maxOutput: 16_385,
      pricing: {
        input: 0.15,
        output: 0.6,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        'ChatGPT-4o 是一款动态模型，实时更新以保持当前最新版本。它结合了强大的语言理解与生成能力，适合于大规模应用场景，包括客户服务、教育和技术支持。',
      displayName: 'GPT-4o',
      enabled: true,
      functionCall: true,
      id: 'openai/gpt-4o',
      pricing: {
        input: 2.5,
        output: 10,
      },
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Haiku 是 Anthropic 的最快且最紧凑的模型，旨在实现近乎即时的响应。它具有快速且准确的定向性能。',
      displayName: 'Claude 3 Haiku',
      enabled: true,
      functionCall: true,
      id: 'anthropic/claude-3-haiku',
      maxOutput: 4096,
      pricing: {
        cachedInput: 0.025,
        input: 0.25,
        output: 1.25,
        writeCacheInput: 0.3125,
      },
      releasedAt: '2024-03-07',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3.5 Sonnet 提供了超越 Opus 的能力和比 Sonnet 更快的速度，同时保持与 Sonnet 相同的价格。Sonnet 特别擅长编程、数据科学、视觉处理、代理任务。',
      displayName: 'Claude 3.5 Sonnet',
      enabled: true,
      functionCall: true,
      id: 'anthropic/claude-3.5-sonnet',
      maxOutput: 8192,
      pricing: {
        cachedInput: 0.3,
        input: 3,
        output: 15,
        writeCacheInput: 3.75,
      },
      releasedAt: '2024-06-20',
      vision: true,
    },
    {
      contextWindowTokens: 200_000,
      description:
        'Claude 3 Opus 是 Anthropic 用于处理高度复杂任务的最强大模型。它在性能、智能、流畅性和理解力方面表现卓越。',
      displayName: 'Claude 3 Opus',
      enabled: true,
      functionCall: true,
      id: 'anthropic/claude-3-opus',
      maxOutput: 4096,
      pricing: {
        cachedInput: 1.5,
        input: 15,
        output: 75,
        writeCacheInput: 18.75,
      },
      releasedAt: '2024-02-29',
      vision: true,
    },
    {
      contextWindowTokens: 1_000_000 + 8192,
      description: 'Gemini 1.5 Flash 提供了优化后的多模态处理能力，适用多种复杂任务场景。',
      displayName: 'Gemini 1.5 Flash',
      enabled: true,
      functionCall: true,
      id: 'google/gemini-flash-1.5',
      maxOutput: 8192,
      pricing: {
        input: 0.075,
        output: 0.3,
      },
      vision: true,
    },
    {
      contextWindowTokens: 2_000_000 + 8192,
      description: 'Gemini 1.5 Pro 结合最新优化技术，带来更高效的多模态数据处理能力。',
      displayName: 'Gemini 1.5 Pro',
      enabled: true,
      functionCall: true,
      id: 'google/gemini-pro-1.5',
      maxOutput: 8192,
      pricing: {
        input: 3.5,
        output: 10.5,
      },
      vision: true,
    },
    {
      contextWindowTokens: 128_000,
      description:
        '融合通用与代码能力的全新开源模型, 不仅保留了原有 Chat 模型的通用对话能力和 Coder 模型的强大代码处理能力，还更好地对齐了人类偏好。此外，DeepSeek-V2.5 在写作任务、指令跟随等多个方面也实现了大幅提升。',
      displayName: 'DeepSeek V2.5',
      enabled: true,
      functionCall: true,
      id: 'deepseek/deepseek-chat',
      pricing: {
        input: 0.14,
        output: 0.28,
      },
      releasedAt: '2024-09-05',
    },
    {
      contextWindowTokens: 131_072,
      description:
        'LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 11B Vision',
      enabled: true,
      id: 'meta-llama/llama-3.2-11b-vision-instruct',
      pricing: {
        input: 0.162,
        output: 0.162,
      },
      vision: true,
    },
    {
      contextWindowTokens: 131_072,
      description:
        'LLaMA 3.2 旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，跨越了语言生成和视觉推理之间的鸿沟。',
      displayName: 'Llama 3.2 90B Vision',
      enabled: true,
      id: 'meta-llama/llama-3.2-90b-vision-instruct',
      pricing: {
        input: 0.4,
        output: 0.4,
      },
      vision: true,
    },
    {
      contextWindowTokens: 32_768,
      description: 'Qwen2 是全新的大型语言模型系列，具有更强的理解和生成能力。',
      displayName: 'Qwen2 7B (Free)',
      enabled: true,
      id: 'qwen/qwen-2-7b-instruct:free',
    },
    {
      contextWindowTokens: 32_768,
      description: 'LLaMA 3.1 提供多语言支持，是业界领先的生成模型之一。',
      displayName: 'Llama 3.1 8B (Free)',
      enabled: true,
      id: 'meta-llama/llama-3.1-8b-instruct:free',
    },
    {
      contextWindowTokens: 8192,
      description: 'Gemma 2 是Google轻量化的开源文本模型系列。',
      displayName: 'Gemma 2 9B (Free)',
      enabled: true,
      id: 'google/gemma-2-9b-it:free',
    },
  ],
  checkModel: 'google/gemma-2-9b-it:free',
  description:
    'OpenRouter 是一个提供多种前沿大模型接口的服务平台，支持 OpenAI、Anthropic、LLaMA 及更多，适合多样化的开发和应用需求。用户可根据自身需求灵活选择最优的模型和价格，助力AI体验的提升。',
  id: 'openrouter',
  modelList: { showModelFetcher: true },
  modelsUrl: 'https://openrouter.ai/models',
  name: 'OpenRouter',
  url: 'https://openrouter.ai',
};

export default OpenRouter;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/index.ts
================================================================================

import { ChatModelCard, ModelProviderCard } from '@/types/llm';

import Ai21Provider from './ai21';
import Ai360Provider from './ai360';
import AnthropicProvider from './anthropic';
import AzureProvider from './azure';
import BaichuanProvider from './baichuan';
import BedrockProvider from './bedrock';
import CloudflareProvider from './cloudflare';
import DeepSeekProvider from './deepseek';
import FireworksAIProvider from './fireworksai';
import GiteeAIProvider from './giteeai';
import GithubProvider from './github';
import GoogleProvider from './google';
import GroqProvider from './groq';
import HigressProvider from './higress';
import HuggingFaceProvider from './huggingface';
import HunyuanProvider from './hunyuan';
import InternLMProvider from './internlm';
import MinimaxProvider from './minimax';
import MistralProvider from './mistral';
import MoonshotProvider from './moonshot';
import NovitaProvider from './novita';
import OllamaProvider from './ollama';
import OpenAIProvider from './openai';
import OpenRouterProvider from './openrouter';
import PerplexityProvider from './perplexity';
import QwenProvider from './qwen';
import SenseNovaProvider from './sensenova';
import SiliconCloudProvider from './siliconcloud';
import SparkProvider from './spark';
import StepfunProvider from './stepfun';
import TaichuProvider from './taichu';
import TogetherAIProvider from './togetherai';
import UpstageProvider from './upstage';
import WenxinProvider from './wenxin';
import XAIProvider from './xai';
import ZeroOneProvider from './zeroone';
import ZhiPuProvider from './zhipu';

export const LOBE_DEFAULT_MODEL_LIST: ChatModelCard[] = [
  OpenAIProvider.chatModels,
  QwenProvider.chatModels,
  ZhiPuProvider.chatModels,
  BedrockProvider.chatModels,
  DeepSeekProvider.chatModels,
  GoogleProvider.chatModels,
  GroqProvider.chatModels,
  GithubProvider.chatModels,
  MinimaxProvider.chatModels,
  MistralProvider.chatModels,
  MoonshotProvider.chatModels,
  OllamaProvider.chatModels,
  OpenRouterProvider.chatModels,
  TogetherAIProvider.chatModels,
  FireworksAIProvider.chatModels,
  PerplexityProvider.chatModels,
  AnthropicProvider.chatModels,
  HuggingFaceProvider.chatModels,
  XAIProvider.chatModels,
  ZeroOneProvider.chatModels,
  StepfunProvider.chatModels,
  NovitaProvider.chatModels,
  BaichuanProvider.chatModels,
  TaichuProvider.chatModels,
  CloudflareProvider.chatModels,
  Ai360Provider.chatModels,
  SiliconCloudProvider.chatModels,
  GiteeAIProvider.chatModels,
  UpstageProvider.chatModels,
  SparkProvider.chatModels,
  Ai21Provider.chatModels,
  HunyuanProvider.chatModels,
  WenxinProvider.chatModels,
  SenseNovaProvider.chatModels,
  InternLMProvider.chatModels,
  HigressProvider.chatModels,
].flat();

export const DEFAULT_MODEL_PROVIDER_LIST = [
  OpenAIProvider,
  { ...AzureProvider, chatModels: [] },
  OllamaProvider,
  AnthropicProvider,
  BedrockProvider,
  GoogleProvider,
  DeepSeekProvider,
  HuggingFaceProvider,
  OpenRouterProvider,
  CloudflareProvider,
  GithubProvider,
  NovitaProvider,
  TogetherAIProvider,
  FireworksAIProvider,
  GroqProvider,
  PerplexityProvider,
  MistralProvider,
  Ai21Provider,
  UpstageProvider,
  XAIProvider,
  QwenProvider,
  WenxinProvider,
  HunyuanProvider,
  SparkProvider,
  ZhiPuProvider,
  ZeroOneProvider,
  SenseNovaProvider,
  StepfunProvider,
  MoonshotProvider,
  BaichuanProvider,
  MinimaxProvider,
  Ai360Provider,
  TaichuProvider,
  InternLMProvider,
  SiliconCloudProvider,
  HigressProvider,
  GiteeAIProvider,
];

export const filterEnabledModels = (provider: ModelProviderCard) => {
  return provider.chatModels.filter((v) => v.enabled).map((m) => m.id);
};

export const isProviderDisableBroswerRequest = (id: string) => {
  const provider = DEFAULT_MODEL_PROVIDER_LIST.find((v) => v.id === id && v.disableBrowserRequest);
  return !!provider;
};

export { default as Ai21ProviderCard } from './ai21';
export { default as Ai360ProviderCard } from './ai360';
export { default as AnthropicProviderCard } from './anthropic';
export { default as AzureProviderCard } from './azure';
export { default as BaichuanProviderCard } from './baichuan';
export { default as BedrockProviderCard } from './bedrock';
export { default as CloudflareProviderCard } from './cloudflare';
export { default as DeepSeekProviderCard } from './deepseek';
export { default as FireworksAIProviderCard } from './fireworksai';
export { default as GiteeAIProviderCard } from './giteeai';
export { default as GithubProviderCard } from './github';
export { default as GoogleProviderCard } from './google';
export { default as GroqProviderCard } from './groq';
export { default as HigressProviderCard } from './higress';
export { default as HuggingFaceProviderCard } from './huggingface';
export { default as HunyuanProviderCard } from './hunyuan';
export { default as InternLMProviderCard } from './internlm';
export { default as MinimaxProviderCard } from './minimax';
export { default as MistralProviderCard } from './mistral';
export { default as MoonshotProviderCard } from './moonshot';
export { default as NovitaProviderCard } from './novita';
export { default as OllamaProviderCard } from './ollama';
export { default as OpenAIProviderCard } from './openai';
export { default as OpenRouterProviderCard } from './openrouter';
export { default as PerplexityProviderCard } from './perplexity';
export { default as QwenProviderCard } from './qwen';
export { default as SenseNovaProviderCard } from './sensenova';
export { default as SiliconCloudProviderCard } from './siliconcloud';
export { default as SparkProviderCard } from './spark';
export { default as StepfunProviderCard } from './stepfun';
export { default as TaichuProviderCard } from './taichu';
export { default as TogetherAIProviderCard } from './togetherai';
export { default as UpstageProviderCard } from './upstage';
export { default as WenxinProviderCard } from './wenxin';
export { default as XAIProviderCard } from './xai';
export { default as ZeroOneProviderCard } from './zeroone';
export { default as ZhiPuProviderCard } from './zhipu';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/modelProviders/zeroone.ts
================================================================================

import { ModelProviderCard } from '@/types/llm';

// ref: https://platform.lingyiwanwu.com/docs#%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%AE%A1%E8%B4%B9
const ZeroOne: ModelProviderCard = {
  chatModels: [
    {
      contextWindowTokens: 16_384,
      description: '最新高性能模型，保证高质量输出同时，推理速度大幅提升。',
      displayName: 'Yi Lightning',
      enabled: true,
      id: 'yi-lightning',
      pricing: {
        currency: 'CNY',
        input: 0.99,
        output: 0.99,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '小而精悍，轻量极速模型。提供强化数学运算和代码编写能力。',
      displayName: 'Yi Spark',
      enabled: true,
      id: 'yi-spark',
      pricing: {
        currency: 'CNY',
        input: 1,
        output: 1,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '中型尺寸模型升级微调，能力均衡，性价比高。深度优化指令遵循能力。',
      displayName: 'Yi Medium',
      enabled: true,
      id: 'yi-medium',
      pricing: {
        currency: 'CNY',
        input: 2.5,
        output: 2.5,
      },
    },
    {
      contextWindowTokens: 200_000,
      description: '200K 超长上下文窗口，提供长文本深度理解和生成能力。',
      displayName: 'Yi Medium 200K',
      enabled: true,
      id: 'yi-medium-200k',
      pricing: {
        currency: 'CNY',
        input: 12,
        output: 12,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '超高性价比、卓越性能。根据性能和推理速度、成本，进行平衡性高精度调优。',
      displayName: 'Yi Large Turbo',
      enabled: true,
      id: 'yi-large-turbo',
      pricing: {
        currency: 'CNY',
        input: 12,
        output: 12,
      },
    },
    {
      contextWindowTokens: 16_384,
      description:
        '基于 yi-large 超强模型的高阶服务，结合检索与生成技术提供精准答案，实时全网检索信息服务。',
      displayName: 'Yi Large RAG',
      enabled: true,
      id: 'yi-large-rag',
      pricing: {
        currency: 'CNY',
        input: 25,
        output: 25,
      },
    },
    {
      contextWindowTokens: 32_768,
      description:
        '在 yi-large 模型的基础上支持并强化了工具调用的能力，适用于各种需要搭建 agent 或 workflow 的业务场景。',
      displayName: 'Yi Large FC',
      enabled: true,
      functionCall: true,
      id: 'yi-large-fc',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 32_768,
      description: '全新千亿参数模型，提供超强问答及文本生成能力。',
      displayName: 'Yi Large',
      id: 'yi-large',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '复杂视觉任务模型，提供高性能图片理解、分析能力。',
      displayName: 'Yi Vision',
      enabled: true,
      id: 'yi-vision',
      pricing: {
        currency: 'CNY',
        input: 6,
        output: 6,
      },
      vision: true,
    },
    {
      contextWindowTokens: 16_384,
      description: '初期版本，推荐使用 yi-large（新版本）。',
      displayName: 'Yi Large Preview',
      id: 'yi-large-preview',
      pricing: {
        currency: 'CNY',
        input: 20,
        output: 20,
      },
    },
    {
      contextWindowTokens: 16_384,
      description: '轻量化版本，推荐使用 yi-lightning。',
      displayName: 'Yi Lightning Lite',
      id: 'yi-lightning-lite',
      pricing: {
        currency: 'CNY',
        input: 0.99,
        output: 0.99,
      },
    },
  ],
  checkModel: 'yi-lightning',
  description:
    '零一万物致力于推动以人为本的AI 2.0技术革命，旨在通过大语言模型创造巨大的经济和社会价值，并开创新的AI生态与商业模式。',
  id: 'zeroone',
  modelsUrl: 'https://platform.lingyiwanwu.com/docs#模型与计费',
  name: '01.AI',
  url: 'https://www.lingyiwanwu.com/',
};

export default ZeroOne;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/config/featureFlags/schema.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix */
import { z } from 'zod';

export const FeatureFlagsSchema = z.object({
  /**
   * Enable WebRTC sync
   */
  webrtc_sync: z.boolean().optional(),
  check_updates: z.boolean().optional(),
  pin_list: z.boolean().optional(),

  // settings
  language_model_settings: z.boolean().optional(),

  openai_api_key: z.boolean().optional(),
  openai_proxy_url: z.boolean().optional(),

  create_session: z.boolean().optional(),
  edit_agent: z.boolean().optional(),

  plugins: z.boolean().optional(),
  dalle: z.boolean().optional(),
  speech_to_text: z.boolean().optional(),
  token_counter: z.boolean().optional(),

  welcome_suggest: z.boolean().optional(),
  changelog: z.boolean().optional(),

  clerk_sign_up: z.boolean().optional(),

  market: z.boolean().optional(),
  knowledge_base: z.boolean().optional(),

  rag_eval: z.boolean().optional(),

  // internal flag
  cloud_promotion: z.boolean().optional(),

  // the flags below can only be used with commercial license
  // if you want to use it in the commercial usage
  // please contact us for more information: hello@lobehub.com
  commercial_hide_github: z.boolean().optional(),
  commercial_hide_docs: z.boolean().optional(),
});

export type IFeatureFlags = z.infer<typeof FeatureFlagsSchema>;

export const DEFAULT_FEATURE_FLAGS: IFeatureFlags = {
  webrtc_sync: false,
  pin_list: false,

  language_model_settings: true,

  openai_api_key: true,
  openai_proxy_url: true,

  create_session: true,
  edit_agent: true,

  plugins: true,
  dalle: true,

  check_updates: true,
  welcome_suggest: true,
  token_counter: true,

  knowledge_base: true,
  rag_eval: false,

  clerk_sign_up: true,

  cloud_promotion: false,

  market: true,
  speech_to_text: true,
  changelog: true,

  // the flags below can only be used with commercial license
  // if you want to use it in the commercial usage
  // please contact us for more information: hello@lobehub.com
  commercial_hide_github: false,
  commercial_hide_docs: false,
};

export const mapFeatureFlagsEnvToState = (config: IFeatureFlags) => {
  return {
    enableWebrtc: config.webrtc_sync,
    isAgentEditable: config.edit_agent,

    showCreateSession: config.create_session,
    showLLM: config.language_model_settings,
    showPinList: config.pin_list,

    showOpenAIApiKey: config.openai_api_key,
    showOpenAIProxyUrl: config.openai_proxy_url,

    enablePlugins: config.plugins,
    showDalle: config.dalle,
    showChangelog: config.changelog,

    enableCheckUpdates: config.check_updates,
    showWelcomeSuggest: config.welcome_suggest,

    enableClerkSignUp: config.clerk_sign_up,

    enableKnowledgeBase: config.knowledge_base,
    enableRAGEval: config.rag_eval,

    showCloudPromotion: config.cloud_promotion,

    showMarket: config.market,
    enableSTT: config.speech_to_text,

    hideGitHub: config.commercial_hide_github,
    hideDocs: config.commercial_hide_docs,
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileSidePanel/index.tsx
================================================================================

'use client';

import { DraggablePanel, DraggablePanelContainer, type DraggablePanelProps } from '@lobehub/ui';
import { createStyles, useResponsive } from 'antd-style';
import isEqual from 'fast-deep-equal';
import { PropsWithChildren, memo, useEffect, useState } from 'react';

import { FOLDER_WIDTH } from '@/const/layoutTokens';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';

export const useStyles = createStyles(({ css, token }) => ({
  panel: css`
    height: 100%;
    background: ${token.colorBgContainer};
  `,
}));

const FileSidePanel = memo<PropsWithChildren>(({ children }) => {
  const { md = true } = useResponsive();

  const { styles } = useStyles();
  const [filePanelWidth, showFilePanel, updateSystemStatus] = useGlobalStore((s) => [
    systemStatusSelectors.filePanelWidth(s),
    systemStatusSelectors.showFilePanel(s),
    s.updateSystemStatus,
  ]);

  const [tmpWidth, setWidth] = useState(filePanelWidth);
  if (tmpWidth !== filePanelWidth) setWidth(filePanelWidth);
  const [cacheExpand, setCacheExpand] = useState<boolean>(Boolean(showFilePanel));

  const handleExpand = (expand: boolean) => {
    if (isEqual(expand, showFilePanel)) return;
    updateSystemStatus({ showFilePanel: expand });
    setCacheExpand(expand);
  };
  useEffect(() => {
    if (md && cacheExpand) updateSystemStatus({ showFilePanel: true });
    if (!md) updateSystemStatus({ showFilePanel: false });
  }, [md, cacheExpand]);

  const handleSizeChange: DraggablePanelProps['onSizeChange'] = (_, size) => {
    if (!size) return;
    const nextWidth = typeof size.width === 'string' ? Number.parseInt(size.width) : size.width;
    if (!nextWidth) return;

    if (isEqual(nextWidth, filePanelWidth)) return;
    setWidth(nextWidth);
    updateSystemStatus({ filePanelWidth: nextWidth });
  };

  return (
    <DraggablePanel
      className={styles.panel}
      defaultSize={{ width: tmpWidth }}
      expand={showFilePanel}
      maxWidth={320}
      minWidth={FOLDER_WIDTH}
      mode={md ? 'fixed' : 'float'}
      onExpandChange={handleExpand}
      onSizeChange={handleSizeChange}
      placement="left"
      size={{ height: '100%', width: filePanelWidth }}
    >
      <DraggablePanelContainer
        style={{
          flex: 'none',
          height: '100%',
          minWidth: FOLDER_WIDTH,
        }}
      >
        {children}
      </DraggablePanelContainer>
    </DraggablePanel>
  );
});

export default FileSidePanel;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Portal/FilePreview/Body/index.tsx
================================================================================

import { Icon, Markdown } from '@lobehub/ui';
import { Segmented } from 'antd';
import { BoltIcon, FileIcon } from 'lucide-react';
import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import Loading from '@/components/Loading/CircleLoading';
import FileViewer from '@/features/FileViewer';
import { useChatStore } from '@/store/chat';
import { chatPortalSelectors } from '@/store/chat/selectors';
import { useFileStore } from '@/store/file';

const FilePreview = () => {
  const previewFileId = useChatStore(chatPortalSelectors.previewFileId);
  const chunkText = useChatStore(chatPortalSelectors.chunkText);
  const useFetchFileItem = useFileStore((s) => s.useFetchFileItem);
  const { t } = useTranslation('portal');

  const [tab, setTab] = useState('chunk');
  const { data, isLoading } = useFetchFileItem(previewFileId);

  if (isLoading) return <Loading />;
  if (!data) return;

  const showChunk = tab === 'chunk' && !!chunkText;
  return (
    <Flexbox
      height={'100%'}
      paddingBlock={'0 4px'}
      paddingInline={4}
      style={{ borderRadius: 4, overflow: 'hidden' }}
    >
      {chunkText && (
        <Segmented
          block
          onChange={setTab}
          options={[
            { icon: <Icon icon={BoltIcon} />, label: t('FilePreview.tabs.chunk'), value: 'chunk' },
            { icon: <Icon icon={FileIcon} />, label: t('FilePreview.tabs.file'), value: 'file' },
          ]}
          value={tab}
        />
      )}

      {showChunk ? (
        <Markdown style={{ overflow: 'scroll', paddingInline: 8 }}>{chunkText}</Markdown>
      ) : (
        <Flexbox flex={1} paddingBlock={8} style={{ overflow: 'scroll' }}>
          <FileViewer {...data} />
        </Flexbox>
      )}
    </Flexbox>
  );
};

export default FilePreview;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/PWAInstall/Install.tsx
================================================================================

'use client';

import dynamic from 'next/dynamic';
import { memo, useEffect, useLayoutEffect } from 'react';
import { useTranslation } from 'react-i18next';

import { BRANDING_NAME } from '@/const/branding';
import { PWA_INSTALL_ID } from '@/const/layoutTokens';
import { usePWAInstall } from '@/hooks/usePWAInstall';
import { useGlobalStore } from '@/store/global';
import { systemStatusSelectors } from '@/store/global/selectors';
import { useUserStore } from '@/store/user';

// @ts-ignore
const PWA: any = dynamic(() => import('@khmyznikov/pwa-install/dist/pwa-install.react.js'), {
  ssr: false,
});

const PWAInstall = memo(() => {
  const { t } = useTranslation('metadata');

  const { install, canInstall } = usePWAInstall();

  const isShowPWAGuide = useUserStore((s) => s.isShowPWAGuide);
  const [hidePWAInstaller, updateSystemStatus] = useGlobalStore((s) => [
    systemStatusSelectors.hidePWAInstaller(s),
    s.updateSystemStatus,
  ]);

  // we need to make the pwa installer hidden by default
  useLayoutEffect(() => {
    sessionStorage.setItem('pwa-hide-install', 'true');
  }, []);

  const pwaInstall =
    // eslint-disable-next-line unicorn/prefer-query-selector
    typeof window === 'undefined' ? undefined : document.getElementById(PWA_INSTALL_ID);

  // add an event listener to control the user close installer action
  useEffect(() => {
    if (!pwaInstall) return;

    const handler = (e: Event) => {
      const event = e as CustomEvent;

      // it means user hide installer
      if (event.detail.message === 'dismissed') {
        updateSystemStatus({ hidePWAInstaller: true });
      }
    };

    pwaInstall.addEventListener('pwa-user-choice-result-event', handler);
    return () => {
      pwaInstall.removeEventListener('pwa-user-choice-result-event', handler);
    };
  }, [pwaInstall]);

  // trigger the PWA guide on demand
  useEffect(() => {
    if (!canInstall || hidePWAInstaller) return;

    // trigger the pwa installer and register the service worker
    if (isShowPWAGuide) {
      install();
      if ('serviceWorker' in navigator && window.serwist !== undefined) {
        window.serwist.register();
      }
    }
  }, [canInstall, hidePWAInstaller, isShowPWAGuide]);

  return (
    <PWA
      description={t('chat.description', { appName: BRANDING_NAME })}
      id={PWA_INSTALL_ID}
      manifest-url={'/manifest.webmanifest'}
    />
  );
});

export default PWAInstall;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/AgentSetting/AgentPlugin/LoadingList.tsx
================================================================================

import { Skeleton } from 'antd';
import { createStyles } from 'antd-style';

const useStyles = createStyles(({ css, prefixCls }) => ({
  avatar: css`
    .${prefixCls}-skeleton-header {
      padding-inline-end: 0;
    }
  `,
  label: css`
    li {
      height: 14px !important;
    }

    li + li {
      margin-block-start: 12px !important;
    }
  `,
}));

const LoadingList = () => {
  const { styles } = useStyles();

  const loadingItem = {
    avatar: (
      <Skeleton
        active
        avatar={{ size: 40 }}
        className={styles.avatar}
        paragraph={false}
        title={false}
      />
    ),
    label: (
      <Skeleton
        active
        avatar={false}
        paragraph={{ className: styles.label, style: { marginBottom: 0 } }}
        style={{ width: 300 }}
        title={false}
      />
    ),
  };

  return [loadingItem, loadingItem, loadingItem];
};

export default LoadingList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/AgentSetting/AgentTTS/index.tsx
================================================================================

'use client';

import { VoiceList } from '@lobehub/tts';
import { Form, ItemGroup } from '@lobehub/ui';
import { Select, Switch } from 'antd';
import { debounce } from 'lodash-es';
import { Mic } from 'lucide-react';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';

import { FORM_STYLE } from '@/const/layoutTokens';
import { useUserStore } from '@/store/user';
import { userGeneralSettingsSelectors } from '@/store/user/selectors';

import { useStore } from '../store';
import { useAgentSyncSettings } from '../useSyncAgemtSettings';
import SelectWithTTSPreview from './SelectWithTTSPreview';
import { ttsOptions } from './options';

const TTS_SETTING_KEY = 'tts';
const { openaiVoiceOptions, localeOptions } = VoiceList;

const AgentTTS = memo(() => {
  const { t } = useTranslation('setting');
  const [form] = Form.useForm();
  const voiceList = useUserStore((s) => {
    const locale = userGeneralSettingsSelectors.currentLanguage(s);
    return (all?: boolean) => new VoiceList(all ? undefined : locale);
  });
  const [showAllLocaleVoice, ttsService, updateConfig] = useStore((s) => [
    s.config.tts.showAllLocaleVoice,
    s.config.tts.ttsService,
    s.setAgentConfig,
  ]);

  useAgentSyncSettings(form);

  const { edgeVoiceOptions, microsoftVoiceOptions } = useMemo(
    () => voiceList(showAllLocaleVoice),
    [showAllLocaleVoice],
  );

  const tts: ItemGroup = {
    children: [
      {
        children: <Select options={ttsOptions} />,
        desc: t('settingTTS.ttsService.desc'),
        label: t('settingTTS.ttsService.title'),
        name: [TTS_SETTING_KEY, 'ttsService'],
      },
      {
        children: <Switch />,
        desc: t('settingTTS.showAllLocaleVoice.desc'),
        hidden: ttsService === 'openai',
        label: t('settingTTS.showAllLocaleVoice.title'),
        minWidth: undefined,
        name: [TTS_SETTING_KEY, 'showAllLocaleVoice'],
        valuePropName: 'checked',
      },
      {
        children: <SelectWithTTSPreview options={openaiVoiceOptions} server={'openai'} />,
        desc: t('settingTTS.voice.desc'),
        hidden: ttsService !== 'openai',
        label: t('settingTTS.voice.title'),
        name: [TTS_SETTING_KEY, 'voice', 'openai'],
      },
      {
        children: <SelectWithTTSPreview options={edgeVoiceOptions} server={'edge'} />,
        desc: t('settingTTS.voice.desc'),
        divider: false,
        hidden: ttsService !== 'edge',
        label: t('settingTTS.voice.title'),
        name: [TTS_SETTING_KEY, 'voice', 'edge'],
      },
      {
        children: <SelectWithTTSPreview options={microsoftVoiceOptions} server={'microsoft'} />,
        desc: t('settingTTS.voice.desc'),
        divider: false,
        hidden: ttsService !== 'microsoft',
        label: t('settingTTS.voice.title'),
        name: [TTS_SETTING_KEY, 'voice', 'microsoft'],
      },
      {
        children: (
          <Select
            options={[
              { label: t('settingTheme.lang.autoMode'), value: 'auto' },
              ...(localeOptions || []),
            ]}
          />
        ),
        desc: t('settingTTS.sttLocale.desc'),
        label: t('settingTTS.sttLocale.title'),
        name: [TTS_SETTING_KEY, 'sttLocale'],
      },
    ],
    icon: Mic,
    title: t('settingTTS.title'),
  };

  return (
    <Form
      form={form}
      initialValues={{
        [TTS_SETTING_KEY]: {
          voice: {
            edge: edgeVoiceOptions?.[0].value,
            microsoft: microsoftVoiceOptions?.[0].value,
            openai: openaiVoiceOptions?.[0].value,
          },
        },
      }}
      items={[tts]}
      itemsType={'group'}
      onValuesChange={debounce(updateConfig, 100)}
      variant={'pure'}
      {...FORM_STYLE}
    />
  );
});

export default AgentTTS;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/AgentSetting/AgentTTS/options.tsx
================================================================================

import { Azure, OpenAI } from '@lobehub/icons';
import { SelectProps } from 'antd';

import { LabelRenderer } from '@/components/ModelSelect';

export const ttsOptions: SelectProps['options'] = [
  {
    label: <LabelRenderer Icon={OpenAI.Avatar} label={'OpenAI'} />,
    value: 'openai',
  },
  {
    label: <LabelRenderer Icon={Azure.Avatar} label={'Edge Speech'} />,
    value: 'edge',
  },
  {
    label: <LabelRenderer Icon={Azure.Avatar} label={'Microsoft Speech'} />,
    value: 'microsoft',
  },
];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/AgentSetting/store/action.ts
================================================================================

import { DeepPartial } from 'utility-types';
import { StateCreator } from 'zustand/vanilla';

import { chainPickEmoji } from '@/chains/pickEmoji';
import { chainSummaryAgentName } from '@/chains/summaryAgentName';
import { chainSummaryDescription } from '@/chains/summaryDescription';
import { chainSummaryTags } from '@/chains/summaryTags';
import { TraceNameMap, TracePayload, TraceTopicType } from '@/const/trace';
import { chatService } from '@/services/chat';
import { useUserStore } from '@/store/user';
import { systemAgentSelectors } from '@/store/user/slices/settings/selectors';
import { LobeAgentChatConfig, LobeAgentConfig } from '@/types/agent';
import { MetaData } from '@/types/meta';
import { SystemAgentItem } from '@/types/user/settings';
import { MessageTextChunk } from '@/utils/fetch';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

import { SessionLoadingState } from '../store/initialState';
import { State, initialState } from './initialState';
import { ConfigDispatch, configReducer } from './reducers/config';
import { MetaDataDispatch, metaDataReducer } from './reducers/meta';

export interface PublicAction {
  /**
   * 自动选择表情
   * @param id - 表情的 ID
   */
  autoPickEmoji: () => Promise<void>;
  /**
   * 自动完成代理描述
   * @param id - 代理的 ID
   * @returns 一个 Promise，用于异步操作完成后的处理
   */
  autocompleteAgentDescription: () => Promise<void>;
  autocompleteAgentTags: () => Promise<void>;
  /**
   * 自动完成代理标题
   * @param id - 代理的 ID
   * @returns 一个 Promise，用于异步操作完成后的处理
   */
  autocompleteAgentTitle: () => Promise<void>;
  /**
   * 自动完成助理元数据
   */
  autocompleteAllMeta: (replace?: boolean) => void;
  autocompleteMeta: (key: keyof MetaData) => void;
}

export interface Action extends PublicAction {
  dispatchConfig: (payload: ConfigDispatch) => void;
  dispatchMeta: (payload: MetaDataDispatch) => void;
  getCurrentTracePayload: (data: Partial<TracePayload>) => TracePayload;

  internal_getSystemAgentForMeta: () => SystemAgentItem;
  resetAgentConfig: () => void;

  resetAgentMeta: () => void;
  setAgentConfig: (config: DeepPartial<LobeAgentConfig>) => void;
  setAgentMeta: (meta: Partial<MetaData>) => void;

  setChatConfig: (config: Partial<LobeAgentChatConfig>) => void;
  streamUpdateMetaArray: (key: keyof MetaData) => any;
  streamUpdateMetaString: (key: keyof MetaData) => any;
  toggleAgentPlugin: (pluginId: string, state?: boolean) => void;

  /**
   * 更新加载状态
   * @param key - SessionLoadingState 的键
   * @param value - 加载状态的值
   */
  updateLoadingState: (key: keyof SessionLoadingState, value: boolean) => void;
}

export type Store = Action & State;

const t = setNamespace('AgentSettings');

export const store: StateCreator<Store, [['zustand/devtools', never]]> = (set, get) => ({
  ...initialState,
  autoPickEmoji: async () => {
    const { config, meta, dispatchMeta } = get();

    const systemRole = config.systemRole;

    chatService.fetchPresetTaskResult({
      onFinish: async (emoji) => {
        dispatchMeta({ type: 'update', value: { avatar: emoji } });
      },
      onLoadingChange: (loading) => {
        get().updateLoadingState('avatar', loading);
      },
      params: merge(
        get().internal_getSystemAgentForMeta(),
        chainPickEmoji([meta.title, meta.description, systemRole].filter(Boolean).join(',')),
      ),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.EmojiPicker }),
    });
  },
  autocompleteAgentDescription: async () => {
    const { dispatchMeta, config, meta, updateLoadingState, streamUpdateMetaString } = get();

    const systemRole = config.systemRole;

    if (!systemRole) return;

    const preValue = meta.description;

    // 替换为 ...
    dispatchMeta({ type: 'update', value: { description: '...' } });

    chatService.fetchPresetTaskResult({
      onError: () => {
        dispatchMeta({ type: 'update', value: { description: preValue } });
      },
      onLoadingChange: (loading) => {
        updateLoadingState('description', loading);
      },
      onMessageHandle: streamUpdateMetaString('description'),
      params: merge(get().internal_getSystemAgentForMeta(), chainSummaryDescription(systemRole)),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.SummaryAgentDescription }),
    });
  },
  autocompleteAgentTags: async () => {
    const { dispatchMeta, config, meta, updateLoadingState, streamUpdateMetaArray } = get();

    const systemRole = config.systemRole;

    if (!systemRole) return;

    const preValue = meta.tags;

    // 替换为 ...
    dispatchMeta({ type: 'update', value: { tags: ['...'] } });

    // Get current agent for agentMeta
    chatService.fetchPresetTaskResult({
      onError: () => {
        dispatchMeta({ type: 'update', value: { tags: preValue } });
      },
      onLoadingChange: (loading) => {
        updateLoadingState('tags', loading);
      },
      onMessageHandle: streamUpdateMetaArray('tags'),
      params: merge(
        get().internal_getSystemAgentForMeta(),
        chainSummaryTags([meta.title, meta.description, systemRole].filter(Boolean).join(',')),
      ),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.SummaryAgentTags }),
    });
  },
  autocompleteAgentTitle: async () => {
    const { dispatchMeta, config, meta, updateLoadingState, streamUpdateMetaString } = get();

    const systemRole = config.systemRole;

    if (!systemRole) return;

    const previousTitle = meta.title;

    // 替换为 ...
    dispatchMeta({ type: 'update', value: { title: '...' } });

    chatService.fetchPresetTaskResult({
      onError: () => {
        dispatchMeta({ type: 'update', value: { title: previousTitle } });
      },
      onLoadingChange: (loading) => {
        updateLoadingState('title', loading);
      },
      onMessageHandle: streamUpdateMetaString('title'),
      params: merge(
        get().internal_getSystemAgentForMeta(),
        chainSummaryAgentName([meta.description, systemRole].filter(Boolean).join(',')),
      ),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.SummaryAgentTitle }),
    });
  },
  autocompleteAllMeta: (replace) => {
    const { meta } = get();

    if (!meta.title || replace) {
      get().autocompleteAgentTitle();
    }

    if (!meta.description || replace) {
      get().autocompleteAgentDescription();
    }

    if (!meta.avatar || replace) {
      get().autoPickEmoji();
    }

    if (!meta.tags || replace) {
      get().autocompleteAgentTags();
    }
  },
  autocompleteMeta: (key) => {
    const {
      autoPickEmoji,
      autocompleteAgentTitle,
      autocompleteAgentDescription,
      autocompleteAgentTags,
    } = get();

    switch (key) {
      case 'avatar': {
        autoPickEmoji();
        return;
      }

      case 'description': {
        autocompleteAgentDescription();
        return;
      }

      case 'title': {
        autocompleteAgentTitle();
        return;
      }

      case 'tags': {
        autocompleteAgentTags();
        return;
      }
    }
  },
  dispatchConfig: (payload) => {
    const nextConfig = configReducer(get().config, payload);

    set({ config: nextConfig }, false, payload);

    get().onConfigChange?.(nextConfig);
  },
  dispatchMeta: (payload) => {
    const nextValue = metaDataReducer(get().meta, payload);

    set({ meta: nextValue }, false, payload);

    get().onMetaChange?.(nextValue);
  },
  getCurrentTracePayload: (data) => ({
    sessionId: get().id,
    topicId: TraceTopicType.AgentSettings,
    ...data,
  }),

  internal_getSystemAgentForMeta: () => {
    return systemAgentSelectors.agentMeta(useUserStore.getState());
  },

  resetAgentConfig: () => {
    get().dispatchConfig({ type: 'reset' });
  },

  resetAgentMeta: () => {
    get().dispatchMeta({ type: 'reset' });
  },
  setAgentConfig: (config) => {
    get().dispatchConfig({ config, type: 'update' });
  },
  setAgentMeta: (meta) => {
    get().dispatchMeta({ type: 'update', value: meta });
  },

  setChatConfig: (config) => {
    get().setAgentConfig({ chatConfig: config });
  },

  streamUpdateMetaArray: (key: keyof MetaData) => {
    let value = '';
    return (chunk: MessageTextChunk) => {
      switch (chunk.type) {
        case 'text': {
          value += chunk.text;
          get().dispatchMeta({ type: 'update', value: { [key]: value.split(',') } });
        }
      }
    };
  },

  streamUpdateMetaString: (key: keyof MetaData) => {
    let value = '';
    return (chunk: MessageTextChunk) => {
      switch (chunk.type) {
        case 'text': {
          value += chunk.text;
          get().dispatchMeta({ type: 'update', value: { [key]: value } });
        }
      }
    };
  },

  toggleAgentPlugin: (id, state) => {
    get().dispatchConfig({ pluginId: id, state, type: 'togglePlugin' });
  },

  updateLoadingState: (key, value) => {
    set(
      { autocompleteLoading: { ...get().autocompleteLoading, [key]: value } },
      false,
      t('updateLoadingState', { key, value }),
    );
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileViewer/Renderer/PDF/index.tsx
================================================================================

'use client';

import type { PDFDocumentProxy } from 'pdfjs-dist';
import { Fragment, memo, useCallback, useState } from 'react';
import { Flexbox } from 'react-layout-kit';
import { Document, Page, pdfjs } from 'react-pdf';
import 'react-pdf/dist/esm/Page/AnnotationLayer.css';
import 'react-pdf/dist/esm/Page/TextLayer.css';

import { lambdaQuery } from '@/libs/trpc/client';

import HighlightLayer from './HighlightLayer';
import { useStyles } from './style';
import useResizeObserver from './useResizeObserver';

// 如果海外的地址： https://unpkg.com/pdfjs-dist@${pdfjs.version}/build/pdf.worker.min.mjs
pdfjs.GlobalWorkerOptions.workerSrc = `https://registry.npmmirror.com/pdfjs-dist/${pdfjs.version}/files/build/pdf.worker.min.mjs`;

const options = {
  cMapUrl: '/cmaps/',
  standardFontDataUrl: '/standard_fonts/',
};

const maxWidth = 1200;

interface PDFViewerProps {
  fileId: string;
  url: string | null;
}

const PDFViewer = memo<PDFViewerProps>(({ url, fileId }) => {
  const { styles } = useStyles();
  const [numPages, setNumPages] = useState<number>(0);
  const [containerRef, setContainerRef] = useState<HTMLElement | null>(null);
  const [containerWidth, setContainerWidth] = useState<number>();
  const [isLoaded, setIsLoaded] = useState(false);

  // eslint-disable-next-line no-undef
  const onResize = useCallback<ResizeObserverCallback>((entries) => {
    const [entry] = entries;

    if (entry) {
      setContainerWidth(entry.contentRect.width);
    }
  }, []);

  useResizeObserver(containerRef, onResize);

  const onDocumentLoadSuccess = ({ numPages: nextNumPages }: PDFDocumentProxy) => {
    setNumPages(nextNumPages);
    setIsLoaded(true);
  };

  const { data } = lambdaQuery.chunk.getChunksByFileId.useInfiniteQuery(
    { id: fileId },
    { getNextPageParam: (lastPage) => lastPage.nextCursor },
  );

  const dataSource = data?.pages.flatMap((page) => page.items) || [];

  return (
    <Flexbox className={styles.container}>
      <Flexbox
        align={'center'}
        className={styles.documentContainer}
        padding={24}
        ref={setContainerRef}
        style={{ height: isLoaded ? undefined : '100%' }}
      >
        <Document
          className={styles.document}
          file={url}
          onLoadSuccess={onDocumentLoadSuccess}
          options={options}
        >
          {Array.from({ length: numPages }, (el, index) => {
            const width = containerWidth ? Math.min(containerWidth, maxWidth) : maxWidth;

            return (
              <Fragment key={`page_${index + 1}`}>
                <Page className={styles.page} pageNumber={index + 1} width={width}>
                  <HighlightLayer dataSource={dataSource} pageNumber={index + 1} width={width} />
                </Page>
              </Fragment>
            );
          })}
        </Document>
      </Flexbox>
    </Flexbox>
  );
});

export default PDFViewer;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileViewer/Renderer/PDF/HighlightLayer.tsx
================================================================================

import { FC, memo } from 'react';

import { useFileStore } from '@/store/file';
import { ChunkMetadata, Coordinates, FileChunk } from '@/types/chunk';

interface HighlightRectProps {
  coordinates: Coordinates;
  highlight: boolean;
}

const HighlightRect: FC<HighlightRectProps> = ({ coordinates, highlight }) => {
  const { points } = coordinates;

  // 假设points数组包含矩形的四个顶点坐标
  const [topLeft, topRight, bottomRight, bottomLeft] = points;

  // 计算矩形的属性
  const minX = Math.min(topLeft[0], topRight[0], bottomRight[0], bottomLeft[0]);
  const minY = Math.min(topLeft[1], topRight[1], bottomRight[1], bottomLeft[1]);
  const width = Math.max(topLeft[0], topRight[0], bottomRight[0], bottomLeft[0]) - minX;
  const height = Math.max(topLeft[1], topRight[1], bottomRight[1], bottomLeft[1]) - minY;

  return (
    <rect
      fill={highlight ? 'rgba(255, 255, 0, 0.5)' : 'rgba(255, 255, 0, 0.3)'}
      height={height}
      stroke="rgba(255, 255, 0, 0.7)"
      strokeWidth="1"
      width={width}
      x={minX}
      y={minY}
    />
  );
};

interface HighlightLayerProps {
  dataSource: FileChunk[];
  pageNumber: number;
  width: number;
}

const HighlightLayer = memo<HighlightLayerProps>(({ dataSource, pageNumber, width }) => {
  const chunks = dataSource
    .filter((chunk) => chunk.pageNumber && chunk.pageNumber === pageNumber)
    .filter(Boolean);
  const highlightChunkIds = useFileStore((s) => s.highlightChunkIds);

  const isExist = chunks.length > 0;

  if (!isExist) return null;

  const metadata = chunks[0].metadata as ChunkMetadata;
  if (!metadata.coordinates) return;

  const { layout_width, layout_height } = metadata.coordinates;

  const height = metadata.coordinates.layout_height * (width / metadata.coordinates.layout_width);

  return (
    <svg
      height={height}
      style={{ left: 0, position: 'absolute', top: 0, zIndex: 100 }}
      viewBox={`0 0 ${layout_width} ${layout_height}`}
      width={width}
    >
      {chunks.map(
        (chunk, index) =>
          chunk.metadata && (
            <HighlightRect
              coordinates={chunk.metadata.coordinates}
              highlight={highlightChunkIds.includes(chunk.id)}
              key={index}
            />
          ),
      )}
      s
    </svg>
  );
});

export default HighlightLayer;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/PluginStore/Loading.tsx
================================================================================

import { Skeleton } from 'antd';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const Loading = memo(() => {
  return (
    <Flexbox>
      <Skeleton paragraph={{ rows: 8 }} />
    </Flexbox>
  );
});

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/PluginStore/PluginItem/index.tsx
================================================================================

import { Avatar, Tooltip } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import Link from 'next/link';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import PluginTag from '@/features/PluginStore/PluginItem/PluginTag';
import { InstallPluginMeta } from '@/types/tool/plugin';

import Actions from './Action';

const { Paragraph } = Typography;

const useStyles = createStyles(({ css, token }) => ({
  desc: css`
    margin: 0 !important;
    font-size: 12px;
    line-height: 1;
    color: ${token.colorTextDescription};
  `,
  link: css`
    overflow: hidden;
    color: ${token.colorText};
  `,
  title: css`
    margin: 0 !important;
    font-size: 14px;
    font-weight: bold;
    line-height: 1;
  `,
}));

const PluginItem = memo<InstallPluginMeta>(({ identifier, homepage, author, type, meta = {} }) => {
  const { styles } = useStyles();

  return (
    <Flexbox
      align={'center'}
      gap={8}
      horizontal
      justify={'space-between'}
      paddingBlock={12}
      paddingInline={16}
      style={{ position: 'relative' }}
    >
      <Flexbox
        align={'center'}
        flex={1}
        gap={8}
        horizontal
        style={{ overflow: 'hidden', position: 'relative' }}
      >
        <Avatar avatar={meta.avatar} style={{ flex: 'none', overflow: 'hidden' }} />
        <Flexbox flex={1} gap={4} style={{ overflow: 'hidden', position: 'relative' }}>
          <Flexbox align={'center'} gap={8} horizontal>
            <Tooltip title={identifier}>
              {homepage ? (
                <Link className={styles.link} href={homepage} target={'_blank'}>
                  <Paragraph className={styles.title} ellipsis={{ rows: 1 }}>
                    {meta.title}
                  </Paragraph>
                </Link>
              ) : (
                <Paragraph className={styles.title} ellipsis={{ rows: 1 }}>
                  {meta.title}
                </Paragraph>
              )}
            </Tooltip>
            <PluginTag author={author} type={type} />
          </Flexbox>
          <Paragraph className={styles.desc} ellipsis={{ rows: 1 }}>
            {meta.description}
          </Paragraph>
        </Flexbox>
      </Flexbox>
      <Actions identifier={identifier} type={type} />
    </Flexbox>
  );
});

export default PluginItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/KnowledgeBaseModal/AssignKnowledgeBase/Loading.tsx
================================================================================

import { Skeleton } from 'antd';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const Loading = memo(() => {
  return (
    <Flexbox>
      <Skeleton paragraph={{ rows: 8 }} />
    </Flexbox>
  );
});

export default Loading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/KnowledgeBaseModal/AssignKnowledgeBase/Item/index.tsx
================================================================================

import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import KnowledgeIcon from '@/components/KnowledgeIcon';
import { KnowledgeItem } from '@/types/knowledgeBase';

import Actions from './Action';

const { Paragraph } = Typography;

const useStyles = createStyles(({ css, token }) => ({
  desc: css`
    margin: 0 !important;
    font-size: 12px;
    line-height: 1;
    color: ${token.colorTextDescription};
  `,
  link: css`
    overflow: hidden;
    color: ${token.colorText};
  `,
  title: css`
    margin: 0 !important;
    font-size: 14px;
    line-height: 1;
  `,
}));

const PluginItem = memo<KnowledgeItem>(({ id, fileType, name, type, description, enabled }) => {
  const { styles } = useStyles();

  return (
    <Flexbox
      align={'center'}
      gap={8}
      horizontal
      justify={'space-between'}
      paddingBlock={12}
      paddingInline={16}
      style={{ position: 'relative' }}
    >
      <Flexbox
        align={'center'}
        flex={1}
        gap={8}
        horizontal
        style={{ overflow: 'hidden', position: 'relative' }}
      >
        <KnowledgeIcon fileType={fileType} name={name} size={{ file: 40, repo: 40 }} type={type} />
        <Flexbox flex={1} gap={4} style={{ overflow: 'hidden', position: 'relative' }}>
          <Flexbox align={'center'} gap={8} horizontal>
            <Paragraph className={styles.title} ellipsis={{ rows: 1 }}>
              {name}
            </Paragraph>
          </Flexbox>
          {description && (
            <Paragraph className={styles.desc} ellipsis={{ rows: 1 }}>
              {description}
            </Paragraph>
          )}
        </Flexbox>
      </Flexbox>
      <Actions enabled={enabled} id={id} type={type} />
    </Flexbox>
  );
});

export default PluginItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/index.tsx
================================================================================

'use client';

import { Typography } from 'antd';
import dynamic from 'next/dynamic';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import FileList from './FileList';
import Header from './Header';
import UploadDock from './UploadDock';

const ChunkDrawer = dynamic(() => import('./ChunkDrawer'), { ssr: false });

interface FileManagerProps {
  category?: string;
  knowledgeBaseId?: string;
  title: string;
}
const FileManager = memo<FileManagerProps>(({ title, knowledgeBaseId, category }) => {
  return (
    <>
      <Header knowledgeBaseId={knowledgeBaseId} />
      <Flexbox gap={12} height={'100%'}>
        <Typography.Text
          style={{ fontSize: 16, fontWeight: 'bold', marginBlock: 16, marginInline: 24 }}
        >
          {title}
        </Typography.Text>

        <FileList category={category} knowledgeBaseId={knowledgeBaseId} />
      </Flexbox>
      <UploadDock />
      <ChunkDrawer />
    </>
  );
});

export default FileManager;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/index.tsx
================================================================================

import { Drawer } from 'antd';
import { useTheme } from 'antd-style';
import dynamic from 'next/dynamic';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { fileManagerSelectors, useFileStore } from '@/store/file';

import Content from './Content';

const FileViewer = dynamic(() => import('@/features/FileViewer'), { ssr: false });

const ChunkDrawer = memo(() => {
  const [fileId, open, closeChunkDrawer] = useFileStore((s) => [
    s.chunkDetailId,
    !!s.chunkDetailId,
    s.closeChunkDrawer,
  ]);
  const file = useFileStore(fileManagerSelectors.getFileById(fileId));

  const theme = useTheme();
  return (
    <Drawer
      onClose={() => {
        closeChunkDrawer();
      }}
      open={open}
      styles={{
        body: { padding: 0 },
      }}
      title={file?.name}
      width={'90%'}
    >
      <Flexbox height={'100%'} horizontal style={{ overflow: 'hidden' }}>
        {file && (
          <Flexbox flex={2} style={{ overflow: 'scroll' }}>
            <FileViewer {...file} />
          </Flexbox>
        )}
        <Flexbox flex={1} style={{ borderInlineStart: `1px solid ${theme.colorSplit}` }}>
          <Content />
        </Flexbox>
      </Flexbox>
    </Drawer>
  );
});

export default ChunkDrawer;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/Content.tsx
================================================================================

import { SearchBar } from '@lobehub/ui';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { useFileStore } from '@/store/file';
import { fileChunkSelectors } from '@/store/file/slices/chunk';

import ChunkList from './ChunkList';
import SimilaritySearchList from './SimilaritySearchList';

const Content = memo(() => {
  const [fileId, showSimilaritySearch, semanticSearch] = useFileStore((s) => [
    fileChunkSelectors.enabledChunkFileId(s),
    fileChunkSelectors.showSimilaritySearchResult(s),
    s.semanticSearch,
  ]);

  if (!fileId) return;

  return (
    <Flexbox gap={8} height={'100%'} paddingBlock={'16px 0'}>
      <Flexbox paddingInline={12}>
        <SearchBar
          onChange={(text) => {
            if (!text) useFileStore.setState({ isSimilaritySearch: false });
          }}
          onSearch={async (text) => {
            useFileStore.setState({ isSimilaritySearch: !!text });
            semanticSearch(text, fileId);
          }}
        />
      </Flexbox>
      {showSimilaritySearch ? <SimilaritySearchList /> : <ChunkList fileId={fileId} />}
    </Flexbox>
  );
});

export default Content;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/Loading/index.tsx
================================================================================

import { Skeleton } from 'antd';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const SkeletonLoading = memo(() => (
  <Flexbox padding={12}>
    <Skeleton active paragraph={{ width: '70%' }} title={false} />
    <Skeleton active paragraph={{ width: '40%' }} title={false} />
    <Skeleton active paragraph={{ width: '80%' }} title={false} />
    <Skeleton active paragraph={{ width: '30%' }} title={false} />
    <Skeleton active paragraph={{ width: '50%' }} title={false} />
    <Skeleton active paragraph={{ width: '70%' }} title={false} />
  </Flexbox>
));

export default SkeletonLoading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/ChunkList/index.tsx
================================================================================

import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';
import { Virtuoso } from 'react-virtuoso';

import { lambdaQuery } from '@/libs/trpc/client';

import SkeletonLoading from '../Loading';
import ChunkItem from './ChunkItem';

interface ChunkListProps {
  fileId: string;
}
const ChunkList = memo<ChunkListProps>(({ fileId }) => {
  const { data, isLoading, fetchNextPage } = lambdaQuery.chunk.getChunksByFileId.useInfiniteQuery(
    { id: fileId },
    {
      getNextPageParam: (lastPage) => lastPage.nextCursor,
    },
  );

  const dataSource = data?.pages.flatMap((page) => page.items) || [];

  return isLoading ? (
    <SkeletonLoading />
  ) : (
    <Flexbox flex={1}>
      <Virtuoso
        data={dataSource}
        endReached={() => {
          fetchNextPage();
        }}
        itemContent={(index, item) => (
          <Flexbox key={item.id} paddingInline={12}>
            <ChunkItem {...item} index={index} />
          </Flexbox>
        )}
      />
    </Flexbox>
  );
});

export default ChunkList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/ChunkList/ChunkItem.tsx
================================================================================

import { createStyles } from 'antd-style';
import { memo, useMemo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { useFileStore } from '@/store/file';
import { FileChunk } from '@/types/chunk';

const useStyles = createStyles(({ css, token }) => ({
  container: css`
    padding-block: 12px;
    padding-inline: 8px;
    border-block-end: 1px dashed ${token.colorBorderSecondary};
    border-radius: 4px;

    &:hover {
      background: ${token.colorFillTertiary};
    }
  `,
  text: css`
    font-size: 14px;
    line-height: 24px;
  `,
  title: css`
    font-size: 18px;
  `,
}));

type ChunkItemProps = FileChunk;

const ChunkItem = memo<ChunkItemProps>(({ text, type, id }) => {
  const { styles, cx } = useStyles();

  const highlightChunks = useFileStore((s) => s.highlightChunks);

  const typeClassName = useMemo(() => {
    switch (type) {
      default: {
        return styles.text;
      }
      case 'Title': {
        return styles.title;
      }
    }
  }, [type]);

  return (
    <Flexbox
      className={cx(styles.container, typeClassName)}
      onMouseEnter={() => {
        highlightChunks([id]);
      }}
      onMouseLeave={() => {
        highlightChunks([]);
      }}
    >
      {text}
    </Flexbox>
  );
});

export default ChunkItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/SimilaritySearchList/index.tsx
================================================================================

import isEqual from 'fast-deep-equal';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';
import { Virtuoso } from 'react-virtuoso';

import { useFileStore } from '@/store/file';

import SkeletonLoading from '../Loading';
import ChunkItem from './Item';

const SimilaritySearchList = memo(() => {
  const isSimilaritySearching = useFileStore((s) => s.isSimilaritySearching);
  const dataSource = useFileStore((s) => s.similaritySearchChunks, isEqual);

  return isSimilaritySearching ? (
    <SkeletonLoading />
  ) : (
    <Flexbox flex={1}>
      <Virtuoso
        data={dataSource}
        itemContent={(index, item) => (
          <Flexbox key={item.id} paddingInline={12}>
            <ChunkItem {...item} index={index} />
          </Flexbox>
        )}
      />
    </Flexbox>
  );
});

export default SimilaritySearchList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/ChunkDrawer/SimilaritySearchList/Item.tsx
================================================================================

import { Tag } from 'antd';
import { createStyles } from 'antd-style';
import { memo, useMemo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { SemanticSearchChunk } from '@/types/chunk';

const useStyles = createStyles(({ css, token }) => ({
  container: css`
    padding-block: 12px;
    padding-inline: 8px;
    border-block-end: 1px solid ${token.colorBorderSecondary};
    border-radius: 4px;

    &:hover {
      background: ${token.colorFillTertiary};
    }
  `,
  pageNumber: css`
    font-size: 12px;
    color: ${token.colorTextDescription};
  `,
  text: css`
    font-size: 14px;
    line-height: 24px;
  `,
  title: css`
    font-size: 18px;
  `,
}));

interface ChunkItemProps extends Omit<SemanticSearchChunk, 'index'> {
  index: number;
}

const SearchItem = memo<ChunkItemProps>(({ text, pageNumber, type, similarity }) => {
  const { styles, cx } = useStyles();

  const typeClassName = useMemo(() => {
    switch (type) {
      default: {
        return styles.text;
      }
      case 'Title': {
        return styles.title;
      }
    }
  }, [type]);

  return (
    <Flexbox className={cx(styles.container, typeClassName)} gap={8}>
      {text}

      <Flexbox align={'center'} distribution={'space-between'} horizontal>
        <Tag bordered={false}>{similarity.toFixed(2)}</Tag>
        <Flexbox className={styles.pageNumber}>第 {pageNumber} 页</Flexbox>
      </Flexbox>
    </Flexbox>
  );
});

export default SearchItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/FileList/useCheckTaskStatus.ts
================================================================================

import { useEffect } from 'react';

import { useFileStore } from '@/store/file';
import { AsyncTaskStatus } from '@/types/asyncTask';
import { FileListItem } from '@/types/files';

export const useCheckTaskStatus = (data: FileListItem[] | undefined) => {
  const [refreshFileList] = useFileStore((s) => [s.refreshFileList]);
  const hasProcessingChunkTask = data?.some(
    (item) => item.chunkingStatus === AsyncTaskStatus.Processing,
  );
  const hasProcessingEmbeddingTask = data?.some(
    (item) => item.embeddingStatus === AsyncTaskStatus.Processing,
  );

  const isProcessing = hasProcessingChunkTask || hasProcessingEmbeddingTask;

  // every 3s to check if the chunking status is changed
  useEffect(() => {
    if (!isProcessing) return;

    const interval = setInterval(refreshFileList, 5000);
    return () => {
      clearInterval(interval);
    };
  }, [isProcessing]);
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/FileList/FileListItem/index.tsx
================================================================================

import { Icon, Tooltip } from '@lobehub/ui';
import { Button, Checkbox } from 'antd';
import { createStyles } from 'antd-style';
import dayjs from 'dayjs';
import relativeTime from 'dayjs/plugin/relativeTime';
import { isNull } from 'lodash-es';
import { FileBoxIcon } from 'lucide-react';
import { useRouter } from 'next/navigation';
import { rgba } from 'polished';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import FileIcon from '@/components/FileIcon';
import { fileManagerSelectors, useFileStore } from '@/store/file';
import { FileListItem } from '@/types/files';
import { formatSize } from '@/utils/format';
import { isChunkingUnsupported } from '@/utils/isChunkingUnsupported';

import ChunksBadge from './ChunkTag';
import DropdownMenu from './DropdownMenu';

dayjs.extend(relativeTime);

export const FILE_DATE_WIDTH = 160;
export const FILE_SIZE_WIDTH = 140;

const useStyles = createStyles(({ css, token, cx, isDarkMode }) => {
  const hover = css`
    opacity: 0;
  `;
  return {
    checkbox: hover,
    container: css`
      cursor: pointer;
      margin-inline: 24px;
      border-block-end: 1px solid ${isDarkMode ? token.colorSplit : rgba(token.colorSplit, 0.06)};

      &:hover {
        background: ${token.colorFillTertiary};

        .${cx(hover)} {
          opacity: 1;
        }
      }

      .chunk-tag {
        opacity: 1;
      }
    `,

    hover,
    item: css`
      padding-block: 0;
      padding-inline: 0 24px;
      color: ${token.colorTextSecondary};
    `,
    name: css`
      overflow: hidden;
      display: -webkit-box;
      -webkit-box-orient: vertical;
      -webkit-line-clamp: 1;

      margin-inline-start: 12px;

      color: ${token.colorText};
    `,
    selected: css`
      background: ${token.colorFillTertiary};

      &:hover {
        background: ${token.colorFillSecondary};
      }
    `,
  };
});

interface FileRenderItemProps extends FileListItem {
  index: number;
  knowledgeBaseId?: string;
  onSelectedChange: (id: string, selected: boolean) => void;
  selected?: boolean;
}

const FileRenderItem = memo<FileRenderItemProps>(
  ({
    size,
    chunkingError,
    embeddingError,
    embeddingStatus,
    finishEmbedding,
    chunkCount,
    url,
    name,
    fileType,
    id,
    createdAt,
    selected,
    chunkingStatus,
    onSelectedChange,
    knowledgeBaseId,
  }) => {
    const { t } = useTranslation('components');
    const { styles, cx } = useStyles();
    const router = useRouter();
    const [isCreatingFileParseTask, parseFiles] = useFileStore((s) => [
      fileManagerSelectors.isCreatingFileParseTask(id)(s),
      s.parseFilesToChunks,
    ]);

    const isSupportedForChunking = !isChunkingUnsupported(fileType);

    const displayTime =
      dayjs().diff(dayjs(createdAt), 'd') < 7
        ? dayjs(createdAt).fromNow()
        : dayjs(createdAt).format('YYYY-MM-DD');

    return (
      <Flexbox
        align={'center'}
        className={cx(styles.container, selected && styles.selected)}
        height={64}
        horizontal
      >
        <Flexbox
          align={'center'}
          className={styles.item}
          distribution={'space-between'}
          flex={1}
          horizontal
          onClick={() => {
            router.push(`/files/${id}`);
          }}
        >
          <Flexbox align={'center'} horizontal>
            <Center
              height={48}
              onClick={(e) => {
                e.stopPropagation();

                onSelectedChange(id, !selected);
              }}
              style={{ paddingInline: 4 }}
            >
              <Checkbox
                checked={selected}
                className={selected ? '' : styles.hover}
                style={{ borderRadius: '50%' }}
              />
            </Center>
            <FileIcon fileName={name} fileType={fileType} />
            <span className={styles.name}>{name}</span>
          </Flexbox>
          <Flexbox
            align={'center'}
            gap={8}
            horizontal
            onClick={(e) => {
              e.stopPropagation();
            }}
          >
            {isCreatingFileParseTask || isNull(chunkingStatus) || !chunkingStatus ? (
              <div className={isCreatingFileParseTask ? undefined : styles.hover}>
                <Tooltip
                  overlayStyle={{ pointerEvents: 'none' }}
                  title={t(
                    isSupportedForChunking
                      ? 'FileManager.actions.chunkingTooltip'
                      : 'FileManager.actions.chunkingUnsupported',
                  )}
                >
                  <Button
                    disabled={!isSupportedForChunking}
                    icon={<Icon icon={FileBoxIcon} />}
                    loading={isCreatingFileParseTask}
                    onClick={() => {
                      parseFiles([id]);
                    }}
                    size={'small'}
                    type={'text'}
                  >
                    {t(
                      isCreatingFileParseTask
                        ? 'FileManager.actions.createChunkingTask'
                        : 'FileManager.actions.chunking',
                    )}
                  </Button>
                </Tooltip>
              </div>
            ) : (
              <div style={{ cursor: 'default' }}>
                <ChunksBadge
                  chunkCount={chunkCount}
                  chunkingError={chunkingError}
                  chunkingStatus={chunkingStatus}
                  embeddingError={embeddingError}
                  embeddingStatus={embeddingStatus}
                  finishEmbedding={finishEmbedding}
                  id={id}
                />
              </div>
            )}
            <div className={styles.hover}>
              <DropdownMenu filename={name} id={id} knowledgeBaseId={knowledgeBaseId} url={url} />
            </div>
          </Flexbox>
        </Flexbox>
        <Flexbox className={styles.item} width={FILE_DATE_WIDTH}>
          {displayTime}
        </Flexbox>
        <Flexbox className={styles.item} width={FILE_SIZE_WIDTH}>
          {formatSize(size)}
        </Flexbox>
      </Flexbox>
    );
  },
);

export default FileRenderItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/FileList/FileListItem/ChunkTag.tsx
================================================================================

import { memo } from 'react';

import FileParsingStatusTag from '@/components/FileParsingStatus';
import { fileManagerSelectors, useFileStore } from '@/store/file';
import { FileParsingTask } from '@/types/asyncTask';

interface ChunkTagProps extends FileParsingTask {
  id: string;
}

const ChunksBadge = memo<ChunkTagProps>(({ id, ...res }) => {
  const [
    isCreatingChunkEmbeddingTask,
    embeddingChunks,
    reParseFile,
    openChunkDrawer,
    reEmbeddingChunks,
  ] = useFileStore((s) => [
    fileManagerSelectors.isCreatingChunkEmbeddingTask(id)(s),
    s.embeddingChunks,
    s.reParseFile,
    s.openChunkDrawer,
    s.reEmbeddingChunks,
  ]);

  return (
    <FileParsingStatusTag
      onClick={(status) => {
        if (status === 'success') openChunkDrawer(id);
      }}
      onEmbeddingClick={() => embeddingChunks([id])}
      onErrorClick={(task) => {
        if (task === 'chunking') reParseFile(id);
        if (task === 'embedding') reEmbeddingChunks(id);
      }}
      preparingEmbedding={isCreatingChunkEmbeddingTask}
      {...res}
    />
  );
});

export default ChunksBadge;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/FileList/ToolBar/index.tsx
================================================================================

import { createStyles } from 'antd-style';
import { rgba } from 'polished';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

import { useAddFilesToKnowledgeBaseModal } from '@/features/KnowledgeBaseModal';
import { useFileStore } from '@/store/file';
import { useKnowledgeBaseStore } from '@/store/knowledgeBase';
import { isChunkingUnsupported } from '@/utils/isChunkingUnsupported';

import Config from './Config';
import MultiSelectActions, { MultiSelectActionType } from './MultiSelectActions';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  container: css`
    height: 40px;
    padding-block-end: 12px;
    border-block-end: 1px solid ${isDarkMode ? token.colorSplit : rgba(token.colorSplit, 0.06)};
  `,
}));

interface MultiSelectActionsProps {
  config: { showFilesInKnowledgeBase: boolean };
  knowledgeBaseId?: string;
  onConfigChange: (config: { showFilesInKnowledgeBase: boolean }) => void;
  selectCount: number;
  selectFileIds: string[];
  setSelectedFileIds: (ids: string[]) => void;
  showConfig?: boolean;
  total?: number;
  totalFileIds: string[];
}

const ToolBar = memo<MultiSelectActionsProps>(
  ({
    selectCount,
    showConfig,
    setSelectedFileIds,
    selectFileIds,
    total,
    totalFileIds,
    config,
    onConfigChange,
    knowledgeBaseId,
  }) => {
    const { styles } = useStyles();

    const [removeFiles, parseFilesToChunks, fileList] = useFileStore((s) => [
      s.removeFiles,
      s.parseFilesToChunks,
      s.fileList,
    ]);
    const [removeFromKnowledgeBase] = useKnowledgeBaseStore((s) => [
      s.removeFilesFromKnowledgeBase,
    ]);

    const { open } = useAddFilesToKnowledgeBaseModal();

    const onActionClick = async (type: MultiSelectActionType) => {
      switch (type) {
        case 'delete': {
          await removeFiles(selectFileIds);
          setSelectedFileIds([]);

          return;
        }
        case 'removeFromKnowledgeBase': {
          if (!knowledgeBaseId) return;

          await removeFromKnowledgeBase(knowledgeBaseId, selectFileIds);
          setSelectedFileIds([]);
          return;
        }
        case 'addToKnowledgeBase': {
          open({
            fileIds: selectFileIds,
            onClose: () => setSelectedFileIds([]),
          });
          return;
        }
        case 'addToOtherKnowledgeBase': {
          open({
            fileIds: selectFileIds,
            knowledgeBaseId,
            onClose: () => setSelectedFileIds([]),
          });
          return;
        }

        case 'batchChunking': {
          const chunkableFileIds = selectFileIds.filter((id) => {
            const file = fileList.find((f) => f.id === id);
            return file && !isChunkingUnsupported(file.fileType);
          });
          await parseFilesToChunks(chunkableFileIds, { skipExist: true });
          setSelectedFileIds([]);
          return;
        }
      }
    };

    const isInKnowledgeBase = !!knowledgeBaseId;
    return (
      <Flexbox align={'center'} className={styles.container} horizontal justify={'space-between'}>
        <MultiSelectActions
          isInKnowledgeBase={isInKnowledgeBase}
          onActionClick={onActionClick}
          onClickCheckbox={() => {
            setSelectedFileIds(selectCount === total ? [] : totalFileIds);
          }}
          selectCount={selectCount}
          total={total}
        />
        {showConfig && <Config config={config} onConfigChange={onConfigChange} />}
      </Flexbox>
    );
  },
);

export default ToolBar;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/FileList/ToolBar/MultiSelectActions.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { App, Button, Checkbox, Skeleton } from 'antd';
import { createStyles } from 'antd-style';
import { BookMinusIcon, BookPlusIcon, FileBoxIcon, Trash2Icon } from 'lucide-react';
import { rgba } from 'polished';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  container: css`
    height: 40px;
    padding-block-end: 12px;
    border-block-end: 1px solid ${isDarkMode ? token.colorSplit : rgba(token.colorSplit, 0.06)};
  `,
  total: css`
    cursor: pointer;
    height: 27px;
  `,
}));

export type MultiSelectActionType =
  | 'addToKnowledgeBase'
  | 'delete'
  | 'batchChunking'
  | 'removeFromKnowledgeBase'
  | 'addToOtherKnowledgeBase';

interface MultiSelectActionsProps {
  isInKnowledgeBase?: boolean;
  onActionClick: (type: MultiSelectActionType) => Promise<void>;
  onClickCheckbox: () => void;
  selectCount: number;
  total?: number;
}

const MultiSelectActions = memo<MultiSelectActionsProps>(
  ({ selectCount, isInKnowledgeBase, total, onActionClick, onClickCheckbox }) => {
    const { t } = useTranslation(['components', 'common']);
    const { styles } = useStyles();

    const isSelectedFiles = selectCount > 0;
    const { modal, message } = App.useApp();
    return (
      <Flexbox align={'center'} gap={12} horizontal>
        <Flexbox
          align={'center'}
          className={styles.total}
          gap={8}
          horizontal
          onClick={onClickCheckbox}
          paddingInline={4}
        >
          <Checkbox
            checked={selectCount === total}
            indeterminate={isSelectedFiles && selectCount !== total}
          />
          {typeof total === 'undefined' ? (
            <Skeleton
              active
              paragraph={{ rows: 1, style: { marginBottom: 0, width: 60 }, width: '100%' }}
              title={false}
            />
          ) : (
            <div style={{ height: 18 }}>
              {isSelectedFiles
                ? t('FileManager.total.selectedCount', { count: selectCount })
                : t('FileManager.total.fileCount', { count: total })}
            </div>
          )}
        </Flexbox>
        {isSelectedFiles && (
          <Flexbox gap={8} horizontal>
            {isInKnowledgeBase ? (
              <>
                <Button
                  icon={<Icon icon={BookMinusIcon} />}
                  onClick={() => {
                    modal.confirm({
                      okButtonProps: {
                        danger: true,
                      },
                      onOk: async () => {
                        await onActionClick('removeFromKnowledgeBase');
                        message.success(t('FileManager.actions.removeFromKnowledgeBaseSuccess'));
                      },
                      title: t('FileManager.actions.confirmRemoveFromKnowledgeBase', {
                        count: selectCount,
                      }),
                    });
                  }}
                  size={'small'}
                >
                  {t('FileManager.actions.removeFromKnowledgeBase')}
                </Button>
                <Button
                  icon={<Icon icon={BookPlusIcon} />}
                  onClick={() => {
                    onActionClick('addToOtherKnowledgeBase');
                  }}
                  size={'small'}
                >
                  {t('FileManager.actions.addToOtherKnowledgeBase')}
                </Button>
              </>
            ) : (
              <Button
                icon={<Icon icon={BookPlusIcon} />}
                onClick={() => {
                  onActionClick('addToKnowledgeBase');
                }}
                size={'small'}
              >
                {t('FileManager.actions.addToKnowledgeBase')}
              </Button>
            )}
            <Button
              icon={<Icon icon={FileBoxIcon} />}
              onClick={async () => {
                await onActionClick('batchChunking');
              }}
              size={'small'}
            >
              {t('FileManager.actions.batchChunking')}
            </Button>
            <Button
              danger
              icon={<Icon icon={Trash2Icon} />}
              onClick={async () => {
                modal.confirm({
                  okButtonProps: {
                    danger: true,
                  },
                  onOk: async () => {
                    await onActionClick('delete');
                    message.success(t('FileManager.actions.deleteSuccess'));
                  },
                  title: t('FileManager.actions.confirmDeleteMultiFiles', { count: selectCount }),
                });
              }}
              size={'small'}
            >
              {t('batchDelete', { ns: 'common' })}
            </Button>
          </Flexbox>
        )}
      </Flexbox>
    );
  },
);

export default MultiSelectActions;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/FileManager/Header/UploadFileButton.tsx
================================================================================

'use client';

import { Icon } from '@lobehub/ui';
import { Button, Dropdown, MenuProps, Upload } from 'antd';
import { css, cx } from 'antd-style';
import { FileUp, FolderUp, PlusCircleIcon } from 'lucide-react';
import { useMemo } from 'react';
import { useTranslation } from 'react-i18next';

import DragUpload from '@/components/DragUpload';
import { useFileStore } from '@/store/file';

const hotArea = css`
  &::before {
    content: '';
    position: absolute;
    inset: 0;
    background-color: transparent;
  }
`;

const UploadFileButton = ({ knowledgeBaseId }: { knowledgeBaseId?: string }) => {
  const { t } = useTranslation('file');

  const pushDockFileList = useFileStore((s) => s.pushDockFileList);
  const items = useMemo<MenuProps['items']>(
    () => [
      {
        icon: <Icon icon={FileUp} />,
        key: 'upload-file',
        label: (
          <Upload
            beforeUpload={async (file) => {
              await pushDockFileList([file], knowledgeBaseId);

              return false;
            }}
            multiple={true}
            showUploadList={false}
          >
            <div className={cx(hotArea)}>{t('header.actions.uploadFile')}</div>
          </Upload>
        ),
      },
      {
        icon: <Icon icon={FolderUp} />,
        key: 'upload-folder',
        label: (
          <Upload
            beforeUpload={async (file) => {
              await pushDockFileList([file], knowledgeBaseId);

              return false;
            }}
            directory
            multiple={true}
            showUploadList={false}
          >
            <div className={cx(hotArea)}>{t('header.actions.uploadFolder')}</div>
          </Upload>
        ),
      },
    ],
    [],
  );
  return (
    <>
      <Dropdown menu={{ items }} placement="bottomRight">
        <Button icon={<Icon icon={PlusCircleIcon} />}>{t('header.uploadButton')}</Button>
      </Dropdown>
      <DragUpload
        enabledFiles
        onUploadFiles={(files) => pushDockFileList(files, knowledgeBaseId)}
      />
    </>
  );
};

export default UploadFileButton;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/ChatInput/Desktop/index.tsx
================================================================================

'use client';

import { DraggablePanel } from '@lobehub/ui';
import { ReactNode, memo, useCallback, useState } from 'react';
import { Flexbox } from 'react-layout-kit';

import { CHAT_TEXTAREA_HEIGHT, CHAT_TEXTAREA_MAX_HEIGHT } from '@/const/layoutTokens';

import { ActionKeys } from '../ActionBar/config';
import LocalFiles from './FilePreview';
import Head from './Header';

export type FooterRender = (params: {
  expand: boolean;
  onExpandChange: (expand: boolean) => void;
}) => ReactNode;

interface DesktopChatInputProps {
  inputHeight: number;
  leftActions: ActionKeys[];
  onInputHeightChange?: (height: number) => void;
  renderFooter: FooterRender;
  renderTextArea: (onSend: () => void) => ReactNode;
  rightActions: ActionKeys[];
}

const DesktopChatInput = memo<DesktopChatInputProps>(
  ({
    leftActions,
    rightActions,
    renderTextArea,
    inputHeight,
    onInputHeightChange,
    renderFooter,
  }) => {
    const [expand, setExpand] = useState<boolean>(false);

    const onSend = useCallback(() => {
      setExpand(false);
    }, []);

    return (
      <>
        {!expand && leftActions.includes('fileUpload') && <LocalFiles />}
        <DraggablePanel
          fullscreen={expand}
          maxHeight={CHAT_TEXTAREA_MAX_HEIGHT}
          minHeight={CHAT_TEXTAREA_HEIGHT}
          onSizeChange={(_, size) => {
            if (!size) return;
            const height =
              typeof size.height === 'string' ? Number.parseInt(size.height) : size.height;
            if (!height) return;

            onInputHeightChange?.(height);
          }}
          placement="bottom"
          size={{ height: inputHeight, width: '100%' }}
          style={{ zIndex: 10 }}
        >
          <Flexbox
            gap={8}
            height={'100%'}
            padding={'12px 0 16px'}
            style={{ minHeight: CHAT_TEXTAREA_HEIGHT, position: 'relative' }}
          >
            <Head
              expand={expand}
              leftActions={leftActions}
              rightActions={rightActions}
              setExpand={setExpand}
            />
            {renderTextArea(onSend)}
            {renderFooter({ expand, onExpandChange: setExpand })}
          </Flexbox>
        </DraggablePanel>
      </>
    );
  },
);

DesktopChatInput.displayName = 'DesktopChatInput';

export default DesktopChatInput;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/ChatInput/Desktop/FilePreview/index.tsx
================================================================================

import { memo } from 'react';

import DragUpload from '@/components/DragUpload';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/slices/chat';
import { useFileStore } from '@/store/file';
import { useUserStore } from '@/store/user';
import { modelProviderSelectors } from '@/store/user/selectors';

import FileItemList from './FileList';

const FilePreview = memo(() => {
  const model = useAgentStore(agentSelectors.currentAgentModel);

  const canUploadImage = useUserStore(modelProviderSelectors.isModelEnabledUpload(model));

  const [uploadFiles] = useFileStore((s) => [s.uploadChatFiles]);

  const upload = async (fileList: FileList | File[] | undefined) => {
    if (!fileList || fileList.length === 0) return;

    // Filter out files that are not images if the model does not support image uploads
    const files = Array.from(fileList).filter((file) => {
      if (canUploadImage) return true;

      return !file.type.startsWith('image');
    });

    uploadFiles(files);
  };

  return (
    <>
      <DragUpload onUploadFiles={upload} />
      <FileItemList />
    </>
  );
});

export default FilePreview;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/ChatInput/STT/openai.tsx
================================================================================

import { getRecordMineType } from '@lobehub/tts';
import { OpenAISTTOptions, useOpenAISTT } from '@lobehub/tts/react';
import isEqual from 'fast-deep-equal';
import { memo, useCallback, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { SWRConfiguration } from 'swr';

import { createHeaderWithOpenAI } from '@/services/_header';
import { API_ENDPOINTS } from '@/services/_url';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/selectors';
import { useChatStore } from '@/store/chat';
import { chatSelectors } from '@/store/chat/slices/message/selectors';
import { useUserStore } from '@/store/user';
import { settingsSelectors, userGeneralSettingsSelectors } from '@/store/user/selectors';
import { ChatMessageError } from '@/types/message';
import { getMessageError } from '@/utils/fetch';

import CommonSTT from './common';

interface STTConfig extends SWRConfiguration {
  onTextChange: (value: string) => void;
}

const useOpenaiSTT = (config: STTConfig) => {
  const ttsSettings = useUserStore(settingsSelectors.currentTTS, isEqual);
  const ttsAgentSettings = useAgentStore(agentSelectors.currentAgentTTS, isEqual);
  const locale = useUserStore(userGeneralSettingsSelectors.currentLanguage);

  const autoStop = ttsSettings.sttAutoStop;
  const sttLocale =
    ttsAgentSettings?.sttLocale && ttsAgentSettings.sttLocale !== 'auto'
      ? ttsAgentSettings.sttLocale
      : locale;

  return useOpenAISTT(sttLocale, {
    ...config,
    api: {
      headers: createHeaderWithOpenAI(),
      serviceUrl: API_ENDPOINTS.stt,
    },
    autoStop,
    options: {
      mineType: getRecordMineType(),
      model: ttsSettings.openAI.sttModel,
    },
  } as OpenAISTTOptions);
};

const OpenaiSTT = memo<{ mobile?: boolean }>(({ mobile }) => {
  const [error, setError] = useState<ChatMessageError>();
  const { t } = useTranslation('chat');

  const [loading, updateInputMessage] = useChatStore((s) => [
    chatSelectors.isAIGenerating(s),
    s.updateInputMessage,
  ]);

  const setDefaultError = useCallback(
    (err?: any) => {
      setError({ body: err, message: t('stt.responseError', { ns: 'error' }), type: 500 });
    },
    [t],
  );

  const { start, isLoading, stop, formattedTime, time, response, isRecording } = useOpenaiSTT({
    onError: (err) => {
      stop();
      setDefaultError(err);
    },
    onErrorRetry: (err) => {
      stop();
      setDefaultError(err);
    },
    onSuccess: async () => {
      if (!response) return;
      if (response.status === 200) return;
      const message = await getMessageError(response);
      if (message) {
        setError(message);
      } else {
        setDefaultError();
      }
      stop();
    },
    onTextChange: (text) => {
      if (loading) stop();
      if (text) updateInputMessage(text);
    },
  });

  const desc = t('stt.action');

  const handleTriggerStartStop = useCallback(() => {
    if (loading) return;
    if (!isLoading) {
      start();
    } else {
      stop();
    }
  }, [loading, isLoading, start, stop]);

  const handleCloseError = useCallback(() => {
    setError(undefined);
    stop();
  }, [stop]);

  const handleRetry = useCallback(() => {
    setError(undefined);
    start();
  }, [start]);

  return (
    <CommonSTT
      desc={desc}
      error={error}
      formattedTime={formattedTime}
      handleCloseError={handleCloseError}
      handleRetry={handleRetry}
      handleTriggerStartStop={handleTriggerStartStop}
      isLoading={isLoading}
      isRecording={isRecording}
      mobile={mobile}
      time={time}
    />
  );
});

export default OpenaiSTT;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/ChatInput/STT/index.tsx
================================================================================

import isEqual from 'fast-deep-equal';
import { memo } from 'react';

import { featureFlagsSelectors, useServerConfigStore } from '@/store/serverConfig';
import { useUserStore } from '@/store/user';
import { settingsSelectors } from '@/store/user/selectors';

import BrowserSTT from './browser';
import OpenaiSTT from './openai';

const STT = memo<{ mobile?: boolean }>(({ mobile }) => {
  const { sttServer } = useUserStore(settingsSelectors.currentTTS, isEqual);

  const { enableSTT } = useServerConfigStore(featureFlagsSelectors);
  if (!enableSTT) return;

  switch (sttServer) {
    case 'openai': {
      return <OpenaiSTT mobile={mobile} />;
    }
  }
  return <BrowserSTT mobile={mobile} />;
});

export default STT;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Messages/Assistant/index.tsx
================================================================================

import { Skeleton } from 'antd';
import { ReactNode, Suspense, memo, useContext } from 'react';
import { Flexbox } from 'react-layout-kit';

import { LOADING_FLAT } from '@/const/message';
import { InPortalThreadContext } from '@/features/Conversation/components/ChatItem/InPortalThreadContext';
import { useChatStore } from '@/store/chat';
import { chatSelectors } from '@/store/chat/selectors';
import { ChatMessage } from '@/types/message';

import { DefaultMessage } from '../Default';
import FileChunks from './FileChunks';
import ToolCall from './ToolCallItem';

export const AssistantMessage = memo<
  ChatMessage & {
    editableContent: ReactNode;
  }
>(({ id, tools, content, chunksList, ...props }) => {
  const editing = useChatStore(chatSelectors.isMessageEditing(id));
  const generating = useChatStore(chatSelectors.isMessageGenerating(id));

  const inThread = useContext(InPortalThreadContext);
  const isToolCallGenerating = generating && (content === LOADING_FLAT || !content) && !!tools;

  return editing ? (
    <DefaultMessage
      content={content}
      id={id}
      isToolCallGenerating={isToolCallGenerating}
      {...props}
    />
  ) : (
    <Flexbox gap={8} id={id}>
      {!!chunksList && chunksList.length > 0 && <FileChunks data={chunksList} />}
      {content && (
        <DefaultMessage
          addIdOnDOM={false}
          content={content}
          id={id}
          isToolCallGenerating={isToolCallGenerating}
          {...props}
        />
      )}
      {tools && (
        <Suspense
          fallback={<Skeleton.Button active style={{ height: 46, minWidth: 200, width: '100%' }} />}
        >
          <Flexbox gap={8}>
            {tools.map((toolCall, index) => (
              <ToolCall
                apiName={toolCall.apiName}
                arguments={toolCall.arguments}
                id={toolCall.id}
                identifier={toolCall.identifier}
                index={index}
                key={toolCall.id}
                messageId={id}
                showPortal={!inThread}
              />
            ))}
          </Flexbox>
        </Suspense>
      )}
    </Flexbox>
  );
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Messages/Assistant/FileChunks/index.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { createStyles } from 'antd-style';
import { BookOpenTextIcon, ChevronDown, ChevronRight } from 'lucide-react';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { ChatFileChunk } from '@/types/message';

import ChunkItem from './Item';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  container: css`
    cursor: pointer;

    padding-block: 8px;
    padding-inline: 12px;
    padding-inline-end: 12px;

    color: ${token.colorText};

    background: ${token.colorFillTertiary};
    border-radius: 8px;

    &:hover {
      background: ${isDarkMode ? '' : token.colorFillSecondary};
    }
  `,
  title: css`
    overflow: hidden;
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-line-clamp: 1;

    font-size: 12px;
    text-overflow: ellipsis;
  `,
}));

interface FileChunksProps {
  data: ChatFileChunk[];
}

const FileChunks = memo<FileChunksProps>(({ data }) => {
  const { t } = useTranslation('chat');
  const { styles, theme } = useStyles();

  const [showDetail, setShowDetail] = useState(false);

  return (
    <Flexbox
      className={styles.container}
      gap={16}
      onClick={() => {
        setShowDetail(!showDetail);
      }}
      width={'100%'}
    >
      <Flexbox distribution={'space-between'} flex={1} horizontal>
        <Flexbox gap={8} horizontal>
          <Icon color={theme.geekblue} icon={BookOpenTextIcon} /> {t('rag.referenceChunks')}
        </Flexbox>
        <Icon icon={showDetail ? ChevronDown : ChevronRight} />
      </Flexbox>
      {showDetail && (
        <Flexbox gap={8} horizontal wrap={'wrap'}>
          {data.map((item, index) => {
            return <ChunkItem index={index} key={item.id} {...item}></ChunkItem>;
          })}
        </Flexbox>
      )}
    </Flexbox>
  );
});

export default FileChunks;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Messages/Assistant/FileChunks/Item/index.tsx
================================================================================

import { Tooltip } from '@lobehub/ui';
import { Typography } from 'antd';
import { memo } from 'react';
import { Center, Flexbox } from 'react-layout-kit';

import FileIcon from '@/components/FileIcon';
import { useIsMobile } from '@/hooks/useIsMobile';
import { useChatStore } from '@/store/chat';
import { ChatFileChunk } from '@/types/message';

import { useStyles } from './style';

export interface ChunkItemProps extends ChatFileChunk {
  index: number;
}

const ChunkItem = memo<ChunkItemProps>(({ id, fileId, similarity, text, filename, fileType }) => {
  const { styles, cx } = useStyles();
  const openFilePreview = useChatStore((s) => s.openFilePreview);

  const isMobile = useIsMobile();
  return (
    <Flexbox
      align={'center'}
      className={cx(styles.container, isMobile && styles.mobile)}
      gap={4}
      horizontal
      key={id}
      onClick={(e) => {
        e.stopPropagation();
        openFilePreview({ chunkId: id, chunkText: text, fileId });
      }}
    >
      <FileIcon fileName={filename} fileType={fileType} size={20} variant={'pure'} />
      <Flexbox gap={12} horizontal justify={'space-between'} style={{ maxWidth: 200 }}>
        <Typography.Text ellipsis={{ tooltip: false }}>{filename}</Typography.Text>
        {/*<Typography.Text*/}
        {/*  ellipsis={{ suffix: '...' }}*/}
        {/*  style={{ fontSize: 12, lineHeight: 1 }}*/}
        {/*  type={'secondary'}*/}
        {/*>*/}
        {/*  {text}*/}
        {/*</Typography.Text>*/}
        {similarity && (
          <Tooltip title={similarity}>
            <Center className={styles.badge}>{similarity.toFixed(1)}</Center>
          </Tooltip>
        )}
      </Flexbox>
    </Flexbox>
  );
});

export default ChunkItem;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Messages/User/BelowMessage.tsx
================================================================================

import { ActionIcon } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import isEqual from 'fast-deep-equal';
import { RotateCwIcon, Trash2 } from 'lucide-react';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { useChatStore } from '@/store/chat';
import { ChatMessage } from '@/types/message';

const useStyles = createStyles(({ css, cx }) => ({
  action: cx(
    css`
      align-self: flex-end;
      opacity: 0;
    `,
    'rag-query-actions',
  ),
  container: css`
    &:hover {
      .rag-query-actions {
        opacity: 1;
      }
    }
  `,
  content: css`
    overflow-y: scroll;
    flex-wrap: wrap;

    width: 100%;
    max-height: 54px;
    margin-block-start: 6px;
  `,
}));

export const UserBelowMessage = memo<ChatMessage>(({ ragQuery, content, id }) => {
  const { styles } = useStyles();

  const { t } = useTranslation('chat');

  const [deleteUserMessageRagQuery, rewriteQuery] = useChatStore((s) => [
    s.deleteUserMessageRagQuery,
    s.rewriteQuery,
  ]);

  return (
    !!ragQuery &&
    !isEqual(ragQuery, content) && (
      <Flexbox className={styles.container}>
        <Flexbox align={'center'} className={styles.content} gap={4} horizontal>
          <Typography.Text style={{ fontSize: 12 }} type={'secondary'}>
            {ragQuery}
          </Typography.Text>
        </Flexbox>
        <Flexbox className={styles.action} horizontal>
          <ActionIcon
            icon={Trash2}
            onClick={() => {
              deleteUserMessageRagQuery(id);
            }}
            size={'small'}
            title={t('rag.userQuery.actions.delete')}
          />
          <ActionIcon
            icon={RotateCwIcon}
            onClick={() => {
              rewriteQuery(id);
            }}
            size={'small'}
            title={t('rag.userQuery.actions.regenerate')}
          />
        </Flexbox>
      </Flexbox>
    )
  );
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/components/SkeletonList.tsx
================================================================================

'use client';

import { Skeleton } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';
import { Flexbox } from 'react-layout-kit';

const useStyles = createStyles(({ css, prefixCls }) => ({
  message: css`
    display: flex;
    gap: 12px;
    .${prefixCls}-skeleton-header {
      padding: 0;
    }
  `,
  user: css`
    flex-direction: row-reverse;

    .${prefixCls}-skeleton-paragraph {
      display: flex;
      flex-direction: column;
      align-items: flex-end;
    }
  `,
}));
interface SkeletonListProps {
  mobile?: boolean;
}
const SkeletonList = memo<SkeletonListProps>(({ mobile }) => {
  const { cx, styles } = useStyles();

  return (
    <Flexbox gap={24} padding={mobile ? 8 : 12} style={{ marginTop: 24 }}>
      <Skeleton
        active
        avatar={{ size: mobile ? 32 : 40 }}
        className={cx(styles.message, styles.user)}
        paragraph={{ width: mobile ? ['80%', '40%'] : ['50%', '30%'] }}
        title={false}
      />
      <Skeleton
        active
        avatar={{ size: mobile ? 32 : 40 }}
        className={styles.message}
        paragraph={{ width: mobile ? ['80%', '40%'] : ['50%', '30%'] }}
        title={false}
      />
    </Flexbox>
  );
});
export default SkeletonList;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/components/ChatItem/index.tsx
================================================================================

'use client';

import { ChatItem } from '@lobehub/ui';
import { createStyles } from 'antd-style';
import isEqual from 'fast-deep-equal';
import { MouseEventHandler, ReactNode, memo, useCallback, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/selectors';
import { useChatStore } from '@/store/chat';
import { chatSelectors } from '@/store/chat/selectors';
import { useUserStore } from '@/store/user';
import { userGeneralSettingsSelectors } from '@/store/user/selectors';
import { ChatMessage } from '@/types/message';

import ErrorMessageExtra, { useErrorContent } from '../../Error';
import { renderMessagesExtra } from '../../Extras';
import {
  markdownCustomRenders,
  renderBelowMessages,
  renderMessages,
  useAvatarsClick,
} from '../../Messages';
import History from '../History';
import { markdownElements } from '../MarkdownElements';
import { InPortalThreadContext } from './InPortalThreadContext';
import { processWithArtifact } from './utils';

const rehypePlugins = markdownElements.map((element) => element.rehypePlugin);

const useStyles = createStyles(({ css, prefixCls }) => ({
  loading: css`
    opacity: 0.6;
  `,
  message: css`
    position: relative;
    // prevent the textarea too long
    .${prefixCls}-input {
      max-height: 900px;
    }
  `,
}));

export interface ChatListItemProps {
  actionBar?: ReactNode;
  className?: string;
  disableEditing?: boolean;
  enableHistoryDivider?: boolean;
  endRender?: ReactNode;
  id: string;
  inPortalThread?: boolean;
  index: number;
}

const Item = memo<ChatListItemProps>(
  ({
    className,
    enableHistoryDivider,
    id,
    actionBar,
    endRender,
    disableEditing,
    inPortalThread = false,
  }) => {
    const fontSize = useUserStore(userGeneralSettingsSelectors.fontSize);
    const { t } = useTranslation('common');
    const { styles, cx } = useStyles();
    const [type = 'chat'] = useAgentStore((s) => {
      const config = agentSelectors.currentAgentChatConfig(s);
      return [config.displayMode];
    });

    const item = useChatStore(chatSelectors.getMessageById(id), isEqual);

    const [
      isMessageLoading,
      generating,
      isInRAGFlow,
      editing,
      toggleMessageEditing,
      updateMessageContent,
    ] = useChatStore((s) => [
      chatSelectors.isMessageLoading(id)(s),
      chatSelectors.isMessageGenerating(id)(s),
      chatSelectors.isMessageInRAGFlow(id)(s),
      chatSelectors.isMessageEditing(id)(s),
      s.toggleMessageEditing,
      s.modifyMessageContent,
    ]);

    // when the message is in RAG flow or the AI generating, it should be in loading state
    const isProcessing = isInRAGFlow || generating;

    const onAvatarsClick = useAvatarsClick(item?.role);

    const renderMessage = useCallback(
      (editableContent: ReactNode) => {
        if (!item?.role) return;
        const RenderFunction = renderMessages[item.role] ?? renderMessages['default'];

        if (!RenderFunction) return;

        return <RenderFunction {...item} editableContent={editableContent} />;
      },
      [item],
    );

    const BelowMessage = useCallback(
      ({ data }: { data: ChatMessage }) => {
        if (!item?.role) return;
        const RenderFunction = renderBelowMessages[item.role] ?? renderBelowMessages['default'];

        if (!RenderFunction) return;

        return <RenderFunction {...data} />;
      },
      [item?.role],
    );

    const MessageExtra = useCallback(
      ({ data }: { data: ChatMessage }) => {
        if (!item?.role) return;
        let RenderFunction;
        if (renderMessagesExtra?.[item.role]) RenderFunction = renderMessagesExtra[item.role];

        if (!RenderFunction) return;
        return <RenderFunction {...data} />;
      },
      [item?.role],
    );

    const markdownCustomRender = useCallback(
      (dom: ReactNode, { text }: { text: string }) => {
        if (!item?.role) return dom;
        let RenderFunction;

        if (renderMessagesExtra?.[item.role]) RenderFunction = markdownCustomRenders[item.role];
        if (!RenderFunction) return dom;

        return <RenderFunction displayMode={type} dom={dom} id={id} text={text} />;
      },
      [item?.role, type],
    );

    const error = useErrorContent(item?.error);

    // remove line breaks in artifact tag to make the ast transform easier
    const message =
      !editing && item?.role === 'assistant' ? processWithArtifact(item?.content) : item?.content;

    // ======================= Performance Optimization ======================= //
    // these useMemo/useCallback are all for the performance optimization
    // maybe we can remove it in React 19
    // ======================================================================== //

    const components = useMemo(
      () =>
        Object.fromEntries(
          markdownElements.map((element) => {
            const Component = element.Component;

            return [element.tag, (props: any) => <Component {...props} id={id} />];
          }),
        ),
      [id],
    );

    const markdownProps = useMemo(
      () => ({
        components,
        customRender: markdownCustomRender,
        rehypePlugins,
      }),
      [components, markdownCustomRender],
    );

    const onChange = useCallback((value: string) => updateMessageContent(id, value), [id]);

    const onDoubleClick = useCallback<MouseEventHandler<HTMLDivElement>>(
      (e) => {
        if (!item || disableEditing) return;
        if (item.id === 'default' || item.error) return;
        if (item.role && ['assistant', 'user'].includes(item.role) && e.altKey) {
          toggleMessageEditing(id, true);
        }
      },
      [item, disableEditing],
    );

    const text = useMemo(
      () => ({
        cancel: t('cancel'),
        confirm: t('ok'),
        edit: t('edit'),
      }),
      [t],
    );

    const onEditingChange = useCallback((edit: boolean) => {
      toggleMessageEditing(id, edit);
    }, []);

    const belowMessage = useMemo(() => item && <BelowMessage data={item} />, [item]);
    const errorMessage = useMemo(() => item && <ErrorMessageExtra data={item} />, [item]);
    const messageExtra = useMemo(() => item && <MessageExtra data={item} />, [item]);

    return (
      item && (
        <InPortalThreadContext.Provider value={inPortalThread}>
          {enableHistoryDivider && <History />}
          <Flexbox className={cx(styles.message, className, isMessageLoading && styles.loading)}>
            <ChatItem
              actions={actionBar}
              avatar={item.meta}
              belowMessage={belowMessage}
              editing={editing}
              error={error}
              errorMessage={errorMessage}
              fontSize={fontSize}
              loading={isProcessing}
              markdownProps={markdownProps}
              message={message}
              messageExtra={messageExtra}
              onAvatarClick={onAvatarsClick}
              onChange={onChange}
              onDoubleClick={onDoubleClick}
              onEditingChange={onEditingChange}
              placement={type === 'chat' ? (item.role === 'user' ? 'right' : 'left') : 'left'}
              primary={item.role === 'user'}
              renderMessage={renderMessage}
              text={text}
              time={item.updatedAt || item.createdAt}
              type={type === 'chat' ? 'block' : 'pure'}
            />
            {endRender}
          </Flexbox>
        </InPortalThreadContext.Provider>
      )
    );
  },
);

Item.displayName = 'ChatItem';

export default Item;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/components/MarkdownElements/LobeThinking/rehypePlugin.ts
================================================================================

import type { Node } from 'unist';
import { visit } from 'unist-util-visit';

import { ARTIFACT_THINKING_TAG } from '@/const/plugin';

// eslint-disable-next-line unicorn/consistent-function-scoping
const rehypePlugin = () => (tree: Node) => {
  visit(tree, 'element', (node: any, index, parent) => {
    if (node.type === 'element' && node.tagName === 'p') {
      const children = node.children || [];
      const openTagIndex = children.findIndex(
        (child: any) => child.type === 'raw' && child.value === `<${ARTIFACT_THINKING_TAG}>`,
      );
      const closeTagIndex = children.findIndex(
        (child: any) => child.type === 'raw' && child.value === `</${ARTIFACT_THINKING_TAG}>`,
      );

      if (openTagIndex !== -1) {
        // 有闭合标签的情况
        if (closeTagIndex !== -1 && closeTagIndex > openTagIndex) {
          const content = children.slice(openTagIndex + 1, closeTagIndex);
          const lobeThinkingNode = {
            children: content,
            properties: {},
            tagName: ARTIFACT_THINKING_TAG,
            type: 'element',
          };

          // Replace the entire paragraph with our new lobeThinking node
          parent.children.splice(index, 1, lobeThinkingNode);
          return index; // Skip processing the newly inserted node
        } else {
          // 无闭合标签的情况
          const content = children.slice(openTagIndex + 1);
          const lobeThinkingNode = {
            children: content,
            properties: {},
            tagName: ARTIFACT_THINKING_TAG,
            type: 'element',
          };

          // Replace the entire paragraph with our new lobeThinking node
          parent.children.splice(index, 1, lobeThinkingNode);
          return index; // Skip processing the newly inserted node
        }
      }
    }
  });
};

export default rehypePlugin;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/index.tsx
================================================================================

import { IPluginErrorType, PluginErrorType } from '@lobehub/chat-plugin-sdk';
import type { AlertProps } from '@lobehub/ui';
import { Skeleton } from 'antd';
import dynamic from 'next/dynamic';
import { Suspense, memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';

import { useProviderName } from '@/hooks/useProviderName';
import { AgentRuntimeErrorType, ILobeAgentRuntimeErrorType } from '@/libs/agent-runtime';
import { ChatErrorType, ErrorType } from '@/types/fetch';
import { ChatMessage, ChatMessageError } from '@/types/message';

import ClerkLogin from './ClerkLogin';
import ErrorJsonViewer from './ErrorJsonViewer';
import InvalidAPIKey from './InvalidAPIKey';
import InvalidAccessCode from './InvalidAccessCode';
import OpenAiBizError from './OpenAiBizError';

const loading = () => <Skeleton active />;

const OllamaBizError = dynamic(() => import('./OllamaBizError'), { loading, ssr: false });
const PluginSettings = dynamic(() => import('./PluginSettings'), { loading, ssr: false });

// Config for the errorMessage display
const getErrorAlertConfig = (
  errorType?: IPluginErrorType | ILobeAgentRuntimeErrorType | ErrorType,
): AlertProps | undefined => {
  // OpenAIBizError / ZhipuBizError / GoogleBizError / ...
  if (typeof errorType === 'string' && (errorType.includes('Biz') || errorType.includes('Invalid')))
    return {
      extraDefaultExpand: true,
      extraIsolate: true,
      type: 'warning',
    };

  switch (errorType) {
    case AgentRuntimeErrorType.PermissionDenied:
    case AgentRuntimeErrorType.QuotaLimitReached:
    case AgentRuntimeErrorType.LocationNotSupportError: {
      return {
        type: 'warning',
      };
    }

    case AgentRuntimeErrorType.NoOpenAIAPIKey: {
      return {
        extraDefaultExpand: true,
        extraIsolate: true,
        type: 'warning',
      };
    }

    default: {
      return undefined;
    }
  }
};

export const useErrorContent = (error: any) => {
  const { t } = useTranslation('error');
  const providerName = useProviderName(error?.body?.provider || '');

  return useMemo<AlertProps | undefined>(() => {
    if (!error) return;
    const messageError = error;

    const alertConfig = getErrorAlertConfig(messageError.type);

    return {
      message: t(`response.${messageError.type}` as any, { provider: providerName }),
      ...alertConfig,
    };
  }, [error]);
};

const ErrorMessageExtra = memo<{ data: ChatMessage }>(({ data }) => {
  const error = data.error as ChatMessageError;
  if (!error?.type) return;

  switch (error.type) {
    case PluginErrorType.PluginSettingsInvalid: {
      return <PluginSettings id={data.id} plugin={data.plugin} />;
    }

    case AgentRuntimeErrorType.OpenAIBizError: {
      return <OpenAiBizError {...data} />;
    }

    case AgentRuntimeErrorType.OllamaBizError: {
      return <OllamaBizError {...data} />;
    }

    case ChatErrorType.InvalidClerkUser: {
      return <ClerkLogin id={data.id} />;
    }

    case ChatErrorType.InvalidAccessCode: {
      return <InvalidAccessCode id={data.id} provider={data.error?.body?.provider} />;
    }

    case AgentRuntimeErrorType.NoOpenAIAPIKey: {
      {
        return <InvalidAPIKey id={data.id} provider={data.error?.body?.provider} />;
      }
    }
  }

  if (error.type.toString().includes('Invalid')) {
    return <InvalidAPIKey id={data.id} provider={data.error?.body?.provider} />;
  }

  return <ErrorJsonViewer error={data.error} id={data.id} />;
});

export default memo<{ data: ChatMessage }>(({ data }) => (
  <Suspense fallback={<Skeleton active style={{ width: '100%' }} />}>
    <ErrorMessageExtra data={data} />
  </Suspense>
));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/InvalidAccessCode.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { Segmented } from 'antd';
import { SegmentedLabeledOption } from 'antd/es/segmented';
import { AsteriskSquare, KeySquare, ScanFace } from 'lucide-react';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { useServerConfigStore } from '@/store/serverConfig';
import { featureFlagsSelectors, serverConfigSelectors } from '@/store/serverConfig/selectors';

import APIKeyForm from './APIKeyForm';
import AccessCodeForm from './AccessCodeForm';
import OAuthForm from './OAuthForm';
import { ErrorActionContainer } from './style';

enum Tab {
  Api = 'api',
  Oauth = 'oauth',
  Password = 'password',
}

interface InvalidAccessCodeProps {
  id: string;
  provider?: string;
}

const InvalidAccessCode = memo<InvalidAccessCodeProps>(({ id, provider }) => {
  const { t } = useTranslation('error');
  const isEnabledOAuth = useServerConfigStore(serverConfigSelectors.enabledOAuthSSO);
  const defaultTab = isEnabledOAuth ? Tab.Oauth : Tab.Password;
  const [mode, setMode] = useState<Tab>(defaultTab);
  const { showOpenAIApiKey } = useServerConfigStore(featureFlagsSelectors);
  const isEnabledTab = showOpenAIApiKey || isEnabledOAuth;

  return (
    <ErrorActionContainer>
      {isEnabledTab && (
        <Segmented
          block
          onChange={(value) => setMode(value as Tab)}
          options={
            [
              isEnabledOAuth
                ? {
                    icon: <Icon icon={ScanFace} />,
                    label: t('oauth', { ns: 'common' }),
                    value: Tab.Oauth,
                  }
                : undefined,
              {
                icon: <Icon icon={AsteriskSquare} />,
                label: t('unlock.tabs.password'),
                value: Tab.Password,
              },
              showOpenAIApiKey
                ? {
                    icon: <Icon icon={KeySquare} />,
                    label: t('unlock.tabs.apiKey'),
                    value: Tab.Api,
                  }
                : undefined,
            ].filter(Boolean) as SegmentedLabeledOption[]
          }
          style={{ width: '100%' }}
          value={mode}
        />
      )}

      <Flexbox gap={24}>
        {mode === Tab.Password && <AccessCodeForm id={id} />}
        {showOpenAIApiKey && mode === Tab.Api && <APIKeyForm id={id} provider={provider} />}
        {isEnabledOAuth && mode === Tab.Oauth && <OAuthForm id={id} />}
      </Flexbox>
    </ErrorActionContainer>
  );
});

export default InvalidAccessCode;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/OpenAiBizError.tsx
================================================================================

import { memo } from 'react';

import { ChatMessage } from '@/types/message';

import ErrorJsonViewer from './ErrorJsonViewer';
import InvalidAPIKey from './InvalidAPIKey';

interface OpenAIError {
  code: 'invalid_api_key' | 'insufficient_quota' | string;
  message: string;
  param?: any;
  type: string;
}

interface OpenAIErrorResponse {
  error: OpenAIError;
}

const OpenAiBizError = memo<ChatMessage>(({ error, id }) => {
  const errorBody: OpenAIErrorResponse = (error as any)?.body;

  const errorCode = errorBody.error?.code;

  if (errorCode === 'invalid_api_key') return <InvalidAPIKey id={id} />;

  return <ErrorJsonViewer error={error} id={id} />;
});

export default OpenAiBizError;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/APIKeyForm/index.tsx
================================================================================

import { ProviderIcon } from '@lobehub/icons';
import { Button } from 'antd';
import { memo, useMemo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import { ModelProvider } from '@/libs/agent-runtime';
import { useChatStore } from '@/store/chat';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import BedrockForm from './Bedrock';
import ProviderApiKeyForm from './ProviderApiKeyForm';
import WenxinForm from './Wenxin';

interface APIKeyFormProps {
  id: string;
  provider?: string;
}

const APIKeyForm = memo<APIKeyFormProps>(({ id, provider }) => {
  const { t } = useTranslation('error');

  const [resend, deleteMessage] = useChatStore((s) => [s.regenerateMessage, s.deleteMessage]);

  const apiKeyPlaceholder = useMemo(() => {
    switch (provider) {
      case ModelProvider.Anthropic: {
        return 'sk-ant_*****************************';
      }

      case ModelProvider.OpenRouter: {
        return 'sk-or-********************************';
      }

      case ModelProvider.Perplexity: {
        return 'pplx-********************************';
      }

      case ModelProvider.ZhiPu: {
        return '*********************.*************';
      }

      case ModelProvider.Groq: {
        return 'gsk_*****************************';
      }

      case ModelProvider.DeepSeek: {
        return 'sk_******************************';
      }

      case ModelProvider.Qwen: {
        return 'sk-********************************';
      }

      case ModelProvider.Github: {
        return 'ghp_*****************************';
      }

      default: {
        return '*********************************';
      }
    }
  }, [provider]);

  return (
    <Center gap={16} style={{ maxWidth: 300 }}>
      {provider === ModelProvider.Bedrock ? (
        <BedrockForm />
      ) : provider === ModelProvider.Wenxin ? (
        <WenxinForm />
      ) : (
        <ProviderApiKeyForm
          apiKeyPlaceholder={apiKeyPlaceholder}
          avatar={<ProviderIcon provider={provider} size={80} type={'avatar'} />}
          provider={provider as GlobalLLMProviderKey}
          showEndpoint={provider === ModelProvider.OpenAI}
        />
      )}
      <Flexbox gap={12} width={'100%'}>
        <Button
          block
          onClick={() => {
            resend(id);
            deleteMessage(id);
          }}
          style={{ marginTop: 8 }}
          type={'primary'}
        >
          {t('unlock.confirm')}
        </Button>
        <Button
          onClick={() => {
            deleteMessage(id);
          }}
        >
          {t('unlock.closeMessage')}
        </Button>
      </Flexbox>
    </Center>
  );
});

export default APIKeyForm;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/APIKeyForm/Bedrock.tsx
================================================================================

import { Aws } from '@lobehub/icons';
import { Icon } from '@lobehub/ui';
import { Button, Input, Select } from 'antd';
import { useTheme } from 'antd-style';
import { Network, ShieldPlus } from 'lucide-react';
import { memo, useState } from 'react';
import { useTranslation } from 'react-i18next';

import { ModelProvider } from '@/libs/agent-runtime';
import { useUserStore } from '@/store/user';
import { keyVaultsConfigSelectors } from '@/store/user/selectors';

import { FormAction } from '../style';

const BedrockForm = memo(() => {
  const { t } = useTranslation('modelProvider');
  const [showRegion, setShow] = useState(false);
  const [showSessionToken, setShowSessionToken] = useState(false);

  const [accessKeyId, secretAccessKey, sessionToken, region, setConfig] = useUserStore((s) => [
    keyVaultsConfigSelectors.bedrockConfig(s).accessKeyId,
    keyVaultsConfigSelectors.bedrockConfig(s).secretAccessKey,
    keyVaultsConfigSelectors.bedrockConfig(s).sessionToken,
    keyVaultsConfigSelectors.bedrockConfig(s).region,
    s.updateKeyVaultConfig,
  ]);

  const theme = useTheme();
  return (
    <FormAction
      avatar={<Aws.Color color={theme.colorText} size={56} />}
      description={t('bedrock.unlock.description')}
      title={t('bedrock.unlock.title')}
    >
      <Input.Password
        autoComplete={'new-password'}
        onChange={(e) => {
          setConfig(ModelProvider.Bedrock, { accessKeyId: e.target.value });
        }}
        placeholder={'Aws Access Key Id'}
        type={'block'}
        value={accessKeyId}
      />
      <Input.Password
        autoComplete={'new-password'}
        onChange={(e) => {
          setConfig(ModelProvider.Bedrock, { secretAccessKey: e.target.value });
        }}
        placeholder={'Aws Secret Access Key'}
        type={'block'}
        value={secretAccessKey}
      />
      {showSessionToken ? (
        <Input.Password
          autoComplete={'new-password'}
          onChange={(e) => {
            setConfig(ModelProvider.Bedrock, { sessionToken: e.target.value });
          }}
          placeholder={'Aws Session Token'}
          type={'block'}
          value={sessionToken}
        />
      ) : (
        <Button
          block
          icon={<Icon icon={ShieldPlus} />}
          onClick={() => {
            setShowSessionToken(true);
          }}
          type={'text'}
        >
          {t('bedrock.unlock.customSessionToken')}
        </Button>
      )}
      {showRegion ? (
        <Select
          onChange={(region) => {
            setConfig('bedrock', { region });
          }}
          options={['us-east-1', 'us-west-2', 'ap-southeast-1'].map((i) => ({
            label: i,
            value: i,
          }))}
          placeholder={'https://api.openai.com/v1'}
          style={{ width: '100%' }}
          value={region}
        />
      ) : (
        <Button
          block
          icon={<Icon icon={Network} />}
          onClick={() => {
            setShow(true);
          }}
          type={'text'}
        >
          {t('bedrock.unlock.customRegion')}
        </Button>
      )}
    </FormAction>
  );
});

export default BedrockForm;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/features/Conversation/Error/APIKeyForm/ProviderApiKeyForm.tsx
================================================================================

import { Icon } from '@lobehub/ui';
import { Button, Input } from 'antd';
import { Network } from 'lucide-react';
import { ReactNode, memo, useState } from 'react';
import { useTranslation } from 'react-i18next';

import { useProviderName } from '@/hooks/useProviderName';
import { featureFlagsSelectors, useServerConfigStore } from '@/store/serverConfig';
import { useUserStore } from '@/store/user';
import { keyVaultsConfigSelectors } from '@/store/user/selectors';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import { FormAction } from '../style';

interface ProviderApiKeyFormProps {
  apiKeyPlaceholder?: string;
  avatar?: ReactNode;
  provider: GlobalLLMProviderKey;
  showEndpoint?: boolean;
}

const ProviderApiKeyForm = memo<ProviderApiKeyFormProps>(
  ({ provider, avatar, showEndpoint = false, apiKeyPlaceholder }) => {
    const { t } = useTranslation(['modelProvider', 'error']);
    const { t: errorT } = useTranslation('error');
    const [showProxy, setShow] = useState(false);

    const [apiKey, proxyUrl, setConfig] = useUserStore((s) => [
      keyVaultsConfigSelectors.getVaultByProvider(provider)(s)?.apiKey,
      keyVaultsConfigSelectors.getVaultByProvider(provider)(s)?.baseURL,
      s.updateKeyVaultConfig,
    ]);
    const { showOpenAIProxyUrl } = useServerConfigStore(featureFlagsSelectors);
    const providerName = useProviderName(provider);

    return (
      <FormAction
        avatar={avatar}
        description={t(`unlock.apiKey.description`, { name: providerName, ns: 'error' })}
        title={t(`unlock.apiKey.title`, { name: providerName, ns: 'error' })}
      >
        <Input.Password
          autoComplete={'new-password'}
          onChange={(e) => {
            setConfig(provider, { apiKey: e.target.value });
          }}
          placeholder={apiKeyPlaceholder || 'sk-***********************'}
          type={'block'}
          value={apiKey}
        />

        {showEndpoint &&
          showOpenAIProxyUrl &&
          (showProxy ? (
            <Input
              onChange={(e) => {
                setConfig(provider, { baseURL: e.target.value });
              }}
              placeholder={'https://api.openai.com/v1'}
              type={'block'}
              value={proxyUrl}
            />
          ) : (
            <Button
              icon={<Icon icon={Network} />}
              onClick={() => {
                setShow(true);
              }}
              type={'text'}
            >
              {errorT('unlock.addProxyUrl')}
            </Button>
          ))}
      </FormAction>
    );
  },
);

export default ProviderApiKeyForm;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/async/file.ts
================================================================================

import { TRPCError } from '@trpc/server';
import { chunk } from 'lodash-es';
import pMap from 'p-map';
import { z } from 'zod';

import { serverDBEnv } from '@/config/db';
import { fileEnv } from '@/config/file';
import { DEFAULT_EMBEDDING_MODEL } from '@/const/settings';
import { NewChunkItem, NewEmbeddingsItem } from '@/database/schemas';
import { serverDB } from '@/database/server';
import { ASYNC_TASK_TIMEOUT, AsyncTaskModel } from '@/database/server/models/asyncTask';
import { ChunkModel } from '@/database/server/models/chunk';
import { EmbeddingModel } from '@/database/server/models/embedding';
import { FileModel } from '@/database/server/models/file';
import { ModelProvider } from '@/libs/agent-runtime';
import { asyncAuthedProcedure, asyncRouter as router } from '@/libs/trpc/async';
import { initAgentRuntimeWithUserPayload } from '@/server/modules/AgentRuntime';
import { S3 } from '@/server/modules/S3';
import { ChunkService } from '@/server/services/chunk';
import {
  AsyncTaskError,
  AsyncTaskErrorType,
  AsyncTaskStatus,
  IAsyncTaskError,
} from '@/types/asyncTask';
import { safeParseJSON } from '@/utils/safeParseJSON';

const fileProcedure = asyncAuthedProcedure.use(async (opts) => {
  const { ctx } = opts;

  return opts.next({
    ctx: {
      asyncTaskModel: new AsyncTaskModel(serverDB, ctx.userId),
      chunkModel: new ChunkModel(serverDB, ctx.userId),
      chunkService: new ChunkService(ctx.userId),
      embeddingModel: new EmbeddingModel(serverDB, ctx.userId),
      fileModel: new FileModel(serverDB, ctx.userId),
    },
  });
});

export const fileRouter = router({
  embeddingChunks: fileProcedure
    .input(
      z.object({
        fileId: z.string(),
        model: z.string().default(DEFAULT_EMBEDDING_MODEL),
        taskId: z.string(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const file = await ctx.fileModel.findById(input.fileId);

      if (!file) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'File not found' });
      }

      const asyncTask = await ctx.asyncTaskModel.findById(input.taskId);

      if (!asyncTask) throw new TRPCError({ code: 'BAD_REQUEST', message: 'Async Task not found' });

      try {
        const timeoutPromise = new Promise((_, reject) => {
          setTimeout(() => {
            reject(
              new AsyncTaskError(
                AsyncTaskErrorType.Timeout,
                'embedding task is timeout, please try again',
              ),
            );
          }, ASYNC_TASK_TIMEOUT);
        });

        const embeddingPromise = async () => {
          // update the task status to success
          await ctx.asyncTaskModel.update(input.taskId, {
            status: AsyncTaskStatus.Processing,
          });

          const startAt = Date.now();

          const CHUNK_SIZE = 50;
          const CONCURRENCY = 10;

          const chunks = await ctx.chunkModel.getChunksTextByFileId(input.fileId);
          const requestArray = chunk(chunks, CHUNK_SIZE);

          try {
            await pMap(
              requestArray,
              async (chunks, index) => {
                const agentRuntime = await initAgentRuntimeWithUserPayload(
                  ModelProvider.OpenAI,
                  ctx.jwtPayload,
                );

                const number = index + 1;
                console.log(`执行第 ${number} 个任务`);

                console.time(`任务[${number}]: embeddings`);

                const embeddings = await agentRuntime.embeddings({
                  dimensions: 1024,
                  input: chunks.map((c) => c.text),
                  model: input.model,
                });
                console.timeEnd(`任务[${number}]: embeddings`);

                const items: NewEmbeddingsItem[] =
                  embeddings?.map((e, idx) => ({
                    chunkId: chunks[idx].id,
                    embeddings: e,
                    fileId: input.fileId,
                    model: input.model,
                  })) || [];

                console.time(`任务[${number}]: insert db`);
                await ctx.embeddingModel.bulkCreate(items);
                console.timeEnd(`任务[${number}]: insert db`);
              },
              { concurrency: CONCURRENCY },
            );
          } catch (e) {
            throw {
              message: JSON.stringify(e),
              name: AsyncTaskErrorType.EmbeddingError,
            };
          }

          const duration = Date.now() - startAt;
          // update the task status to success
          await ctx.asyncTaskModel.update(input.taskId, {
            duration,
            status: AsyncTaskStatus.Success,
          });

          return { success: true };
        };

        // Race between the chunking process and the timeout
        return await Promise.race([embeddingPromise(), timeoutPromise]);
      } catch (e) {
        console.error('embeddingChunks error', e);

        await ctx.asyncTaskModel.update(input.taskId, {
          error: new AsyncTaskError((e as Error).name, (e as Error).message),
          status: AsyncTaskStatus.Error,
        });

        return {
          message: `File ${file.name}(${input.taskId}) failed to embedding: ${(e as Error).message}`,
          success: false,
        };
      }
    }),

  parseFileToChunks: fileProcedure
    .input(
      z.object({
        fileId: z.string(),
        taskId: z.string(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const file = await ctx.fileModel.findById(input.fileId);
      if (!file) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'File not found' });
      }

      const s3 = new S3();

      let content: Uint8Array | undefined;
      try {
        content = await s3.getFileByteArray(file.url);
      } catch (e) {
        console.error(e);
        // if file not found, delete it from db
        if ((e as any).Code === 'NoSuchKey') {
          await ctx.fileModel.delete(input.fileId, serverDBEnv.REMOVE_GLOBAL_FILE);
          throw new TRPCError({ code: 'BAD_REQUEST', message: 'File not found' });
        }
      }

      if (!content) return;

      const asyncTask = await ctx.asyncTaskModel.findById(input.taskId);

      if (!asyncTask) throw new TRPCError({ code: 'BAD_REQUEST', message: 'Async Task not found' });

      try {
        const startAt = Date.now();

        const timeoutPromise = new Promise((_, reject) => {
          setTimeout(() => {
            reject(
              new AsyncTaskError(
                AsyncTaskErrorType.Timeout,
                'chunking task is timeout, please try again',
              ),
            );
          }, ASYNC_TASK_TIMEOUT);
        });

        const chunkingPromise = async () => {
          const chunkService = ctx.chunkService;
          // update the task status to processing
          await ctx.asyncTaskModel.update(input.taskId, { status: AsyncTaskStatus.Processing });

          // partition file to chunks
          const chunkResult = await chunkService.chunkContent({
            content,
            fileType: file.fileType,
            filename: file.name,
          });

          // after finish partition, we need to filter out some elements
          const chunks = chunkResult.chunks.map(
            (item): NewChunkItem => ({ ...item, userId: ctx.userId }),
          );

          const duration = Date.now() - startAt;

          // if no chunk found, throw error
          if (chunks.length === 0) {
            throw {
              message:
                'No chunk found in this file. it may due to current chunking method can not parse file accurately',
              name: AsyncTaskErrorType.NoChunkError,
            };
          }

          await ctx.chunkModel.bulkCreate(chunks, input.fileId);

          if (chunkResult.unstructuredChunks) {
            const unstructuredChunks = chunkResult.unstructuredChunks.map(
              (item): NewChunkItem => ({ ...item, fileId: input.fileId, userId: ctx.userId }),
            );
            await ctx.chunkModel.bulkCreateUnstructuredChunks(unstructuredChunks);
          }

          // update the task status to success
          await ctx.asyncTaskModel.update(input.taskId, {
            duration,
            status: AsyncTaskStatus.Success,
          });

          // if enable auto embedding, trigger the embedding task
          if (fileEnv.CHUNKS_AUTO_EMBEDDING) {
            await chunkService.asyncEmbeddingFileChunks(input.fileId, ctx.jwtPayload);
          }

          return { success: true };
        };
        // Race between the chunking process and the timeout
        return await Promise.race([chunkingPromise(), timeoutPromise]);
      } catch (e) {
        const error = e as any;

        const asyncTaskError = error.body
          ? ({ body: safeParseJSON(error.body) ?? error.body, name: error.name } as IAsyncTaskError)
          : new AsyncTaskError((error as Error).name, error.message);

        console.error('[Chunking Error]', asyncTaskError);
        await ctx.asyncTaskModel.update(input.taskId, {
          error: asyncTaskError,
          status: AsyncTaskStatus.Error,
        });

        return {
          message: `File ${file.name}(${input.taskId}) failed to chunking: ${(e as Error).message}`,
          success: false,
        };
      }
    }),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/async/ragEval.ts
================================================================================

import { TRPCError } from '@trpc/server';
import OpenAI from 'openai';
import { z } from 'zod';

import { chainAnswerWithContext } from '@/chains/answerWithContext';
import { DEFAULT_EMBEDDING_MODEL, DEFAULT_CHAT_MODEL } from '@/const/settings';
import { serverDB } from '@/database/server';
import { ChunkModel } from '@/database/server/models/chunk';
import { EmbeddingModel } from '@/database/server/models/embedding';
import { FileModel } from '@/database/server/models/file';
import {
  EvalDatasetRecordModel,
  EvalEvaluationModel,
  EvaluationRecordModel,
} from '@/database/server/models/ragEval';
import { ModelProvider } from '@/libs/agent-runtime';
import { asyncAuthedProcedure, asyncRouter as router } from '@/libs/trpc/async';
import { initAgentRuntimeWithUserPayload } from '@/server/modules/AgentRuntime';
import { ChunkService } from '@/server/services/chunk';
import { AsyncTaskError } from '@/types/asyncTask';
import { EvalEvaluationStatus } from '@/types/eval';

const ragEvalProcedure = asyncAuthedProcedure.use(async (opts) => {
  const { ctx } = opts;

  return opts.next({
    ctx: {
      chunkModel: new ChunkModel(serverDB, ctx.userId),
      chunkService: new ChunkService(ctx.userId),
      datasetRecordModel: new EvalDatasetRecordModel(ctx.userId),
      embeddingModel: new EmbeddingModel(serverDB, ctx.userId),
      evalRecordModel: new EvaluationRecordModel(ctx.userId),
      evaluationModel: new EvalEvaluationModel(ctx.userId),
      fileModel: new FileModel(serverDB, ctx.userId),
    },
  });
});

export const ragEvalRouter = router({
  runRecordEvaluation: ragEvalProcedure
    .input(
      z.object({
        evalRecordId: z.number(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const evalRecord = await ctx.evalRecordModel.findById(input.evalRecordId);

      if (!evalRecord) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'Evaluation not found' });
      }

      const now = Date.now();
      try {
        const agentRuntime = await initAgentRuntimeWithUserPayload(
          ModelProvider.OpenAI,
          ctx.jwtPayload,
        );

        const { question, languageModel, embeddingModel } = evalRecord;

        let questionEmbeddingId = evalRecord.questionEmbeddingId;
        let context = evalRecord.context;

        // 如果不存在 questionEmbeddingId，那么就需要做一次 embedding
        if (!questionEmbeddingId) {
          const embeddings = await agentRuntime.embeddings({
            dimensions: 1024,
            input: question,
            model: !!embeddingModel ? embeddingModel : DEFAULT_EMBEDDING_MODEL,
          });

          const embeddingId = await ctx.embeddingModel.create({
            embeddings: embeddings?.[0],
            model: embeddingModel,
          });

          await ctx.evalRecordModel.update(evalRecord.id, {
            questionEmbeddingId: embeddingId,
          });

          questionEmbeddingId = embeddingId;
        }

        // 如果不存在 context，那么就需要做一次检索
        if (!context || context.length === 0) {
          const datasetRecord = await ctx.datasetRecordModel.findById(evalRecord.datasetRecordId);

          const embeddingItem = await ctx.embeddingModel.findById(questionEmbeddingId);

          const chunks = await ctx.chunkModel.semanticSearchForChat({
            embedding: embeddingItem!.embeddings!,
            fileIds: datasetRecord!.referenceFiles!,
            query: evalRecord.question,
          });

          context = chunks.map((item) => item.text).filter(Boolean) as string[];
          await ctx.evalRecordModel.update(evalRecord.id, { context });
        }

        // 做一次生成 LLM 答案生成
        const { messages } = chainAnswerWithContext({ context, knowledge: [], question });

        const response = await agentRuntime.chat({
          messages: messages!,
          model: !!languageModel ? languageModel : DEFAULT_CHAT_MODEL,
          responseMode: 'json',
          stream: false,
          temperature: 1,
        });

        const data = (await response.json()) as OpenAI.ChatCompletion;

        const answer = data.choices[0].message.content;

        await ctx.evalRecordModel.update(input.evalRecordId, {
          answer,
          duration: Date.now() - now,
          languageModel,
          status: EvalEvaluationStatus.Success,
        });

        return { success: true };
      } catch (e) {
        await ctx.evalRecordModel.update(input.evalRecordId, {
          error: new AsyncTaskError((e as Error).name, (e as Error).message),
          status: EvalEvaluationStatus.Error,
        });

        await ctx.evaluationModel.update(evalRecord.evaluationId, {
          status: EvalEvaluationStatus.Error,
        });

        console.error('[RAGEvaluation] error', e);

        return { success: false };
      }
    }),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/async/index.ts
================================================================================

import { publicProcedure, asyncRouter as router } from '@/libs/trpc/async';

import { fileRouter } from './file';
import { ragEvalRouter } from './ragEval';

export const asyncRouter = router({
  file: fileRouter,
  healthcheck: publicProcedure.query(() => "i'm live!"),
  ragEval: ragEvalRouter,
});

export type AsyncRouter = typeof asyncRouter;

export * from './caller';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/lambda/file.ts
================================================================================

import { TRPCError } from '@trpc/server';
import { z } from 'zod';

import { serverDBEnv } from '@/config/db';
import { serverDB } from '@/database/server';
import { AsyncTaskModel } from '@/database/server/models/asyncTask';
import { ChunkModel } from '@/database/server/models/chunk';
import { FileModel } from '@/database/server/models/file';
import { authedProcedure, router } from '@/libs/trpc';
import { S3 } from '@/server/modules/S3';
import { getFullFileUrl } from '@/server/utils/files';
import { AsyncTaskStatus, AsyncTaskType } from '@/types/asyncTask';
import { FileListItem, QueryFileListSchema, UploadFileSchema } from '@/types/files';

const fileProcedure = authedProcedure.use(async (opts) => {
  const { ctx } = opts;

  return opts.next({
    ctx: {
      asyncTaskModel: new AsyncTaskModel(serverDB, ctx.userId),
      chunkModel: new ChunkModel(serverDB, ctx.userId),
      fileModel: new FileModel(serverDB, ctx.userId),
    },
  });
});

export const fileRouter = router({
  checkFileHash: fileProcedure
    .input(z.object({ hash: z.string() }))
    .mutation(async ({ ctx, input }) => {
      return ctx.fileModel.checkHash(input.hash);
    }),

  createFile: fileProcedure
    .input(UploadFileSchema.omit({ url: true }).extend({ url: z.string() }))
    .mutation(async ({ ctx, input }) => {
      const { isExist } = await ctx.fileModel.checkHash(input.hash!);

      const { id } = await ctx.fileModel.create(
        {
          fileHash: input.hash,
          fileType: input.fileType,
          knowledgeBaseId: input.knowledgeBaseId,
          metadata: input.metadata,
          name: input.name,
          size: input.size,
          url: input.url,
        },
        // if the file is not exist in global file, create a new one
        !isExist,
      );

      return { id, url: await getFullFileUrl(input.url) };
    }),
  findById: fileProcedure
    .input(
      z.object({
        id: z.string(),
      }),
    )
    .query(async ({ ctx, input }) => {
      const item = await ctx.fileModel.findById(input.id);
      if (!item) throw new TRPCError({ code: 'BAD_REQUEST', message: 'File not found' });

      return { ...item, url: await getFullFileUrl(item?.url) };
    }),

  getFileItemById: fileProcedure
    .input(
      z.object({
        id: z.string(),
      }),
    )
    .query(async ({ ctx, input }): Promise<FileListItem | undefined> => {
      const item = await ctx.fileModel.findById(input.id);

      if (!item) throw new TRPCError({ code: 'BAD_REQUEST', message: 'File not found' });

      let embeddingTask = null;
      if (item.embeddingTaskId) {
        embeddingTask = await ctx.asyncTaskModel.findById(item.embeddingTaskId);
      }
      let chunkingTask = null;
      if (item.chunkTaskId) {
        chunkingTask = await ctx.asyncTaskModel.findById(item.chunkTaskId);
      }

      const chunkCount = await ctx.chunkModel.countByFileId(input.id);

      return {
        ...item,
        chunkCount,
        chunkingError: chunkingTask?.error,
        chunkingStatus: chunkingTask?.status as AsyncTaskStatus,
        embeddingError: embeddingTask?.error,
        embeddingStatus: embeddingTask?.status as AsyncTaskStatus,
        finishEmbedding: embeddingTask?.status === AsyncTaskStatus.Success,
        url: await getFullFileUrl(item.url!),
      };
    }),

  getFiles: fileProcedure.input(QueryFileListSchema).query(async ({ ctx, input }) => {
    const fileList = await ctx.fileModel.query(input);

    const fileIds = fileList.map((item) => item.id);
    const chunks = await ctx.chunkModel.countByFileIds(fileIds);

    const chunkTaskIds = fileList.map((result) => result.chunkTaskId).filter(Boolean) as string[];

    const chunkTasks = await ctx.asyncTaskModel.findByIds(chunkTaskIds, AsyncTaskType.Chunking);

    const embeddingTaskIds = fileList
      .map((result) => result.embeddingTaskId)
      .filter(Boolean) as string[];
    const embeddingTasks = await ctx.asyncTaskModel.findByIds(
      embeddingTaskIds,
      AsyncTaskType.Embedding,
    );

    const resultFiles = [] as any[];
    for (const { chunkTaskId, embeddingTaskId, ...item } of fileList as any[]) {
      const chunkTask = chunkTaskId ? chunkTasks.find((task) => task.id === chunkTaskId) : null;
      const embeddingTask = embeddingTaskId
        ? embeddingTasks.find((task) => task.id === embeddingTaskId)
        : null;

      const fileItem = {
        ...item,
        chunkCount: chunks.find((chunk) => chunk.id === item.id)?.count ?? null,
        chunkingError: chunkTask?.error ?? null,
        chunkingStatus: chunkTask?.status as AsyncTaskStatus,
        embeddingError: embeddingTask?.error ?? null,
        embeddingStatus: embeddingTask?.status as AsyncTaskStatus,
        finishEmbedding: embeddingTask?.status === AsyncTaskStatus.Success,
        url: await getFullFileUrl(item.url!),
      } as FileListItem;
      resultFiles.push(fileItem);
    }

    return resultFiles;
  }),

  removeAllFiles: fileProcedure.mutation(async ({ ctx }) => {
    return ctx.fileModel.clear();
  }),

  removeFile: fileProcedure.input(z.object({ id: z.string() })).mutation(async ({ input, ctx }) => {
    const file = await ctx.fileModel.delete(input.id, serverDBEnv.REMOVE_GLOBAL_FILE);

    if (!file) return;

    // delele the file from remove from S3 if it is not used by other files
    const s3Client = new S3();
    await s3Client.deleteFile(file.url!);
  }),

  removeFileAsyncTask: fileProcedure
    .input(
      z.object({
        id: z.string(),
        type: z.enum(['embedding', 'chunk']),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const file = await ctx.fileModel.findById(input.id);

      if (!file) return;

      const taskId = input.type === 'embedding' ? file.embeddingTaskId : file.chunkTaskId;

      if (!taskId) return;

      await ctx.asyncTaskModel.delete(taskId);
    }),

  removeFiles: fileProcedure
    .input(z.object({ ids: z.array(z.string()) }))
    .mutation(async ({ input, ctx }) => {
      const needToRemoveFileList = await ctx.fileModel.deleteMany(
        input.ids,
        serverDBEnv.REMOVE_GLOBAL_FILE,
      );

      if (!needToRemoveFileList || needToRemoveFileList.length === 0) return;

      // remove from S3
      const s3Client = new S3();

      await s3Client.deleteFiles(needToRemoveFileList.map((file) => file.url!));
    }),
});

export type FileRouter = typeof fileRouter;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/lambda/ragEval.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix  */
import { TRPCError } from '@trpc/server';
import dayjs from 'dayjs';
import JSONL from 'jsonl-parse-stringify';
import pMap from 'p-map';
import { z } from 'zod';

import { DEFAULT_EMBEDDING_MODEL, DEFAULT_CHAT_MODEL } from '@/const/settings';
import { serverDB } from '@/database/server';
import { FileModel } from '@/database/server/models/file';
import {
  EvalDatasetModel,
  EvalDatasetRecordModel,
  EvalEvaluationModel,
  EvaluationRecordModel,
} from '@/database/server/models/ragEval';
import { authedProcedure, router } from '@/libs/trpc';
import { keyVaults } from '@/libs/trpc/middleware/keyVaults';
import { S3 } from '@/server/modules/S3';
import { createAsyncServerClient } from '@/server/routers/async';
import { getFullFileUrl } from '@/server/utils/files';
import {
  EvalDatasetRecord,
  EvalEvaluationStatus,
  InsertEvalDatasetRecord,
  RAGEvalDataSetItem,
  insertEvalDatasetRecordSchema,
  insertEvalDatasetsSchema,
  insertEvalEvaluationSchema,
} from '@/types/eval';

const ragEvalProcedure = authedProcedure.use(keyVaults).use(async (opts) => {
  const { ctx } = opts;

  return opts.next({
    ctx: {
      datasetModel: new EvalDatasetModel(ctx.userId),
      fileModel: new FileModel(serverDB, ctx.userId),
      datasetRecordModel: new EvalDatasetRecordModel(ctx.userId),
      evaluationModel: new EvalEvaluationModel(ctx.userId),
      evaluationRecordModel: new EvaluationRecordModel(ctx.userId),
      s3: new S3(),
    },
  });
});

export const ragEvalRouter = router({
  createDataset: ragEvalProcedure
    .input(
      z.object({
        description: z.string().optional(),
        knowledgeBaseId: z.string(),
        name: z.string(),
      }),
    )
    .mutation(async ({ input, ctx }) => {
      const data = await ctx.datasetModel.create({
        description: input.description,
        knowledgeBaseId: input.knowledgeBaseId,
        name: input.name,
      });

      return data?.id;
    }),

  getDatasets: ragEvalProcedure
    .input(z.object({ knowledgeBaseId: z.string() }))

    .query(async ({ ctx, input }): Promise<RAGEvalDataSetItem[]> => {
      return ctx.datasetModel.query(input.knowledgeBaseId);
    }),

  removeDataset: ragEvalProcedure
    .input(z.object({ id: z.number() }))
    .mutation(async ({ input, ctx }) => {
      return ctx.datasetModel.delete(input.id);
    }),

  updateDataset: ragEvalProcedure
    .input(
      z.object({
        id: z.number(),
        value: insertEvalDatasetsSchema.partial(),
      }),
    )
    .mutation(async ({ input, ctx }) => {
      return ctx.datasetModel.update(input.id, input.value);
    }),

  // Dataset Item operations
  createDatasetRecords: ragEvalProcedure
    .input(
      z.object({
        datasetId: z.number(),
        question: z.string(),
        ideal: z.string().optional(),
        referenceFiles: z.array(z.string()).optional(),
        metadata: z.record(z.unknown()).optional(),
      }),
    )
    .mutation(async ({ input, ctx }) => {
      const data = await ctx.datasetRecordModel.create(input);
      return data?.id;
    }),

  getDatasetRecords: ragEvalProcedure
    .input(z.object({ datasetId: z.number() }))
    .query(async ({ ctx, input }): Promise<EvalDatasetRecord[]> => {
      return ctx.datasetRecordModel.query(input.datasetId);
    }),

  removeDatasetRecords: ragEvalProcedure
    .input(z.object({ id: z.number() }))
    .mutation(async ({ input, ctx }) => {
      return ctx.datasetRecordModel.delete(input.id);
    }),

  updateDatasetRecords: ragEvalProcedure
    .input(
      z.object({
        id: z.number(),
        value: z
          .object({
            question: z.string(),
            ideal: z.string(),
            referenceFiles: z.array(z.string()),
            metadata: z.record(z.unknown()),
          })
          .partial(),
      }),
    )
    .mutation(async ({ input, ctx }) => {
      return ctx.datasetRecordModel.update(input.id, input.value);
    }),

  importDatasetRecords: ragEvalProcedure
    .input(
      z.object({
        datasetId: z.number(),
        pathname: z.string(),
      }),
    )
    .mutation(async ({ input, ctx }) => {
      const dataStr = await ctx.s3.getFileContent(input.pathname);
      const items = JSONL.parse<InsertEvalDatasetRecord>(dataStr);

      insertEvalDatasetRecordSchema.array().parse(items);

      const data = await Promise.all(
        items.map(async ({ referenceFiles, question, ideal }) => {
          const files = typeof referenceFiles === 'string' ? [referenceFiles] : referenceFiles;

          let fileIds: string[] | undefined = undefined;

          if (files) {
            const items = await ctx.fileModel.findByNames(files);

            fileIds = items.map((item) => item.id);
          }

          return {
            question,
            ideal,
            referenceFiles: fileIds,
            datasetId: input.datasetId,
          };
        }),
      );

      return ctx.datasetRecordModel.batchCreate(data);
    }),

  // Evaluation operations
  startEvaluationTask: ragEvalProcedure
    .input(z.object({ id: z.number() }))
    .mutation(async ({ input, ctx }) => {
      // Start evaluation task
      const evaluation = await ctx.evaluationModel.findById(input.id);

      if (!evaluation) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'Evaluation not found' });
      }

      // create evaluation records by dataset records
      const datasetRecords = await ctx.datasetRecordModel.findByDatasetId(evaluation.datasetId);

      if (datasetRecords.length === 0) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'Dataset record is empty' });
      }

      const evalRecords = await ctx.evaluationRecordModel.batchCreate(
        datasetRecords.map((record) => ({
          evaluationId: input.id,
          datasetRecordId: record.id,
          question: record.question!,
          ideal: record.ideal,
          status: EvalEvaluationStatus.Pending,
          embeddingModel: DEFAULT_EMBEDDING_MODEL,
          languageModel: DEFAULT_CHAT_MODEL,
        })),
      );

      const asyncCaller = await createAsyncServerClient(ctx.userId, ctx.jwtPayload);

      await ctx.evaluationModel.update(input.id, { status: EvalEvaluationStatus.Processing });
      try {
        await pMap(
          evalRecords,
          async (record) => {
            asyncCaller.ragEval.runRecordEvaluation
              .mutate({ evalRecordId: record.id })
              .catch(async (e) => {
                await ctx.evaluationModel.update(input.id, { status: EvalEvaluationStatus.Error });

                throw new TRPCError({
                  code: 'BAD_GATEWAY',
                  message: `[ASYNC_TASK] Failed to start evaluation task: ${e.message}`,
                });
              });
          },
          {
            concurrency: 30,
          },
        );

        return { success: true };
      } catch (e) {
        console.error('[startEvaluationTask]:', e);

        await ctx.evaluationModel.update(input.id, { status: EvalEvaluationStatus.Error });

        return { success: false };
      }
    }),

  checkEvaluationStatus: ragEvalProcedure
    .input(z.object({ id: z.number() }))
    .query(async ({ input, ctx }) => {
      const evaluation = await ctx.evaluationModel.findById(input.id);

      if (!evaluation) {
        throw new TRPCError({ code: 'BAD_REQUEST', message: 'Evaluation not found' });
      }

      const records = await ctx.evaluationRecordModel.findByEvaluationId(input.id);

      const isSuccess = records.every((record) => record.status === EvalEvaluationStatus.Success);

      if (isSuccess) {
        // 将结果上传到 S3

        const evalRecords = records.map((record) => ({
          question: record.question,
          context: record.context,
          answer: record.answer,
          ground_truth: record.ideal,
        }));
        const date = dayjs().format('YYYY-MM-DD-HH-mm');
        const filename = `${date}-eval_${evaluation.id}-${evaluation.name}.jsonl`;
        const path = `rag_eval_records/${filename}`;

        await ctx.s3.uploadContent(path, JSONL.stringify(evalRecords));

        // 保存数据
        await ctx.evaluationModel.update(input.id, {
          status: EvalEvaluationStatus.Success,
          evalRecordsUrl: await getFullFileUrl(path),
        });
      }

      return { success: isSuccess };
    }),
  createEvaluation: ragEvalProcedure
    .input(insertEvalEvaluationSchema)
    .mutation(async ({ input, ctx }) => {
      const data = await ctx.evaluationModel.create({
        description: input.description,
        knowledgeBaseId: input.knowledgeBaseId,
        datasetId: input.datasetId,
        name: input.name,
      });

      return data?.id;
    }),

  removeEvaluation: ragEvalProcedure
    .input(z.object({ id: z.number() }))
    .mutation(async ({ input, ctx }) => {
      return ctx.evaluationModel.delete(input.id);
    }),

  getEvaluationList: ragEvalProcedure
    .input(z.object({ knowledgeBaseId: z.string() }))
    .query(async ({ ctx, input }) => {
      return ctx.evaluationModel.queryByKnowledgeBaseId(input.knowledgeBaseId);
    }),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/lambda/chunk.ts
================================================================================

import { inArray } from 'drizzle-orm/expressions';
import { z } from 'zod';

import { DEFAULT_EMBEDDING_MODEL } from '@/const/settings';
import { knowledgeBaseFiles } from '@/database/schemas';
import { serverDB } from '@/database/server';
import { AsyncTaskModel } from '@/database/server/models/asyncTask';
import { ChunkModel } from '@/database/server/models/chunk';
import { EmbeddingModel } from '@/database/server/models/embedding';
import { FileModel } from '@/database/server/models/file';
import { MessageModel } from '@/database/server/models/message';
import { ModelProvider } from '@/libs/agent-runtime';
import { authedProcedure, router } from '@/libs/trpc';
import { keyVaults } from '@/libs/trpc/middleware/keyVaults';
import { initAgentRuntimeWithUserPayload } from '@/server/modules/AgentRuntime';
import { ChunkService } from '@/server/services/chunk';
import { SemanticSearchSchema } from '@/types/rag';

const chunkProcedure = authedProcedure.use(keyVaults).use(async (opts) => {
  const { ctx } = opts;

  return opts.next({
    ctx: {
      asyncTaskModel: new AsyncTaskModel(serverDB, ctx.userId),
      chunkModel: new ChunkModel(serverDB, ctx.userId),
      chunkService: new ChunkService(ctx.userId),
      embeddingModel: new EmbeddingModel(serverDB, ctx.userId),
      fileModel: new FileModel(serverDB, ctx.userId),
      messageModel: new MessageModel(serverDB, ctx.userId),
    },
  });
});

export const chunkRouter = router({
  createEmbeddingChunksTask: chunkProcedure
    .input(
      z.object({
        id: z.string(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const asyncTaskId = await ctx.chunkService.asyncEmbeddingFileChunks(input.id, ctx.jwtPayload);

      return { id: asyncTaskId, success: true };
    }),

  createParseFileTask: chunkProcedure
    .input(
      z.object({
        id: z.string(),
        skipExist: z.boolean().optional(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const asyncTaskId = await ctx.chunkService.asyncParseFileToChunks(
        input.id,
        ctx.jwtPayload,
        input.skipExist,
      );

      return { id: asyncTaskId, success: true };
    }),

  getChunksByFileId: chunkProcedure
    .input(
      z.object({
        cursor: z.number().nullish(),
        id: z.string(),
      }),
    )
    .query(async ({ ctx, input }) => {
      return {
        items: await ctx.chunkModel.findByFileId(input.id, input.cursor || 0),
        nextCursor: input.cursor ? input.cursor + 1 : 1,
      };
    }),

  retryParseFileTask: chunkProcedure
    .input(
      z.object({
        id: z.string(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      const result = await ctx.fileModel.findById(input.id);

      if (!result) return;

      // 1. delete the previous task if exist
      if (result.chunkTaskId) {
        await ctx.asyncTaskModel.delete(result.chunkTaskId);
      }

      // 2. create a new asyncTask for chunking
      const asyncTaskId = await ctx.chunkService.asyncParseFileToChunks(input.id, ctx.jwtPayload);

      return { id: asyncTaskId, success: true };
    }),

  semanticSearch: chunkProcedure
    .input(
      z.object({
        fileIds: z.array(z.string()).optional(),
        model: z.string().default(DEFAULT_EMBEDDING_MODEL),
        query: z.string(),
      }),
    )
    .mutation(async ({ ctx, input }) => {
      console.time('embedding');
      const agentRuntime = await initAgentRuntimeWithUserPayload(
        ModelProvider.OpenAI,
        ctx.jwtPayload,
      );

      const embeddings = await agentRuntime.embeddings({
        dimensions: 1024,
        input: input.query,
        model: input.model,
      });
      console.timeEnd('embedding');

      return ctx.chunkModel.semanticSearch({
        embedding: embeddings![0],
        fileIds: input.fileIds,
        query: input.query,
      });
    }),

  semanticSearchForChat: chunkProcedure
    .input(SemanticSearchSchema)
    .mutation(async ({ ctx, input }) => {
      const item = await ctx.messageModel.findMessageQueriesById(input.messageId);
      let embedding: number[];
      let ragQueryId: string;

      // if there is no message rag or it's embeddings, then we need to create one
      if (!item || !item.embeddings) {
        // TODO: need to support customize
        const agentRuntime = await initAgentRuntimeWithUserPayload(
          ModelProvider.OpenAI,
          ctx.jwtPayload,
        );

        const embeddings = await agentRuntime.embeddings({
          dimensions: 1024,
          input: input.rewriteQuery,
          model: input.model || DEFAULT_EMBEDDING_MODEL,
        });

        embedding = embeddings![0];
        const embeddingsId = await ctx.embeddingModel.create({
          embeddings: embedding,
          model: input.model,
        });

        const result = await ctx.messageModel.createMessageQuery({
          embeddingsId,
          messageId: input.messageId,
          rewriteQuery: input.rewriteQuery,
          userQuery: input.userQuery,
        });

        ragQueryId = result.id;
      } else {
        embedding = item.embeddings;
        ragQueryId = item.id;
      }

      console.time('semanticSearch');
      let finalFileIds = input.fileIds ?? [];

      if (input.knowledgeIds && input.knowledgeIds.length > 0) {
        const knowledgeFiles = await serverDB.query.knowledgeBaseFiles.findMany({
          where: inArray(knowledgeBaseFiles.knowledgeBaseId, input.knowledgeIds),
        });

        finalFileIds = knowledgeFiles.map((f) => f.fileId).concat(finalFileIds);
      }

      const chunks = await ctx.chunkModel.semanticSearchForChat({
        embedding,
        fileIds: finalFileIds,
        query: input.rewriteQuery,
      });
      console.timeEnd('semanticSearch');

      return { chunks, queryId: ragQueryId };
    }),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/routers/lambda/index.ts
================================================================================

/**
 * This file contains the root router of Lobe Chat tRPC-backend
 */
import { publicProcedure, router } from '@/libs/trpc';

import { agentRouter } from './agent';
import { chunkRouter } from './chunk';
import { fileRouter } from './file';
import { importerRouter } from './importer';
import { knowledgeBaseRouter } from './knowledgeBase';
import { messageRouter } from './message';
import { pluginRouter } from './plugin';
import { ragEvalRouter } from './ragEval';
import { sessionRouter } from './session';
import { sessionGroupRouter } from './sessionGroup';
import { threadRouter } from './thread';
import { topicRouter } from './topic';
import { userRouter } from './user';

export const lambdaRouter = router({
  agent: agentRouter,
  chunk: chunkRouter,
  file: fileRouter,
  healthcheck: publicProcedure.query(() => "i'm live!"),
  importer: importerRouter,
  knowledgeBase: knowledgeBaseRouter,
  message: messageRouter,
  plugin: pluginRouter,
  ragEval: ragEvalRouter,
  session: sessionRouter,
  sessionGroup: sessionGroupRouter,
  thread: threadRouter,
  topic: topicRouter,
  user: userRouter,
});

export type LambdaRouter = typeof lambdaRouter;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/globalConfig/index.ts
================================================================================

import { appEnv, getAppConfig } from '@/config/app';
import { authEnv } from '@/config/auth';
import { fileEnv } from '@/config/file';
import { langfuseEnv } from '@/config/langfuse';
import { enableNextAuth } from '@/const/auth';
import { parseSystemAgent } from '@/server/globalConfig/parseSystemAgent';
import { GlobalServerConfig } from '@/types/serverConfig';

import { genServerLLMConfig } from './genServerLLMConfig';
import { parseAgentConfig } from './parseDefaultAgent';

export const getServerGlobalConfig = () => {
  const { ACCESS_CODES, DEFAULT_AGENT_CONFIG } = getAppConfig();

  const config: GlobalServerConfig = {
    defaultAgent: {
      config: parseAgentConfig(DEFAULT_AGENT_CONFIG),
    },
    enableUploadFileToServer: !!fileEnv.S3_SECRET_ACCESS_KEY,
    enabledAccessCode: ACCESS_CODES?.length > 0,
    enabledOAuthSSO: enableNextAuth,
    languageModel: genServerLLMConfig({
      azure: {
        enabledKey: 'ENABLED_AZURE_OPENAI',
        withDeploymentName: true,
      },
      bedrock: {
        enabledKey: 'ENABLED_AWS_BEDROCK',
        modelListKey: 'AWS_BEDROCK_MODEL_LIST',
      },
      giteeai: {
        enabledKey: 'ENABLED_GITEE_AI',
        modelListKey: 'GITEE_AI_MODEL_LIST',
      },
      ollama: {
        fetchOnClient: !process.env.OLLAMA_PROXY_URL,
      },
    }),
    oAuthSSOProviders: authEnv.NEXT_AUTH_SSO_PROVIDERS.trim().split(/[,，]/),
    systemAgent: parseSystemAgent(appEnv.SYSTEM_AGENT),
    telemetry: {
      langfuse: langfuseEnv.ENABLE_LANGFUSE,
    },
  };

  return config;
};

export const getServerDefaultAgentConfig = () => {
  const { DEFAULT_AGENT_CONFIG } = getAppConfig();

  return parseAgentConfig(DEFAULT_AGENT_CONFIG) || {};
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/modules/AgentRuntime/index.ts
================================================================================

import { getLLMConfig } from '@/config/llm';
import { JWTPayload } from '@/const/auth';
import { INBOX_SESSION_ID } from '@/const/session';
import {
  LOBE_CHAT_OBSERVATION_ID,
  LOBE_CHAT_TRACE_ID,
  TracePayload,
  TraceTagMap,
} from '@/const/trace';
import { AgentRuntime, ChatStreamPayload, ModelProvider } from '@/libs/agent-runtime';
import { TraceClient } from '@/libs/traces';

import apiKeyManager from './apiKeyManager';

export interface AgentChatOptions {
  enableTrace?: boolean;
  provider: string;
  trace?: TracePayload;
}

/**
 * Retrieves the options object from environment and apikeymanager
 * based on the provider and payload.
 *
 * @param provider - The model provider.
 * @param payload - The JWT payload.
 * @returns The options object.
 */
const getLlmOptionsFromPayload = (provider: string, payload: JWTPayload) => {
  const llmConfig = getLLMConfig() as Record<string, any>;

  switch (provider) {
    default: {
      let upperProvider = provider.toUpperCase();

      if (!(`${upperProvider}_API_KEY` in llmConfig)) {
        upperProvider = ModelProvider.OpenAI.toUpperCase(); // Use OpenAI options as default
      }

      const apiKey = apiKeyManager.pick(payload?.apiKey || llmConfig[`${upperProvider}_API_KEY`]);
      const baseURL = payload?.endpoint || process.env[`${upperProvider}_PROXY_URL`];

      return baseURL ? { apiKey, baseURL } : { apiKey };
    }

    case ModelProvider.Ollama: {
      const baseURL = payload?.endpoint || process.env.OLLAMA_PROXY_URL;

      return { baseURL };
    }

    case ModelProvider.Azure: {
      const { AZURE_API_KEY, AZURE_API_VERSION, AZURE_ENDPOINT } = llmConfig;
      const apikey = apiKeyManager.pick(payload?.apiKey || AZURE_API_KEY);
      const endpoint = payload?.endpoint || AZURE_ENDPOINT;
      const apiVersion = payload?.azureApiVersion || AZURE_API_VERSION;
      return { apiVersion, apikey, endpoint };
    }

    case ModelProvider.Bedrock: {
      const { AWS_SECRET_ACCESS_KEY, AWS_ACCESS_KEY_ID, AWS_REGION, AWS_SESSION_TOKEN } = llmConfig;
      let accessKeyId: string | undefined = AWS_ACCESS_KEY_ID;
      let accessKeySecret: string | undefined = AWS_SECRET_ACCESS_KEY;
      let region = AWS_REGION;
      let sessionToken: string | undefined = AWS_SESSION_TOKEN;
      // if the payload has the api key, use user
      if (payload.apiKey) {
        accessKeyId = payload?.awsAccessKeyId;
        accessKeySecret = payload?.awsSecretAccessKey;
        sessionToken = payload?.awsSessionToken;
        region = payload?.awsRegion;
      }
      return { accessKeyId, accessKeySecret, region, sessionToken };
    }

    case ModelProvider.Cloudflare: {
      const { CLOUDFLARE_API_KEY, CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID } = llmConfig;

      const apiKey = apiKeyManager.pick(payload?.apiKey || CLOUDFLARE_API_KEY);
      const baseURLOrAccountID =
        payload.apiKey && payload.cloudflareBaseURLOrAccountID
          ? payload.cloudflareBaseURLOrAccountID
          : CLOUDFLARE_BASE_URL_OR_ACCOUNT_ID;

      return { apiKey, baseURLOrAccountID };
    }

    case ModelProvider.GiteeAI: {
      const { GITEE_AI_API_KEY } = llmConfig;

      const apiKey = apiKeyManager.pick(payload?.apiKey || GITEE_AI_API_KEY);

      return { apiKey };
    }

    case ModelProvider.Github: {
      const { GITHUB_TOKEN } = llmConfig;

      const apiKey = apiKeyManager.pick(payload?.apiKey || GITHUB_TOKEN);

      return { apiKey };
    }
  }
};

/**
 * Initializes the agent runtime with the user payload in backend
 * @param provider - The provider name.
 * @param payload - The JWT payload.
 * @param params
 * @returns A promise that resolves when the agent runtime is initialized.
 */
export const initAgentRuntimeWithUserPayload = (
  provider: string,
  payload: JWTPayload,
  params: any = {},
) => {
  return AgentRuntime.initializeWithProviderOptions(provider, {
    [provider]: { ...getLlmOptionsFromPayload(provider, payload), ...params },
  });
};

export const createTraceOptions = (
  payload: ChatStreamPayload,
  { trace: tracePayload, provider }: AgentChatOptions,
) => {
  const { messages, model, tools, ...parameters } = payload;
  // create a trace to monitor the completion
  const traceClient = new TraceClient();
  const trace = traceClient.createTrace({
    id: tracePayload?.traceId,
    input: messages,
    metadata: { provider },
    name: tracePayload?.traceName,
    sessionId: `${tracePayload?.sessionId || INBOX_SESSION_ID}@${tracePayload?.topicId || 'start'}`,
    tags: tracePayload?.tags,
    userId: tracePayload?.userId,
  });

  const generation = trace?.generation({
    input: messages,
    metadata: { provider },
    model,
    modelParameters: parameters as any,
    name: `Chat Completion (${provider})`,
    startTime: new Date(),
  });

  return {
    callback: {
      experimental_onToolCall: async () => {
        trace?.update({
          tags: [...(tracePayload?.tags || []), TraceTagMap.ToolsCall],
        });
      },

      onCompletion: async (completion: string) => {
        generation?.update({
          endTime: new Date(),
          metadata: { provider, tools },
          output: completion,
        });

        trace?.update({ output: completion });
      },

      onFinal: async () => {
        await traceClient.shutdownAsync();
      },

      onStart: () => {
        generation?.update({ completionStartTime: new Date() });
      },
    },
    headers: {
      [LOBE_CHAT_OBSERVATION_ID]: generation?.id,
      [LOBE_CHAT_TRACE_ID]: trace?.id,
    },
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/modules/ContentChunk/index.ts
================================================================================

import { ChunkingLoader } from 'src/libs/langchain';
import { Strategy } from 'unstructured-client/sdk/models/shared';

import { NewChunkItem, NewUnstructuredChunkItem } from '@/database/schemas';
import { ChunkingStrategy, Unstructured } from '@/libs/unstructured';

export interface ChunkContentParams {
  content: Uint8Array;
  fileType: string;
  filename: string;
  mode?: 'fast' | 'hi-res';
}

interface ChunkResult {
  chunks: NewChunkItem[];
  unstructuredChunks?: NewUnstructuredChunkItem[];
}

export class ContentChunk {
  private unstructuredClient: Unstructured;
  private langchainClient: ChunkingLoader;

  constructor() {
    this.unstructuredClient = new Unstructured();
    this.langchainClient = new ChunkingLoader();
  }

  isUsingUnstructured(params: ChunkContentParams) {
    return params.fileType === 'application/pdf' && params.mode === 'hi-res';
  }

  async chunkContent(params: ChunkContentParams): Promise<ChunkResult> {
    if (this.isUsingUnstructured(params))
      return await this.chunkByUnstructured(params.filename, params.content);

    return await this.chunkByLangChain(params.filename, params.content);
  }

  private chunkByUnstructured = async (
    filename: string,
    content: Uint8Array,
  ): Promise<ChunkResult> => {
    const result = await this.unstructuredClient.partition({
      chunkingStrategy: ChunkingStrategy.ByPage,
      fileContent: content,
      filename,
      strategy: Strategy.Auto,
    });

    // after finish partition, we need to filter out some elements
    const documents = result.compositeElements
      .filter((e) => !new Set(['PageNumber', 'Footer']).has(e.type))
      .map((item, index): NewChunkItem => {
        const {
          text_as_html,
          page_number,
          page_name,
          image_mime_type,
          image_base64,
          parent_id,
          languages,
          coordinates,
        } = item.metadata;

        return {
          id: item.element_id,
          index,
          metadata: {
            coordinates,
            image_base64,
            image_mime_type,
            languages,
            page_name,
            page_number,
            parent_id,
            text_as_html,
          },
          text: item.text,
          type: item.type,
        };
      });

    const chunks = result.originElements
      .filter((e) => !new Set(['PageNumber', 'Footer']).has(e.type))
      .map((item, index): NewUnstructuredChunkItem => {
        const {
          text_as_html,
          page_number,
          page_name,
          image_mime_type,
          image_base64,
          parent_id,
          languages,
          coordinates,
        } = item.metadata;

        return {
          compositeId: item.compositeId,
          id: item.element_id,
          index,
          metadata: {
            coordinates,
            image_base64,
            image_mime_type,
            languages,
            page_name,
            page_number,
            text_as_html,
          },
          parentId: parent_id,
          text: item.text,
          type: item.type,
        };
      });

    return { chunks: documents, unstructuredChunks: chunks };
  };

  private chunkByLangChain = async (
    filename: string,
    content: Uint8Array,
  ): Promise<ChunkResult> => {
    const res = await this.langchainClient.partitionContent(filename, content);

    const documents = res.map((item, index) => ({
      id: item.id,
      index,
      metadata: item.metadata,
      text: item.pageContent,
      type: 'LangChainElement',
    }));

    return { chunks: documents };
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/services/discover/index.ts
================================================================================

import { cloneDeep, isString, merge, uniqBy } from 'lodash-es';
import pMap from 'p-map';

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';
import {
  DEFAULT_DISCOVER_ASSISTANT_ITEM,
  DEFAULT_DISCOVER_MODEL_ITEM,
  DEFAULT_DISCOVER_PLUGIN_ITEM,
  DEFAULT_DISCOVER_PROVIDER_ITEM,
} from '@/const/discover';
import { DEFAULT_LANG } from '@/const/locale';
import { Locales } from '@/locales/resources';
import { AssistantStore } from '@/server/modules/AssistantStore';
import { PluginStore } from '@/server/modules/PluginStore';
import {
  AssistantCategory,
  DiscoverAssistantItem,
  DiscoverModelItem,
  DiscoverPlugintem,
  DiscoverProviderItem,
  PluginCategory,
} from '@/types/discover';
import { getToolManifest } from '@/utils/toolManifest';

const revalidate: number = 3600;

export class DiscoverService {
  assistantStore = new AssistantStore();
  pluginStore = new PluginStore();

  // Assistants
  searchAssistant = async (locale: Locales, keywords: string): Promise<DiscoverAssistantItem[]> => {
    const list = await this.getAssistantList(locale);
    return list.filter((item) => {
      return [item.author, item.meta.title, item.meta.description, item.meta?.tags]
        .flat()
        .filter(Boolean)
        .join(',')
        .toLowerCase()
        .includes(decodeURIComponent(keywords).toLowerCase());
    });
  };

  getAssistantCategory = async (
    locale: Locales,
    category: AssistantCategory,
  ): Promise<DiscoverAssistantItem[]> => {
    const list = await this.getAssistantList(locale);
    return list.filter((item) => item.meta.category === category);
  };

  getAssistantList = async (locale: Locales): Promise<DiscoverAssistantItem[]> => {
    const json = await this.assistantStore.getAgentIndex(locale, revalidate);

    // @ts-expect-error 目前类型不一致，未来要统一
    return json.agents;
  };

  getAssistantById = async (
    locale: Locales,
    identifier: string,
  ): Promise<DiscoverAssistantItem | undefined> => {
    let res = await fetch(this.assistantStore.getAgentUrl(identifier, locale), {
      next: { revalidate: 12 * revalidate },
    });

    if (!res.ok) {
      res = await fetch(this.assistantStore.getAgentUrl(DEFAULT_LANG), {
        next: { revalidate: 12 * revalidate },
      });
    }

    if (!res.ok) return;

    let assistant = await res.json();

    if (!assistant) return;

    assistant = merge(cloneDeep(DEFAULT_DISCOVER_ASSISTANT_ITEM), assistant);

    const categoryItems = await this.getAssistantCategory(
      locale,
      assistant.meta.category || AssistantCategory.General,
    );

    assistant = {
      ...assistant,
      suggestions: categoryItems
        .filter((item) => item.identifier !== assistant.identifier)
        .slice(0, 5) as any,
    };

    return assistant;
  };

  getAssistantByIds = async (
    locale: Locales,
    identifiers: string[],
  ): Promise<DiscoverAssistantItem[]> => {
    const list = await pMap(
      identifiers,
      async (identifier) => this.getAssistantById(locale, identifier),
      {
        concurrency: 5,
      },
    );

    return list.filter(Boolean) as DiscoverAssistantItem[];
  };

  // Tools

  searchPlugin = async (locale: Locales, keywords: string): Promise<DiscoverPlugintem[]> => {
    const list = await this.getPluginList(locale);
    return list.filter((item) => {
      return [item.author, item.meta.title, item.meta.description, item.meta?.tags]
        .flat()
        .filter(Boolean)
        .join(',')
        .toLowerCase()
        .includes(decodeURIComponent(keywords).toLowerCase());
    });
  };

  getPluginCategory = async (
    locale: Locales,
    category: PluginCategory,
  ): Promise<DiscoverPlugintem[]> => {
    const list = await this.getPluginList(locale);
    return list.filter((item) => item.meta.category === category);
  };

  getPluginList = async (locale: Locales): Promise<DiscoverPlugintem[]> => {
    let res = await fetch(this.pluginStore.getPluginIndexUrl(locale), {
      next: { revalidate: 12 * revalidate },
    });

    if (!res.ok) {
      res = await fetch(this.pluginStore.getPluginIndexUrl(DEFAULT_LANG), {
        next: { revalidate: 12 * revalidate },
      });
    }

    if (!res.ok) return [];

    const json = await res.json();

    return json.plugins;
  };

  getPluginByIds = async (locale: Locales, identifiers: string[]): Promise<DiscoverPlugintem[]> => {
    let list = await pMap(
      identifiers,
      async (identifier) => this.getPluginById(locale, identifier),
      {
        concurrency: 5,
      },
    );

    return list.filter(Boolean) as DiscoverPlugintem[];
  };

  getPluginById = async (
    locale: Locales,
    identifier: string,
    withManifest?: boolean,
  ): Promise<DiscoverPlugintem | undefined> => {
    const list = await this.getPluginList(locale);
    let plugin = list.find((item) => item.identifier === identifier) as DiscoverPlugintem;

    if (!plugin) return;

    plugin = merge(cloneDeep(DEFAULT_DISCOVER_PLUGIN_ITEM), plugin);

    if (withManifest) {
      const manifest = isString(plugin?.manifest)
        ? await getToolManifest(plugin.manifest)
        : plugin?.manifest;

      plugin = {
        ...plugin,
        manifest,
      } as DiscoverPlugintem;
    }

    const categoryItems = await this.getPluginCategory(
      locale,
      plugin.meta.category || PluginCategory.Tools,
    );

    plugin = {
      ...plugin,
      suggestions: categoryItems
        .filter((item) => item.identifier !== plugin.identifier)
        .slice(0, 5) as any,
    } as DiscoverPlugintem;

    return plugin;
  };

  // Providers

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  getProviderList = async (_locale: Locales): Promise<DiscoverProviderItem[]> => {
    const list = DEFAULT_MODEL_PROVIDER_LIST.filter((item) => item.chatModels.length > 0);
    return list.map((item) => {
      const provider = {
        identifier: item.id,
        meta: {
          ...item,
          title: item.name,
        },
        models: item.chatModels.map((item) => item.id),
      };
      return merge(cloneDeep(DEFAULT_DISCOVER_PROVIDER_ITEM), provider) as DiscoverProviderItem;
    });
  };

  searchProvider = async (locale: Locales, keywords: string): Promise<DiscoverProviderItem[]> => {
    const list = await this.getProviderList(locale);
    return list.filter((item) => {
      return [item.identifier, item.meta.title]
        .filter(Boolean)
        .join(',')
        .toLowerCase()
        .includes(decodeURIComponent(keywords).toLowerCase());
    });
  };

  getProviderById = async (
    locale: Locales,
    id: string,
  ): Promise<DiscoverProviderItem | undefined> => {
    const list = await this.getProviderList(locale);
    let provider = list.find((item) => item.identifier === id);

    if (!provider) return;

    provider = {
      ...provider,
      suggestions: list
        .filter((item) => item.identifier !== provider?.identifier)
        .slice(0, 5) as any,
    };

    return merge(cloneDeep(DEFAULT_DISCOVER_PROVIDER_ITEM), provider) as DiscoverProviderItem;
  };

  getProviderByIds = async (
    locale: Locales,
    identifiers: string[],
  ): Promise<DiscoverProviderItem[]> => {
    const list = await pMap(
      identifiers,
      async (identifier) => this.getProviderById(locale, identifier),
      {
        concurrency: 5,
      },
    );

    return list.filter(Boolean) as DiscoverProviderItem[];
  };

  // Models

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  private _getModelList = async (locale: Locales): Promise<DiscoverModelItem[]> => {
    const list = DEFAULT_MODEL_PROVIDER_LIST.filter((item) => item.chatModels.length > 0);
    const providers = await this.getProviderList(locale);

    return list.flatMap((provider) => {
      return provider.chatModels.map((item) => {
        const ids = item.id.split('/')[1] || item.id;
        const providerIds = providers
          .filter((provider) => provider.models.join('').includes(ids))
          .map((provider) => provider.identifier);
        const model = {
          identifier: item.id,
          meta: {
            ...item,
            category: provider.id,
            title: item.displayName || item.id,
          },
          providers: providerIds,
          suggestions: [],
        };
        return merge(cloneDeep(DEFAULT_DISCOVER_MODEL_ITEM), model) as DiscoverModelItem;
      });
    });
  };

  getModelList = async (locale: Locales): Promise<DiscoverModelItem[]> => {
    const list = await this._getModelList(locale);

    return uniqBy(list, (item) => {
      const ids = item.identifier.split('/');
      return ids[1] || item.identifier;
    });
  };

  searchModel = async (locale: Locales, keywords: string): Promise<DiscoverModelItem[]> => {
    const list = await this.getModelList(locale);
    return list.filter((item) => {
      return [item.identifier, item.meta.title, item.meta.description, item.providers]
        .flat()
        .filter(Boolean)
        .join(',')
        .toLowerCase()
        .includes(decodeURIComponent(keywords).toLowerCase());
    });
  };

  getModelCategory = async (locale: Locales, category: string): Promise<DiscoverModelItem[]> => {
    const list = await this._getModelList(locale);
    return list.filter((item) => item.meta.category === category);
  };

  getModelById = async (locale: Locales, id: string): Promise<DiscoverModelItem | undefined> => {
    const list = await this.getModelList(locale);
    let model = list.find((item) => item.identifier === id);

    if (!model) return;

    const categoryItems = model?.meta?.category
      ? await this.getModelCategory(locale, model.meta.category)
      : [];

    model = {
      ...model,
      suggestions: categoryItems
        .filter((item) => item.identifier !== model?.identifier)
        .slice(0, 5) as any,
    };

    return merge(cloneDeep(DEFAULT_DISCOVER_MODEL_ITEM), model);
  };

  getModelByIds = async (locale: Locales, identifiers: string[]): Promise<DiscoverModelItem[]> => {
    const list = await pMap(
      identifiers,
      async (identifier) => this.getModelById(locale, identifier),
      {
        concurrency: 5,
      },
    );
    return list.filter(Boolean) as DiscoverModelItem[];
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/server/services/chunk/index.ts
================================================================================

import { JWTPayload } from '@/const/auth';
import { serverDB } from '@/database/server';
import { AsyncTaskModel } from '@/database/server/models/asyncTask';
import { FileModel } from '@/database/server/models/file';
import { ChunkContentParams, ContentChunk } from '@/server/modules/ContentChunk';
import { createAsyncServerClient } from '@/server/routers/async';
import {
  AsyncTaskError,
  AsyncTaskErrorType,
  AsyncTaskStatus,
  AsyncTaskType,
} from '@/types/asyncTask';

export class ChunkService {
  private userId: string;
  private chunkClient: ContentChunk;
  private fileModel: FileModel;
  private asyncTaskModel: AsyncTaskModel;

  constructor(userId: string) {
    this.userId = userId;

    this.chunkClient = new ContentChunk();

    this.fileModel = new FileModel(serverDB, userId);
    this.asyncTaskModel = new AsyncTaskModel(serverDB, userId);
  }

  async chunkContent(params: ChunkContentParams) {
    return this.chunkClient.chunkContent(params);
  }

  async asyncEmbeddingFileChunks(fileId: string, payload: JWTPayload) {
    const result = await this.fileModel.findById(fileId);

    if (!result) return;

    // 1. create a asyncTaskId
    const asyncTaskId = await this.asyncTaskModel.create({
      status: AsyncTaskStatus.Pending,
      type: AsyncTaskType.Embedding,
    });

    await this.fileModel.update(fileId, { embeddingTaskId: asyncTaskId });

    const asyncCaller = await createAsyncServerClient(this.userId, payload);

    // trigger embedding task asynchronously
    try {
      await asyncCaller.file.embeddingChunks.mutate({ fileId, taskId: asyncTaskId });
    } catch (e) {
      console.error('[embeddingFileChunks] error:', e);

      await this.asyncTaskModel.update(asyncTaskId, {
        error: new AsyncTaskError(
          AsyncTaskErrorType.TaskTriggerError,
          'trigger chunk embedding async task error. Please make sure the APP_URL is available from your server. You can check the proxy config or WAF blocking',
        ),
        status: AsyncTaskStatus.Error,
      });
    }

    return asyncTaskId;
  }

  /**
   * parse file to chunks with async task
   */
  async asyncParseFileToChunks(fileId: string, payload: JWTPayload, skipExist?: boolean) {
    const result = await this.fileModel.findById(fileId);

    if (!result) return;

    // skip if already exist chunk tasks
    if (skipExist && result.chunkTaskId) return;

    // 1. create a asyncTaskId
    const asyncTaskId = await this.asyncTaskModel.create({
      status: AsyncTaskStatus.Processing,
      type: AsyncTaskType.Chunking,
    });

    await this.fileModel.update(fileId, { chunkTaskId: asyncTaskId });

    const asyncCaller = await createAsyncServerClient(this.userId, payload);

    // trigger parse file task asynchronously
    asyncCaller.file.parseFileToChunks
      .mutate({ fileId: fileId, taskId: asyncTaskId })
      .catch(async (e) => {
        console.error('[ParseFileToChunks] error:', e);

        await this.asyncTaskModel.update(asyncTaskId, {
          error: new AsyncTaskError(
            AsyncTaskErrorType.TaskTriggerError,
            'trigger chunk embedding async task error. Please make sure the APP_URL is available from your server. You can check the proxy config or WAF blocking',
          ),
          status: AsyncTaskStatus.Error,
        });
      });

    return asyncTaskId;
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/isChunkingUnsupported.ts
================================================================================

export const isChunkingUnsupported = (fileType: string): boolean => {
  if (fileType.startsWith('image')) return true;
  if (fileType.startsWith('video')) return true;
  if (fileType.startsWith('audio')) return true;
  return false; // false doesn't mean supported, it means we don't know
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/toolManifest.ts
================================================================================

import { LobeChatPluginManifest, pluginManifestSchema } from '@lobehub/chat-plugin-sdk';

import { API_ENDPOINTS } from '@/services/_url';
import { OpenAIPluginManifest } from '@/types/openai/plugin';

const fetchJSON = async <T = any>(url: string, proxy = false): Promise<T> => {
  // 2. 发送请求
  let res: Response;
  try {
    res = await (proxy ? fetch(API_ENDPOINTS.proxy, { body: url, method: 'POST' }) : fetch(url));
  } catch {
    throw new TypeError('fetchError');
  }

  if (!res.ok) {
    throw new TypeError('fetchError');
  }

  let data;
  const contentType = res.headers.get('Content-Type');

  try {
    if (contentType === 'application/json') {
      data = await res.json();
    } else {
      const { default: YAML } = await import('yaml');

      const yaml = await res.text();
      data = YAML.parse(yaml);
    }
  } catch {
    throw new TypeError('urlError');
  }

  return data;
};

export const convertOpenAIManifestToLobeManifest = (
  data: OpenAIPluginManifest,
): LobeChatPluginManifest => {
  const manifest: LobeChatPluginManifest = {
    api: [],
    homepage: data.legal_info_url,
    identifier: data.name_for_model,
    meta: {
      avatar: data.logo_url,
      description: data.description_for_human,
      title: data.name_for_human,
    },
    openapi: data.api.url,
    systemRole: data.description_for_model,
    type: 'default',
    version: '1',
  };
  switch (data.auth.type) {
    case 'none': {
      break;
    }
    case 'service_http': {
      manifest.settings = {
        properties: {
          apiAuthKey: {
            default: data.auth.verification_tokens['openai'],
            description: 'API Key',
            format: 'password',
            title: 'API Key',
            type: 'string',
          },
        },
        type: 'object',
      };
      break;
    }
  }

  return manifest;
};

export const getToolManifest = async (
  url?: string,
  useProxy: boolean = false,
): Promise<LobeChatPluginManifest> => {
  // 1. valid plugin
  if (!url) {
    throw new TypeError('noManifest');
  }

  // 2. 发送请求

  let data = await fetchJSON<LobeChatPluginManifest>(url, useProxy);

  // @ts-ignore
  // if there is a description_for_model, it is an OpenAI plugin
  // we need convert to lobe plugin
  if (data['description_for_model']) {
    data = convertOpenAIManifestToLobeManifest(data as any);
  }

  // 3. 校验插件文件格式规范
  const parser = pluginManifestSchema.safeParse(data);

  if (!parser.success) {
    throw new TypeError('manifestInvalid', { cause: parser.error });
  }

  // 4. if exist OpenAPI api, merge the OpenAPIs to api
  if (parser.data.openapi) {
    const openapiJson = await fetchJSON(parser.data.openapi, useProxy);

    try {
      const { OpenAPIConvertor } = await import('@lobehub/chat-plugin-sdk/openapi');

      const convertor = new OpenAPIConvertor(openapiJson);
      const openAPIs = await convertor.convertOpenAPIToPluginSchema();
      data.api = [...data.api, ...openAPIs];

      data.settings = await convertor.convertAuthToSettingsSchema(data.settings);
    } catch (error) {
      throw new TypeError('openAPIInvalid', { cause: error });
    }
  }

  return data;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/localStorage.ts
================================================================================

const PREV_KEY = 'LOBE_GLOBAL';

// LOBE_PREFERENCE for userStore
// LOBE_GLOBAL_PREFERENCE for globalStore
type StorageKey = 'LOBE_PREFERENCE' | 'LOBE_SYSTEM_STATUS';

export class AsyncLocalStorage<State> {
  private storageKey: StorageKey;

  constructor(storageKey: StorageKey) {
    this.storageKey = storageKey;

    // skip server side rendering
    if (typeof window === 'undefined') return;

    // migrate old data
    if (localStorage.getItem(PREV_KEY)) {
      const data = JSON.parse(localStorage.getItem(PREV_KEY) || '{}');

      const preference = data.state.preference;

      if (data.state?.preference) {
        localStorage.setItem('LOBE_PREFERENCE', JSON.stringify(preference));
      }
      localStorage.removeItem(PREV_KEY);
    }
  }

  async saveToLocalStorage(state: object) {
    const data = await this.getFromLocalStorage();

    localStorage.setItem(this.storageKey, JSON.stringify({ ...data, ...state }));
  }

  async getFromLocalStorage(key: StorageKey = this.storageKey): Promise<State> {
    return JSON.parse(localStorage.getItem(key) || '{}');
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/parseModels.ts
================================================================================

import { produce } from 'immer';

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
import { ChatModelCard } from '@/types/llm';

/**
 * Parse model string to add or remove models.
 */
export const parseModelString = (modelString: string = '', withDeploymentName = false) => {
  let models: ChatModelCard[] = [];
  let removeAll = false;
  const removedModels: string[] = [];
  const modelNames = modelString.split(/[,，]/).filter(Boolean);

  for (const item of modelNames) {
    const disable = item.startsWith('-');
    const nameConfig = item.startsWith('+') || item.startsWith('-') ? item.slice(1) : item;
    const [idAndDisplayName, ...capabilities] = nameConfig.split('<');
    let [id, displayName] = idAndDisplayName.split('=');

    let deploymentName: string | undefined;

    if (withDeploymentName) {
      [id, deploymentName] = id.split('->');
      if (!deploymentName) deploymentName = id;
    }

    if (disable) {
      // Disable all models.
      if (id === 'all') {
        removeAll = true;
      }
      removedModels.push(id);
      continue;
    }

    // remove empty model name
    if (!item.trim().length) {
      continue;
    }

    // Remove duplicate model entries.
    const existingIndex = models.findIndex(({ id: n }) => n === id);
    if (existingIndex !== -1) {
      models.splice(existingIndex, 1);
    }

    const model: ChatModelCard = {
      displayName: displayName || undefined,
      id,
    };

    if (deploymentName) {
      model.deploymentName = deploymentName;
    }

    if (capabilities.length > 0) {
      const [maxTokenStr, ...capabilityList] = capabilities[0].replace('>', '').split(':');
      model.contextWindowTokens = parseInt(maxTokenStr, 10) || undefined;

      for (const capability of capabilityList) {
        switch (capability) {
          case 'vision': {
            model.vision = true;
            break;
          }
          case 'fc': {
            model.functionCall = true;
            break;
          }
          case 'file': {
            model.files = true;
            break;
          }
          default: {
            console.warn(`Unknown capability: ${capability}`);
          }
        }
      }
    }

    models.push(model);
  }

  return {
    add: models,
    removeAll,
    removed: removedModels,
  };
};

/**
 * Extract a special method to process chatModels
 */
export const transformToChatModelCards = ({
  modelString = '',
  defaultChatModels,
  withDeploymentName = false,
}: {
  defaultChatModels: ChatModelCard[];
  modelString?: string;
  withDeploymentName?: boolean;
}): ChatModelCard[] | undefined => {
  if (!modelString) return undefined;

  const modelConfig = parseModelString(modelString, withDeploymentName);
  let chatModels = modelConfig.removeAll ? [] : defaultChatModels;

  // 处理移除逻辑
  if (!modelConfig.removeAll) {
    chatModels = chatModels.filter((m) => !modelConfig.removed.includes(m.id));
  }

  return produce(chatModels, (draft) => {
    // 处理添加或替换逻辑
    for (const toAddModel of modelConfig.add) {
      // first try to find the model in LOBE_DEFAULT_MODEL_LIST to confirm if it is a known model
      const knownModel = LOBE_DEFAULT_MODEL_LIST.find((model) => model.id === toAddModel.id);

      // if the model is known, update it based on the known model
      if (knownModel) {
        const index = draft.findIndex((model) => model.id === toAddModel.id);
        const modelInList = draft[index];

        // if the model is already in chatModels, update it
        if (modelInList) {
          draft[index] = {
            ...modelInList,
            ...toAddModel,
            displayName: toAddModel.displayName || modelInList.displayName || modelInList.id,
            enabled: true,
          };
        } else {
          // if the model is not in chatModels, add it
          draft.push({
            ...knownModel,
            ...toAddModel,
            displayName: toAddModel.displayName || knownModel.displayName || knownModel.id,
            enabled: true,
          });
        }
      } else {
        // if the model is not in LOBE_DEFAULT_MODEL_LIST, add it as a new custom model
        draft.push({
          ...toAddModel,
          displayName: toAddModel.displayName || toAddModel.id,
          enabled: true,
        });
      }
    }
  });
};

export const extractEnabledModels = (modelString: string = '', withDeploymentName = false) => {
  const modelConfig = parseModelString(modelString, withDeploymentName);
  const list = modelConfig.add.map((m) => m.id);

  if (list.length === 0) return;

  return list;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/errorResponse.ts
================================================================================

import { AgentRuntimeErrorType, ILobeAgentRuntimeErrorType } from '@/libs/agent-runtime';
import { ChatErrorType, ErrorResponse, ErrorType } from '@/types/fetch';

const getStatus = (errorType: ILobeAgentRuntimeErrorType | ErrorType) => {
  // InvalidAccessCode / InvalidAzureAPIKey / InvalidOpenAIAPIKey / InvalidZhipuAPIKey ....
  if (errorType.toString().includes('Invalid')) return 401;

  switch (errorType) {
    // TODO: Need to refactor to Invalid OpenAI API Key
    case AgentRuntimeErrorType.InvalidProviderAPIKey:
    case AgentRuntimeErrorType.NoOpenAIAPIKey: {
      return 401;
    }

    case AgentRuntimeErrorType.LocationNotSupportError: {
      return 403;
    }

    case AgentRuntimeErrorType.QuotaLimitReached: {
      return 429;
    }

    // define the 471~480 as provider error
    case AgentRuntimeErrorType.AgentRuntimeError: {
      return 470;
    }

    case AgentRuntimeErrorType.ProviderBizError:
    case AgentRuntimeErrorType.OpenAIBizError: {
      return 471;
    }

    case ChatErrorType.OllamaServiceUnavailable:
    case AgentRuntimeErrorType.OllamaBizError: {
      return 472;
    }
  }

  return errorType as number;
};

export const createErrorResponse = (
  errorType: ErrorType | ILobeAgentRuntimeErrorType,
  body?: any,
) => {
  const statusCode = getStatus(errorType);

  const data: ErrorResponse = { body, errorType };

  if (typeof statusCode !== 'number' || statusCode < 200 || statusCode > 599) {
    console.error(
      `current StatusCode: \`${statusCode}\` .`,
      'Please go to `./src/app/api/errorResponse.ts` to defined the statusCode.',
    );
  }

  return new Response(JSON.stringify(data), { status: statusCode });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/platform.ts
================================================================================

import UAParser from 'ua-parser-js';

import { isOnServerSide } from '@/utils/env';

export const getParser = () => {
  if (isOnServerSide) return new UAParser('Node');

  let ua = navigator.userAgent;
  return new UAParser(ua);
};

export const getPlatform = () => {
  return getParser().getOS().name;
};

export const getBrowser = () => {
  return getParser().getResult().browser.name;
};

export const browserInfo = {
  browser: getBrowser(),
  isMobile: getParser().getDevice().type === 'mobile',
  os: getParser().getOS().name,
};

export const isMacOS = () => getPlatform() === 'Mac OS';

export const isArc = () => {
  if (isOnServerSide) return false;
  return (
    window.matchMedia('(--arc-palette-focus: var(--arc-background-simple-color))').matches ||
    Boolean('arc' in window || 'ArcControl' in window || 'ARCControl' in window) ||
    Boolean(getComputedStyle(document.documentElement).getPropertyValue('--arc-palette-title'))
  );
};

export const isInStandaloneMode = () => {
  if (isOnServerSide) return false;
  return (
    window.matchMedia('(display-mode: standalone)').matches ||
    ('standalone' in navigator && (navigator as any).standalone === true)
  );
};

export const isSonomaOrLaterSafari = () => {
  if (isOnServerSide) return false;

  // refs: https://github.com/khmyznikov/pwa-install/blob/0904788b9d0e34399846f6cb7dbb5efeabb62c20/src/utils.ts#L24
  const userAgent = navigator.userAgent.toLowerCase();
  if (navigator.maxTouchPoints || !/macintosh/.test(userAgent)) return false;

  // check safari version >= 17
  const version = /version\/(\d{2})\./.exec(userAgent);
  if (!version || !version[1] || !(parseInt(version[1]) >= 17)) return false;

  try {
    // hacky way to detect Sonoma
    const audioCheck = document.createElement('audio').canPlayType('audio/wav; codecs="1"');
    const webGLCheck = new OffscreenCanvas(1, 1).getContext('webgl');
    return Boolean(audioCheck) && Boolean(webGLCheck);
  } catch {
    return false;
  }
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/toolCall.ts
================================================================================

import { Md5 } from 'ts-md5';

import { PLUGIN_SCHEMA_API_MD5_PREFIX, PLUGIN_SCHEMA_SEPARATOR } from '@/const/plugin';

export const genToolCallingName = (identifier: string, name: string, type: string = 'default') => {
  const pluginType = type && type !== 'default' ? `${PLUGIN_SCHEMA_SEPARATOR + type}` : '';

  // 将插件的 identifier 作为前缀，避免重复
  let apiName = identifier + PLUGIN_SCHEMA_SEPARATOR + name + pluginType;

  // OpenAI GPT function_call name can't be longer than 64 characters
  // So we need to use md5 to shorten the name
  // and then find the correct apiName in response by md5
  if (apiName.length >= 64) {
    const md5Content = PLUGIN_SCHEMA_API_MD5_PREFIX + Md5.hashStr(name).toString();

    apiName = identifier + PLUGIN_SCHEMA_SEPARATOR + md5Content + pluginType;
  }

  return apiName;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/fetch/fetchSSE.ts
================================================================================

import { MESSAGE_CANCEL_FLAT } from '@/const/message';
import { LOBE_CHAT_OBSERVATION_ID, LOBE_CHAT_TRACE_ID } from '@/const/trace';
import { ChatErrorType } from '@/types/fetch';
import { SmoothingParams } from '@/types/llm';
import {
  ChatMessageError,
  MessageToolCall,
  MessageToolCallChunk,
  MessageToolCallSchema,
} from '@/types/message';

import { fetchEventSource } from './fetchEventSource';
import { getMessageError } from './parseError';
import { parseToolCalls } from './parseToolCalls';

type SSEFinishType = 'done' | 'error' | 'abort';

export type OnFinishHandler = (
  text: string,
  context: {
    observationId?: string | null;
    toolCalls?: MessageToolCall[];
    traceId?: string | null;
    type?: SSEFinishType;
  },
) => Promise<void>;

export interface MessageTextChunk {
  text: string;
  type: 'text';
}

interface MessageToolCallsChunk {
  isAnimationActives?: boolean[];
  tool_calls: MessageToolCall[];
  type: 'tool_calls';
}

export interface FetchSSEOptions {
  fetcher?: typeof fetch;
  onAbort?: (text: string) => Promise<void>;
  onErrorHandle?: (error: ChatMessageError) => void;
  onFinish?: OnFinishHandler;
  onMessageHandle?: (chunk: MessageTextChunk | MessageToolCallsChunk) => void;
  smoothing?: SmoothingParams | boolean;
}

const START_ANIMATION_SPEED = 4;

const END_ANIMATION_SPEED = 15;

const createSmoothMessage = (params: {
  onTextUpdate: (delta: string, text: string) => void;
  startSpeed?: number;
}) => {
  const { startSpeed = START_ANIMATION_SPEED } = params;

  let buffer = '';
  // why use queue: https://shareg.pt/GLBrjpK
  let outputQueue: string[] = [];
  let isAnimationActive = false;
  let animationFrameId: number | null = null;

  // when you need to stop the animation, call this function
  const stopAnimation = () => {
    isAnimationActive = false;
    if (animationFrameId !== null) {
      cancelAnimationFrame(animationFrameId);
      animationFrameId = null;
    }
  };

  // define startAnimation function to display the text in buffer smooth
  // when you need to start the animation, call this function
  const startAnimation = (speed = startSpeed) =>
    new Promise<void>((resolve) => {
      if (isAnimationActive) {
        resolve();
        return;
      }

      isAnimationActive = true;

      const updateText = () => {
        // 如果动画已经不再激活，则停止更新文本
        if (!isAnimationActive) {
          cancelAnimationFrame(animationFrameId!);
          animationFrameId = null;
          resolve();
          return;
        }

        // 如果还有文本没有显示
        // 检查队列中是否有字符待显示
        if (outputQueue.length > 0) {
          // 从队列中获取前 n 个字符（如果存在）
          const charsToAdd = outputQueue.splice(0, speed).join('');
          buffer += charsToAdd;

          // 更新消息内容，这里可能需要结合实际情况调整
          params.onTextUpdate(charsToAdd, buffer);
        } else {
          // 当所有字符都显示完毕时，清除动画状态
          isAnimationActive = false;
          animationFrameId = null;
          resolve();
          return;
        }

        animationFrameId = requestAnimationFrame(updateText);
      };

      animationFrameId = requestAnimationFrame(updateText);
    });

  const pushToQueue = (text: string) => {
    outputQueue.push(...text.split(''));
  };

  return {
    isAnimationActive,
    isTokenRemain: () => outputQueue.length > 0,
    pushToQueue,
    startAnimation,
    stopAnimation,
  };
};

const createSmoothToolCalls = (params: {
  onToolCallsUpdate: (toolCalls: MessageToolCall[], isAnimationActives: boolean[]) => void;
  startSpeed?: number;
}) => {
  const { startSpeed = START_ANIMATION_SPEED } = params;
  let toolCallsBuffer: MessageToolCall[] = [];

  // 为每个 tool_call 维护一个输出队列和动画控制器

  const outputQueues: string[][] = [];
  const isAnimationActives: boolean[] = [];
  const animationFrameIds: (number | null)[] = [];

  const stopAnimation = (index: number) => {
    isAnimationActives[index] = false;
    if (animationFrameIds[index] !== null) {
      cancelAnimationFrame(animationFrameIds[index]!);
      animationFrameIds[index] = null;
    }
  };

  const startAnimation = (index: number, speed = startSpeed) =>
    new Promise<void>((resolve) => {
      if (isAnimationActives[index]) {
        resolve();
        return;
      }

      isAnimationActives[index] = true;

      const updateToolCall = () => {
        if (!isAnimationActives[index]) {
          resolve();
          return;
        }

        if (outputQueues[index].length > 0) {
          const charsToAdd = outputQueues[index].splice(0, speed).join('');

          const toolCallToUpdate = toolCallsBuffer[index];

          if (toolCallToUpdate) {
            toolCallToUpdate.function.arguments += charsToAdd;

            // 触发 ui 更新
            params.onToolCallsUpdate(toolCallsBuffer, [...isAnimationActives]);
          }

          animationFrameIds[index] = requestAnimationFrame(() => updateToolCall());
        } else {
          isAnimationActives[index] = false;
          animationFrameIds[index] = null;
          resolve();
        }
      };

      animationFrameIds[index] = requestAnimationFrame(() => updateToolCall());
    });

  const pushToQueue = (toolCallChunks: MessageToolCallChunk[]) => {
    toolCallChunks.forEach((chunk) => {
      // init the tool call buffer and output queue
      if (!toolCallsBuffer[chunk.index]) {
        toolCallsBuffer[chunk.index] = MessageToolCallSchema.parse(chunk);
      }

      if (!outputQueues[chunk.index]) {
        outputQueues[chunk.index] = [];
        isAnimationActives[chunk.index] = false;
        animationFrameIds[chunk.index] = null;
      }

      outputQueues[chunk.index].push(...(chunk.function?.arguments || '').split(''));
    });
  };

  const startAnimations = async (speed = startSpeed) => {
    const pools = toolCallsBuffer.map(async (_, index) => {
      if (outputQueues[index].length > 0 && !isAnimationActives[index]) {
        await startAnimation(index, speed);
      }
    });

    await Promise.all(pools);
  };
  const stopAnimations = () => {
    toolCallsBuffer.forEach((_, index) => {
      stopAnimation(index);
    });
  };

  return {
    isAnimationActives,
    isTokenRemain: () => outputQueues.some((token) => token.length > 0),
    pushToQueue,
    startAnimations,
    stopAnimations,
  };
};

/**
 * Fetch data using stream method
 */
// eslint-disable-next-line no-undef
export const fetchSSE = async (url: string, options: RequestInit & FetchSSEOptions = {}) => {
  let output = '';
  let toolCalls: undefined | MessageToolCall[];
  let triggerOnMessageHandler = false;

  let finishedType: SSEFinishType = 'done';
  let response!: Response;

  const { smoothing } = options;

  const textSmoothing = typeof smoothing === 'boolean' ? smoothing : smoothing?.text;
  const toolsCallingSmoothing =
    typeof smoothing === 'boolean' ? smoothing : (smoothing?.toolsCalling ?? true);
  const smoothingSpeed = typeof smoothing === 'object' ? smoothing.speed : undefined;

  const textController = createSmoothMessage({
    onTextUpdate: (delta, text) => {
      output = text;
      options.onMessageHandle?.({ text: delta, type: 'text' });
    },
    startSpeed: smoothingSpeed,
  });

  const toolCallsController = createSmoothToolCalls({
    onToolCallsUpdate: (toolCalls, isAnimationActives) => {
      options.onMessageHandle?.({ isAnimationActives, tool_calls: toolCalls, type: 'tool_calls' });
    },
    startSpeed: smoothingSpeed,
  });

  await fetchEventSource(url, {
    body: options.body,
    fetch: options?.fetcher,
    headers: options.headers as Record<string, string>,
    method: options.method,
    onerror: (error) => {
      if (error === MESSAGE_CANCEL_FLAT || (error as TypeError).name === 'AbortError') {
        finishedType = 'abort';
        options?.onAbort?.(output);
        textController.stopAnimation();
      } else {
        finishedType = 'error';

        options.onErrorHandle?.(
          error.type
            ? error
            : {
                body: {
                  message: error.message,
                  name: error.name,
                  stack: error.stack,
                },
                message: error.message,
                type: ChatErrorType.UnknownChatFetchError,
              },
        );
        return;
      }
    },
    onmessage: (ev) => {
      triggerOnMessageHandler = true;
      let data;
      try {
        data = JSON.parse(ev.data);
      } catch (e) {
        console.warn('parse error:', e);
        options.onErrorHandle?.({
          body: {
            context: {
              chunk: ev.data,
              error: { message: (e as Error).message, name: (e as Error).name },
            },
            message:
              'chat response streaming chunk parse error, please contact your API Provider to fix it.',
          },
          message: 'parse error',
          type: 'StreamChunkError',
        });

        return;
      }

      switch (ev.event) {
        case 'error': {
          finishedType = 'error';
          options.onErrorHandle?.(data);
          break;
        }

        case 'text': {
          if (textSmoothing) {
            textController.pushToQueue(data);

            if (!textController.isAnimationActive) textController.startAnimation();
          } else {
            output += data;
            options.onMessageHandle?.({ text: data, type: 'text' });
          }

          break;
        }

        case 'tool_calls': {
          // get finial
          // if there is no tool calls, we should initialize the tool calls
          if (!toolCalls) toolCalls = [];
          toolCalls = parseToolCalls(toolCalls, data);

          if (toolsCallingSmoothing) {
            // make the tool calls smooth

            // push the tool calls to the smooth queue
            toolCallsController.pushToQueue(data);
            // if there is no animation active, we should start the animation
            if (toolCallsController.isAnimationActives.some((value) => !value)) {
              toolCallsController.startAnimations();
            }
          } else {
            options.onMessageHandle?.({ tool_calls: toolCalls, type: 'tool_calls' });
          }
        }
      }
    },
    onopen: async (res) => {
      response = res.clone();
      // 如果不 ok 说明有请求错误
      if (!response.ok) {
        throw await getMessageError(res);
      }
    },
    signal: options.signal,
  });

  // only call onFinish when response is available
  // so like abort, we don't need to call onFinish
  if (response) {
    textController.stopAnimation();
    toolCallsController.stopAnimations();

    if (response.ok) {
      // if there is no onMessageHandler, we should call onHandleMessage first
      if (!triggerOnMessageHandler) {
        output = await response.clone().text();
        options.onMessageHandle?.({ text: output, type: 'text' });
      }

      const traceId = response.headers.get(LOBE_CHAT_TRACE_ID);
      const observationId = response.headers.get(LOBE_CHAT_OBSERVATION_ID);

      if (textController.isTokenRemain()) {
        await textController.startAnimation(END_ANIMATION_SPEED);
      }

      if (toolCallsController.isTokenRemain()) {
        await toolCallsController.startAnimations(END_ANIMATION_SPEED);
      }

      await options?.onFinish?.(output, { observationId, toolCalls, traceId, type: finishedType });
    }
  }

  return response;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/fetch/parseToolCalls.ts
================================================================================

import { produce } from 'immer';

import { MessageToolCall, MessageToolCallChunk, MessageToolCallSchema } from '@/types/message';

export const parseToolCalls = (origin: MessageToolCall[], value: MessageToolCallChunk[]) =>
  produce(origin, (draft) => {
    // if there is no origin, we should parse all the value and set it to draft
    if (draft.length === 0) {
      draft.push(...value.map((item) => MessageToolCallSchema.parse(item)));
      return;
    }

    // if there is origin, we should merge the value to the origin
    value.forEach(({ index, ...item }) => {
      if (!draft?.[index]) {
        // if not, we should insert it to the draft
        draft?.splice(index, 0, MessageToolCallSchema.parse(item));
      } else {
        // if it is already in the draft, we should merge the arguments to the draft
        if (item.function?.arguments) {
          draft[index].function.arguments += item.function.arguments;
        }
      }
    });
  });


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/utils/fetch/fetchEventSource/parse.ts
================================================================================

//@ts-nocheck
/* eslint-disable unicorn/no-abusive-eslint-disable */
/* eslint-disable */
/**
 * Represents a message sent in an event stream
 * https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format
 */
export interface EventSourceMessage {
  /** The event ID to set the EventSource object's last event ID value. */
  id: string;
  /** A string identifying the type of event described. */
  event: string;
  /** The event data */
  data: string;
  /** The reconnection interval (in milliseconds) to wait before retrying the connection */
  retry?: number;
}

/**
 * Converts a ReadableStream into a callback pattern.
 * @param stream The input ReadableStream.
 * @param onChunk A function that will be called on each new byte chunk in the stream.
 * @returns {Promise<void>} A promise that will be resolved when the stream closes.
 */
export async function getBytes(
  stream: ReadableStream<Uint8Array>,
  onChunk: (arr: Uint8Array) => void,
) {
  const reader = stream.getReader();
  let result: ReadableStreamDefaultReadResult<Uint8Array>;
  while (!(result = await reader.read()).done) {
    onChunk(result.value);
  }
}

const enum ControlChars {
  NewLine = 10,
  CarriageReturn = 13,
  Space = 32,
  Colon = 58,
}

/**
 * Parses arbitary byte chunks into EventSource line buffers.
 * Each line should be of the format "field: value" and ends with \r, \n, or \r\n.
 * @param onLine A function that will be called on each new EventSource line.
 * @returns A function that should be called for each incoming byte chunk.
 */
export function getLines(onLine: (line: Uint8Array, fieldLength: number) => void) {
  let buffer: Uint8Array | undefined;
  let position: number; // current read position
  let fieldLength: number; // length of the `field` portion of the line
  let discardTrailingNewline = false;

  // return a function that can process each incoming byte chunk:
  return function onChunk(arr: Uint8Array) {
    if (buffer === undefined) {
      buffer = arr;
      position = 0;
      fieldLength = -1;
    } else {
      // we're still parsing the old line. Append the new bytes into buffer:
      buffer = concat(buffer, arr);
    }

    const bufLength = buffer.length;
    let lineStart = 0; // index where the current line starts
    while (position < bufLength) {
      if (discardTrailingNewline) {
        if (buffer[position] === ControlChars.NewLine) {
          lineStart = ++position; // skip to next char
        }

        discardTrailingNewline = false;
      }

      // start looking forward till the end of line:
      let lineEnd = -1; // index of the \r or \n char
      for (; position < bufLength && lineEnd === -1; ++position) {
        switch (buffer[position]) {
          case ControlChars.Colon:
            if (fieldLength === -1) {
              // first colon in line
              fieldLength = position - lineStart;
            }
            break;
          // @ts-ignore:7029 \r case below should fallthrough to \n:
          case ControlChars.CarriageReturn:
            discardTrailingNewline = true;
          case ControlChars.NewLine:
            lineEnd = position;
            break;
        }
      }

      if (lineEnd === -1) {
        // We reached the end of the buffer but the line hasn't ended.
        // Wait for the next arr and then continue parsing:
        break;
      }

      // we've reached the line end, send it out:
      onLine(buffer.subarray(lineStart, lineEnd), fieldLength);
      lineStart = position; // we're now on the next line
      fieldLength = -1;
    }

    if (lineStart === bufLength) {
      buffer = undefined; // we've finished reading it
    } else if (lineStart !== 0) {
      // Create a new view into buffer beginning at lineStart so we don't
      // need to copy over the previous lines when we get the new arr:
      buffer = buffer.subarray(lineStart);
      position -= lineStart;
    }
  };
}

/**
 * Parses line buffers into EventSourceMessages.
 * @param onId A function that will be called on each `id` field.
 * @param onRetry A function that will be called on each `retry` field.
 * @param onMessage A function that will be called on each message.
 * @returns A function that should be called for each incoming line buffer.
 */
export function getMessages(
  onId: (id: string) => void,
  onMessage?: (msg: EventSourceMessage) => void,
) {
  let message = newMessage();
  const decoder = new TextDecoder();

  // return a function that can process each incoming line buffer:
  return function onLine(line: Uint8Array, fieldLength: number) {
    if (line.length === 0) {
      // empty line denotes end of message. Trigger the callback and start a new message:
      onMessage?.(message);
      message = newMessage();
    } else if (fieldLength > 0) {
      // exclude comments and lines with no values
      // line is of format "<field>:<value>" or "<field>: <value>"
      // https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation
      const field = decoder.decode(line.subarray(0, fieldLength));
      const valueOffset = fieldLength + (line[fieldLength + 1] === ControlChars.Space ? 2 : 1);
      const value = decoder.decode(line.subarray(valueOffset));

      switch (field) {
        case 'data':
          // if this message already has data, append the new value to the old.
          // otherwise, just set to the new value:
          message.data = message.data ? message.data + '\n' + value : value; // otherwise,
          break;
        case 'event':
          message.event = value;
          break;
        case 'id':
          onId((message.id = value));
          break;
      }
    }
  };
}

function concat(a: Uint8Array, b: Uint8Array) {
  const res = new Uint8Array(a.length + b.length);
  res.set(a);
  res.set(b, a.length);
  return res;
}

function newMessage(): EventSourceMessage {
  // data, event, and id must be initialized to empty strings:
  // https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation
  // retry should be initialized to undefined so we return a consistent shape
  // to the js engine all the time: https://mathiasbynens.be/notes/shapes-ics#takeaways
  return {
    data: '',
    event: '',
    id: '',
    retry: undefined,
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/langchain/loaders/index.ts
================================================================================

import {
  SupportedTextSplitterLanguage,
  SupportedTextSplitterLanguages,
} from 'langchain/text_splitter';

import { LANGCHAIN_SUPPORT_TEXT_LIST } from '@/libs/langchain/file';
import { LangChainLoaderType } from '@/libs/langchain/types';

import { CodeLoader } from './code';
import { CsVLoader } from './csv';
import { DocxLoader } from './docx';
import { LatexLoader } from './latex';
import { MarkdownLoader } from './markdown';
import { PdfLoader } from './pdf';
import { PPTXLoader } from './pptx';
import { TextLoader } from './txt';

class LangChainError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'LangChainChunkingError';
  }
}

export class ChunkingLoader {
  partitionContent = async (filename: string, content: Uint8Array) => {
    try {
      const fileBlob = new Blob([Buffer.from(content)]);
      const txt = this.uint8ArrayToString(content);

      const type = this.getType(filename?.toLowerCase());

      switch (type) {
        case 'code': {
          const ext = filename.split('.').pop();
          return await CodeLoader(txt, ext!);
        }

        case 'ppt': {
          return await PPTXLoader(fileBlob);
        }

        case 'latex': {
          return await LatexLoader(txt);
        }

        case 'pdf': {
          return await PdfLoader(fileBlob);
        }

        case 'markdown': {
          return await MarkdownLoader(txt);
        }

        case 'doc': {
          return await DocxLoader(fileBlob);
        }

        case 'text': {
          return await TextLoader(txt);
        }

        case 'csv': {
          return await CsVLoader(fileBlob);
        }

        default: {
          throw new Error(
            `Unsupported file type [${type}], please check your file is supported, or create report issue here: https://github.com/lobehub/lobe-chat/discussions/3550`,
          );
        }
      }
    } catch (e) {
      throw new LangChainError((e as Error).message);
    }
  };

  private getType = (filename: string): LangChainLoaderType | undefined => {
    if (filename.endsWith('pptx')) {
      return 'ppt';
    }

    if (filename.endsWith('docx') || filename.endsWith('doc')) {
      return 'doc';
    }

    if (filename.endsWith('pdf')) {
      return 'pdf';
    }

    if (filename.endsWith('tex')) {
      return 'latex';
    }

    if (filename.endsWith('md') || filename.endsWith('mdx')) {
      return 'markdown';
    }

    if (filename.endsWith('csv')) {
      return 'csv';
    }

    const ext = filename.split('.').pop();

    if (ext && SupportedTextSplitterLanguages.includes(ext as SupportedTextSplitterLanguage)) {
      return 'code';
    }

    if (ext && LANGCHAIN_SUPPORT_TEXT_LIST.includes(ext)) return 'text';
  };

  private uint8ArrayToString(uint8Array: Uint8Array) {
    const decoder = new TextDecoder();
    return decoder.decode(uint8Array);
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/langchain/loaders/config.ts
================================================================================

const getLoaderConfig = () => ({
  chunkOverlap: 400,
  chunkSize: 800,
});

export const loaderConfig = getLoaderConfig();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/trpc/client/lambda.ts
================================================================================

import { createTRPCClient, httpBatchLink } from '@trpc/client';
import { createTRPCReact } from '@trpc/react-query';
import superjson from 'superjson';
import { DEFAULT_CHAT_PROVIDER } from '@/const/settings/llm';
import { fetchErrorNotification } from '@/components/Error/fetchErrorNotification';
import { loginRequired } from '@/components/Error/loginRequiredNotification';
import { ModelProvider } from '@/libs/agent-runtime';
import type { LambdaRouter } from '@/server/routers/lambda';

import { ErrorResponse } from './types';

const links = [
  httpBatchLink({
    fetch: async (input, init) => {
      const response = await fetch(input, init);
      if (response.ok) return response;

      const errorRes: ErrorResponse = await response.clone().json();

      errorRes.forEach((item) => {
        const errorData = item.error.json;

        const status = errorData.data.httpStatus;

        switch (status) {
          case 401: {
            loginRequired.redirect();
            break;
          }
          default: {
            fetchErrorNotification.error({ errorMessage: errorData.message, status });
          }
        }
      });

      return response;
    },
    headers: async () => {
      // dynamic import to avoid circular dependency
      const { createHeaderWithAuth } = await import('@/services/_auth');

      // TODO: we need to support provider select
      return createHeaderWithAuth({ provider: DEFAULT_CHAT_PROVIDER });
    },
    maxURLLength: 2083,
    transformer: superjson,
    url: '/trpc/lambda',
  }),
];

export const lambdaClient = createTRPCClient<LambdaRouter>({
  links,
});

export const lambdaQuery = createTRPCReact<LambdaRouter>();

export const lambdaQueryClient = lambdaQuery.createClient({ links });


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/unstructured/index.ts
================================================================================

import zlib from 'node:zlib';
import pMap from 'p-map';
import { UnstructuredClient } from 'unstructured-client';
import { Strategy } from 'unstructured-client/sdk/models/shared';
import { PartitionResponse } from 'unstructured-client/src/sdk/models/operations';

import { knowledgeEnv } from '@/config/knowledge';

export enum ChunkingStrategy {
  Basic = 'basic',
  ByPage = 'by_page',
  BySimilarity = 'by_similarity',
  ByTitle = 'by_title',
}

export interface UnstructuredPartitionElement {
  compositeId?: string;
  element_id: string;
  metadata: Metadata;
  text: string;
  type: string;
}

export interface Metadata {
  coordinates: Coordinates;
  filename: string;
  filetype: string;
  image_base64?: string;
  image_mime_type?: string;
  is_continuation?: boolean;
  languages: string[];
  orig_elements?: string;
  page_name?: string;
  page_number: number;
  parent_id?: string;
  text_as_html?: string;
}

export interface Coordinates {
  layout_height: number;
  layout_width: number;
  points: number[][];
  system: string;
}

// class UnstructuredError extends Error {
//   constructor(message: string) {
//     super(message);
//     this.name = 'Unstructured';
//   }
// }

interface PartitionParameters {
  chunkingStrategy?: ChunkingStrategy;
  fileContent: Uint8Array;
  filename: string;
  maxCharacters?: number;
  onResponse?: (response: PartitionResponse) => void;
  strategy?: Strategy;
}

export class Unstructured {
  private client: UnstructuredClient;

  constructor(apikey?: string) {
    this.client = new UnstructuredClient({
      security: { apiKeyAuth: apikey || knowledgeEnv.UNSTRUCTURED_API_KEY! },
      serverURL: knowledgeEnv.UNSTRUCTURED_SERVER_URL,
    });
  }

  async partition(params: PartitionParameters): Promise<{
    compositeElements: UnstructuredPartitionElement[];
    originElements: UnstructuredPartitionElement[];
  }> {
    const hasChunkingStrategy = !!params.chunkingStrategy;
    const response = await this.client.general.partition({
      partitionParameters: {
        chunkingStrategy: params.chunkingStrategy,
        coordinates: true,
        // extractImageBlockTypes: ['Image'],
        files: { content: params.fileContent, fileName: params.filename },

        includeOrigElements: true,
        maxCharacters: params.maxCharacters || 800,
        strategy: params.strategy,
        uniqueElementIds: true,
      },
    });

    if (response.statusCode === 200) {
      // after finish partition, we need to filter out some elements
      const elements = response.elements as UnstructuredPartitionElement[];

      params.onResponse?.(response);

      let originElements: UnstructuredPartitionElement[] = [];
      let compositeElements: UnstructuredPartitionElement[] = [];

      if (hasChunkingStrategy) {
        await pMap(elements, async (element) => {
          // Your Base64 encoded string
          const base64EncodedString = element.metadata.orig_elements as string;
          delete element.metadata.orig_elements;

          if (!base64EncodedString) return;

          // Step 1: Decode the Base64 encoded string to binary
          const binaryBuffer = Buffer.from(base64EncodedString, 'base64');

          // Step 2: Decompress the binary data
          const elements = await this.decompressGzip(binaryBuffer);

          // if element is Table type then get the origin
          if (element.type === 'Table') {
            // skip continuation table due to being split by chunk strategy
            if (element.metadata.is_continuation) {
              return;
            }

            compositeElements = [...compositeElements, elements[0]];
            originElements = [...originElements, elements[0]];
            return;
          }

          compositeElements = [...compositeElements, element];

          originElements = originElements.concat(
            elements.map(
              (e) => ({ ...e, compositeId: element.element_id }) as UnstructuredPartitionElement,
            ),
          );
        });
      } else {
        originElements = elements;
      }

      return { compositeElements, originElements };
    } else {
      return { compositeElements: [], originElements: [] };
    }
  }

  private async decompressGzip(data: Buffer): Promise<UnstructuredPartitionElement[]> {
    return new Promise((resolve, reject) => {
      zlib.inflate(data, (err, decompressedBuffer) => {
        if (err) {
          reject(err);
        }

        // Step 3: Convert the decompressed buffer to a string
        let decompressedString = decompressedBuffer.toString('utf8');

        // Step 4: Parse the JSON string
        let jsonObject = JSON.parse(decompressedString);

        resolve(jsonObject);
      });
    });
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/AgentRuntime.ts
================================================================================

import { ClientOptions } from 'openai';

import type { TracePayload } from '@/const/trace';
import { DEFAULT_CHAT_PROVIDER } from '@/const/settings/llm';
import { LobeRuntimeAI } from './BaseAI';
import { LobeAi21AI } from './ai21';
import { LobeAi360AI } from './ai360';
import { LobeAnthropicAI } from './anthropic';
import { LobeAzureOpenAI } from './azureOpenai';
import { LobeBaichuanAI } from './baichuan';
import { LobeBedrockAI, LobeBedrockAIParams } from './bedrock';
import { LobeCloudflareAI, LobeCloudflareParams } from './cloudflare';
import { LobeDeepSeekAI } from './deepseek';
import { LobeFireworksAI } from './fireworksai';
import { LobeGiteeAI } from './giteeai';
import { LobeGithubAI } from './github';
import { LobeGoogleAI } from './google';
import { LobeGroq } from './groq';
import { LobeHigressAI } from './higress';
import { LobeHuggingFaceAI } from './huggingface';
import { LobeHunyuanAI } from './hunyuan';
import { LobeInternLMAI } from './internlm';
import { LobeMinimaxAI } from './minimax';
import { LobeMistralAI } from './mistral';
import { LobeMoonshotAI } from './moonshot';
import { LobeNovitaAI } from './novita';
import { LobeOllamaAI } from './ollama';
import { LobeOpenAI } from './openai';
import { LobeOpenRouterAI } from './openrouter';
import { LobePerplexityAI } from './perplexity';
import { LobeQwenAI } from './qwen';
import { LobeSenseNovaAI } from './sensenova';
import { LobeSiliconCloudAI } from './siliconcloud';
import { LobeSparkAI } from './spark';
import { LobeStepfunAI } from './stepfun';
import { LobeTaichuAI } from './taichu';
import { LobeTogetherAI } from './togetherai';
import {
  ChatCompetitionOptions,
  ChatStreamPayload,
  EmbeddingsOptions,
  EmbeddingsPayload,
  ModelProvider,
  TextToImagePayload,
  TextToSpeechPayload,
} from './types';
import { LobeUpstageAI } from './upstage';
import { LobeXAI } from './xai';
import { LobeZeroOneAI } from './zeroone';
import { LobeZhipuAI } from './zhipu';

export interface AgentChatOptions {
  enableTrace?: boolean;
  provider: string;
  trace?: TracePayload;
}

class AgentRuntime {
  private _runtime: LobeRuntimeAI;

  constructor(runtime: LobeRuntimeAI) {
    this._runtime = runtime;
  }

  /**
   * Initiates a chat session with the agent.
   *
   * @param payload - The payload containing the chat stream data.
   * @param options - Optional chat competition options.
   * @returns A Promise that resolves to the chat response.
   *
   * @example - Use without trace
   * ```ts
   * const agentRuntime = await initializeWithClientStore(provider, payload);
   * const data = payload as ChatStreamPayload;
   * return await agentRuntime.chat(data);
   * ```
   *
   * @example - Use Langfuse trace
   * ```ts
   * // ============  1. init chat model   ============ //
   * const agentRuntime = await initAgentRuntimeWithUserPayload(provider, jwtPayload);
   * // ============  2. create chat completion   ============ //
   * const data = {
   * // your trace options here
   *  } as ChatStreamPayload;
   * const tracePayload = getTracePayload(req);
   * return await agentRuntime.chat(data, createTraceOptions(data, {
   *   provider,
   *   trace: tracePayload,
   * }));
   * ```
   */
  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
    return this._runtime.chat(payload, options);
  }

  async textToImage(payload: TextToImagePayload) {
    return this._runtime.textToImage?.(payload);
  }

  async models() {
    return this._runtime.models?.();
  }

  async embeddings(payload: EmbeddingsPayload, options?: EmbeddingsOptions) {
    return this._runtime.embeddings?.(payload, options);
  }
  async textToSpeech(payload: TextToSpeechPayload, options?: EmbeddingsOptions) {
    return this._runtime.textToSpeech?.(payload, options);
  }

  /**
   * @description Initialize the runtime with the provider and the options
   * @param provider choose a model provider
   * @param params options of the choosed provider
   * @returns the runtime instance
   * Try to initialize the runtime with the provider and the options.
   * @example
   * ```ts
   * const runtime = await AgentRuntime.initializeWithProviderOptions(provider, {
   *    [provider]: {...options},
   * })
   * ```
   * **Note**: If you try to get a AgentRuntime instance from client or server,
   * you should use the methods to get the runtime instance at first.
   * - `src/app/api/chat/agentRuntime.ts: initAgentRuntimeWithUserPayload` on server
   * - `src/services/chat.ts: initializeWithClientStore` on client
   */
  static async initializeWithProviderOptions(
    provider: string,
    params: Partial<{
      ai21: Partial<ClientOptions>;
      ai360: Partial<ClientOptions>;
      anthropic: Partial<ClientOptions>;
      azure: { apiVersion?: string; apikey?: string; endpoint?: string };
      baichuan: Partial<ClientOptions>;
      bedrock: Partial<LobeBedrockAIParams>;
      cloudflare: Partial<LobeCloudflareParams>;
      deepseek: Partial<ClientOptions>;
      fireworksai: Partial<ClientOptions>;
      giteeai: Partial<ClientOptions>;
      github: Partial<ClientOptions>;
      google: { apiKey?: string; baseURL?: string };
      groq: Partial<ClientOptions>;
      higress: Partial<ClientOptions>;
      huggingface: { apiKey?: string; baseURL?: string };
      hunyuan: Partial<ClientOptions>;
      internlm: Partial<ClientOptions>;
      minimax: Partial<ClientOptions>;
      mistral: Partial<ClientOptions>;
      moonshot: Partial<ClientOptions>;
      novita: Partial<ClientOptions>;
      ollama: Partial<ClientOptions>;
      openai: Partial<ClientOptions>;
      openrouter: Partial<ClientOptions>;
      perplexity: Partial<ClientOptions>;
      qwen: Partial<ClientOptions>;
      sensenova: Partial<ClientOptions>;
      siliconcloud: Partial<ClientOptions>;
      spark: Partial<ClientOptions>;
      stepfun: Partial<ClientOptions>;
      taichu: Partial<ClientOptions>;
      togetherai: Partial<ClientOptions>;
      upstage: Partial<ClientOptions>;
      xai: Partial<ClientOptions>;
      zeroone: Partial<ClientOptions>;
      zhipu: Partial<ClientOptions>;
    }>,
  ) {
    let runtimeModel: LobeRuntimeAI;

    provider = provider || DEFAULT_CHAT_PROVIDER;
    switch (provider) {
      default:
      case ModelProvider.OpenAI: {
        // Will use the openai as default provider
        runtimeModel = new LobeOpenAI(params.openai ?? (params as any)[provider]);
        break;
      }

      case ModelProvider.Azure: {
        runtimeModel = new LobeAzureOpenAI(
          params.azure?.endpoint,
          params.azure?.apikey,
          params.azure?.apiVersion,
        );
        break;
      }

      case ModelProvider.ZhiPu: {
        runtimeModel = new LobeZhipuAI(params.zhipu);
        break;
      }

      case ModelProvider.Google: {
        runtimeModel = new LobeGoogleAI(params.google);
        break;
      }

      case ModelProvider.Moonshot: {
        runtimeModel = new LobeMoonshotAI(params.moonshot);
        break;
      }

      case ModelProvider.Bedrock: {
        runtimeModel = new LobeBedrockAI(params.bedrock);
        break;
      }

      case ModelProvider.Ollama: {
        runtimeModel = new LobeOllamaAI(params.ollama);
        break;
      }

      case ModelProvider.Perplexity: {
        runtimeModel = new LobePerplexityAI(params.perplexity);
        break;
      }

      case ModelProvider.Anthropic: {
        runtimeModel = new LobeAnthropicAI(params.anthropic);
        break;
      }

      case ModelProvider.DeepSeek: {
        runtimeModel = new LobeDeepSeekAI(params.deepseek);
        break;
      }

      case ModelProvider.HuggingFace: {
        runtimeModel = new LobeHuggingFaceAI(params.huggingface);
        break;
      }

      case ModelProvider.Minimax: {
        runtimeModel = new LobeMinimaxAI(params.minimax);
        break;
      }

      case ModelProvider.Mistral: {
        runtimeModel = new LobeMistralAI(params.mistral);
        break;
      }

      case ModelProvider.Groq: {
        runtimeModel = new LobeGroq(params.groq);
        break;
      }

      case ModelProvider.Github: {
        runtimeModel = new LobeGithubAI(params.github);
        break;
      }

      case ModelProvider.OpenRouter: {
        runtimeModel = new LobeOpenRouterAI(params.openrouter);
        break;
      }

      case ModelProvider.TogetherAI: {
        runtimeModel = new LobeTogetherAI(params.togetherai);
        break;
      }

      case ModelProvider.FireworksAI: {
        runtimeModel = new LobeFireworksAI(params.fireworksai);
        break;
      }

      case ModelProvider.ZeroOne: {
        runtimeModel = new LobeZeroOneAI(params.zeroone);
        break;
      }

      case ModelProvider.Qwen: {
        runtimeModel = new LobeQwenAI(params.qwen);
        break;
      }

      case ModelProvider.Stepfun: {
        runtimeModel = new LobeStepfunAI(params.stepfun);
        break;
      }

      case ModelProvider.Novita: {
        runtimeModel = new LobeNovitaAI(params.novita ?? {});
        break;
      }

      case ModelProvider.Baichuan: {
        runtimeModel = new LobeBaichuanAI(params.baichuan ?? {});
        break;
      }

      case ModelProvider.Taichu: {
        runtimeModel = new LobeTaichuAI(params.taichu);
        break;
      }

      case ModelProvider.Ai360: {
        runtimeModel = new LobeAi360AI(params.ai360 ?? {});
        break;
      }

      case ModelProvider.SiliconCloud: {
        runtimeModel = new LobeSiliconCloudAI(params.siliconcloud ?? {});
        break;
      }

      case ModelProvider.GiteeAI: {
        runtimeModel = new LobeGiteeAI(params.giteeai);
        break;
      }

      case ModelProvider.Upstage: {
        runtimeModel = new LobeUpstageAI(params.upstage);
        break;
      }

      case ModelProvider.Spark: {
        runtimeModel = new LobeSparkAI(params.spark);
        break;
      }

      case ModelProvider.Ai21: {
        runtimeModel = new LobeAi21AI(params.ai21);
        break;
      }

      case ModelProvider.Hunyuan: {
        runtimeModel = new LobeHunyuanAI(params.hunyuan);
        break;
      }

      case ModelProvider.SenseNova: {
        runtimeModel = new LobeSenseNovaAI(params.sensenova);
        break;
      }

      case ModelProvider.XAI: {
        runtimeModel = new LobeXAI(params.xai);
        break;
      }

      case ModelProvider.Cloudflare: {
        runtimeModel = new LobeCloudflareAI(params.cloudflare ?? {});
        break;
      }

      case ModelProvider.InternLM: {
        runtimeModel = new LobeInternLMAI(params.internlm);
        break;
      }

      case ModelProvider.Higress: {
        runtimeModel = new LobeHigressAI(params.higress);
        break;
      }
    }
    return new AgentRuntime(runtimeModel);
  }
}

export default AgentRuntime;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/BaseAI.ts
================================================================================

import OpenAI from 'openai';

import { ChatModelCard } from '@/types/llm';

import {
  ChatCompetitionOptions,
  ChatStreamPayload,
  Embeddings,
  EmbeddingsOptions,
  EmbeddingsPayload,
  TextToImagePayload,
  TextToSpeechOptions,
  TextToSpeechPayload,
} from './types';

export interface LobeRuntimeAI {
  baseURL?: string;
  chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions): Promise<Response>;

  embeddings?(payload: EmbeddingsPayload, options?: EmbeddingsOptions): Promise<Embeddings[]>;

  models?(): Promise<any>;

  textToImage?: (payload: TextToImagePayload) => Promise<string[]>;

  textToSpeech?: (
    payload: TextToSpeechPayload,
    options?: TextToSpeechOptions,
  ) => Promise<ArrayBuffer>;
}

export abstract class LobeOpenAICompatibleRuntime {
  abstract baseURL: string;
  abstract client: OpenAI;

  abstract chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions): Promise<Response>;

  abstract models(): Promise<ChatModelCard[]>;

  abstract embeddings(
    payload: EmbeddingsPayload,
    options?: EmbeddingsOptions,
  ): Promise<Embeddings[]>;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/index.ts
================================================================================

export { default as AgentRuntime } from './AgentRuntime';
export { LobeAnthropicAI } from './anthropic';
export { LobeAzureOpenAI } from './azureOpenai';
export * from './BaseAI';
export { LobeBedrockAI } from './bedrock';
export { LobeDeepSeekAI } from './deepseek';
export * from './error';
export { LobeGoogleAI } from './google';
export { LobeGroq } from './groq';
export { LobeMinimaxAI } from './minimax';
export { LobeMistralAI } from './mistral';
export { LobeMoonshotAI } from './moonshot';
export { LobeOllamaAI } from './ollama';
export { LobeOpenAI } from './openai';
export { LobeOpenRouterAI } from './openrouter';
export { LobePerplexityAI } from './perplexity';
export { LobeQwenAI } from './qwen';
export { LobeTogetherAI } from './togetherai';
export * from './types';
export { AgentRuntimeError } from './utils/createError';
export { LobeZeroOneAI } from './zeroone';
export { LobeZhipuAI } from './zhipu';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/error.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix */
// ******* Runtime Biz Error ******* //
export const AgentRuntimeErrorType = {
  AgentRuntimeError: 'AgentRuntimeError', // Agent Runtime 模块运行时错误
  LocationNotSupportError: 'LocationNotSupportError',
  QuotaLimitReached: 'QuotaLimitReached',
  PermissionDenied: 'PermissionDenied',

  InvalidProviderAPIKey: 'InvalidProviderAPIKey',
  ProviderBizError: 'ProviderBizError',

  InvalidOllamaArgs: 'InvalidOllamaArgs',
  OllamaBizError: 'OllamaBizError',

  InvalidBedrockCredentials: 'InvalidBedrockCredentials',
  StreamChunkError: 'StreamChunkError',

  InvalidGithubToken: 'InvalidGithubToken',

  ConnectionCheckFailed: 'ConnectionCheckFailed',

  /**
   * @deprecated
   */
  NoOpenAIAPIKey: 'NoOpenAIAPIKey',
  /**
   * @deprecated
   */
  OpenAIBizError: 'OpenAIBizError',
} as const;

export const AGENT_RUNTIME_ERROR_SET = new Set<string>(Object.values(AgentRuntimeErrorType));

export type ILobeAgentRuntimeErrorType =
  (typeof AgentRuntimeErrorType)[keyof typeof AgentRuntimeErrorType];


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/higress/index.ts
================================================================================

import { uniqueId } from 'lodash-es';

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

// import { OpenRouterModelCard } from './type';

export const LobeHigressAI = LobeOpenAICompatibleFactory({
  constructorOptions: {
    defaultHeaders: {
      'HTTP-Referer': 'https://chat-preview.lobehub.com',
      'X-Title': 'Lobe Chat',
      'x-Request-Id': uniqueId('lobe-chat-'),
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_HIGRESS_CHAT_COMPLETION === '1',
  },
  models: {
    transformModel: (m) => {
      const model = m as any;

      return {
        contextWindowTokens: model.context_length,
        description: model.description,
        displayName: model.name,
        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
        functionCall:
          model.description.includes('function calling') || model.description.includes('tools'),
        id: model.id,
        maxTokens:
          typeof model.top_provider.max_completion_tokens === 'number'
            ? model.top_provider.max_completion_tokens
            : undefined,
        vision:
          model.description.includes('vision') ||
          model.description.includes('multimodal') ||
          model.id.includes('vision'),
      };
    },
  },
  provider: ModelProvider.Higress,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/fireworksai/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeFireworksAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.fireworks.ai/inference/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_FIREWORKSAI_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.FireworksAI,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/moonshot/index.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamPayload, ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeMoonshotAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.moonshot.cn/v1',
  chatCompletion: {
    handlePayload: (payload: ChatStreamPayload) => {
      const { temperature, ...rest } = payload;

      return {
        ...rest,
        temperature: temperature !== undefined ? temperature / 2 : undefined,
      } as OpenAI.ChatCompletionCreateParamsStreaming;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_MOONSHOT_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Moonshot,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/internlm/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeInternLMAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://internlm-chat.intern-ai.org.cn/puyu/api/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      return {
        ...payload,
        stream: !payload.tools,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_INTERNLM_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.InternLM,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/types/embeddings.ts
================================================================================

export interface EmbeddingsPayload {
  /**
   * The number of dimensions the resulting output embeddings should have. Only
   * supported in `text-embedding-3` and later models.
   */
  dimensions?: number;
  /**
   * Input text to embed, encoded as a string or array of tokens. To embed multiple
   * inputs in a single request, pass an array of strings .
   * The input must not exceed the max input tokens for the model (8192 tokens for
   * `text-embedding-ada-002`), cannot be an empty string, and any array must be 2048
   * dimensions or less.
   */
  input: string | Array<string>;

  model: string;
}

export interface EmbeddingsOptions {
  headers?: Record<string, any>;
  signal?: AbortSignal;
  /**
   * userId for the embeddings
   */
  user?: string;
}

/**
 * The embedding vector, which is a list of floats. The length of vector depends on
 * the model as listed in the
 * [embedding guide](https://platform.openai.com/docs/guides/embeddings).
 */
export type Embeddings = Array<number>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/types/chat.ts
================================================================================

import { MessageToolCall } from '@/types/message';

export type LLMRoleType = 'user' | 'system' | 'assistant' | 'function' | 'tool';

interface UserMessageContentPartText {
  text: string;
  type: 'text';
}

interface UserMessageContentPartImage {
  image_url: {
    detail?: 'auto' | 'low' | 'high';
    url: string;
  };
  type: 'image_url';
}

export type UserMessageContentPart = UserMessageContentPartText | UserMessageContentPartImage;

export interface OpenAIChatMessage {
  /**
   * @title 内容
   * @description 消息内容
   */
  content: string | UserMessageContentPart[];

  name?: string;
  /**
   * 角色
   * @description 消息发送者的角色
   */
  role: LLMRoleType;
  tool_call_id?: string;
  tool_calls?: MessageToolCall[];
}

/**
 * @title Chat Stream Payload
 */
export interface ChatStreamPayload {
  /**
   * @title 控制生成文本中的惩罚系数，用于减少重复性
   * @default 0
   */
  frequency_penalty?: number;
  /**
   * @title 生成文本的最大长度
   */
  max_tokens?: number;
  /**
   * @title 聊天信息列表
   */
  messages: OpenAIChatMessage[];
  /**
   * @title 模型名称
   */
  model: string;
  /**
   * @title 返回的文本数量
   */
  n?: number;
  /**
   * 开启的插件列表
   */
  plugins?: string[];
  /**
   * @title 控制生成文本中的惩罚系数，用于减少主题的变化
   * @default 0
   */
  presence_penalty?: number;
  /**
   * @default openai
   */
  provider?: string;

  responseMode?: 'streamText' | 'json';
  /**
   * @title 是否开启流式请求
   * @default true
   */
  stream?: boolean;
  /**
   * @title 生成文本的随机度量，用于控制文本的创造性和多样性
   * @default 1
   */
  temperature: number;
  tool_choice?: string;
  tools?: ChatCompletionTool[];

  /**
   * @title 控制生成文本中最高概率的单个令牌
   * @default 1
   */
  top_p?: number;
}

export interface ChatCompetitionOptions {
  callback?: ChatStreamCallbacks;
  /**
   * response headers
   */
  headers?: Record<string, any>;
  /**
   * send the request to the ai api endpoint
   */
  requestHeaders?: Record<string, any>;
  signal?: AbortSignal;
  /**
   * userId for the chat completion
   */
  user?: string;
}

export interface ChatCompletionFunctions {
  /**
   * The description of what the function does.
   * @type {string}
   * @memberof ChatCompletionFunctions
   */
  description?: string;
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   * @type {string}
   * @memberof ChatCompletionFunctions
   */
  name: string;
  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
   * @type {{ [key: string]: any }}
   * @memberof ChatCompletionFunctions
   */
  parameters?: {
    [key: string]: any;
  };
}

export interface ChatCompletionTool {
  function: ChatCompletionFunctions;

  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function';
}

export interface ChatStreamCallbacks {
  /**
   * `onCompletion`: Called for each tokenized message.
   **/
  onCompletion?: (completion: string) => Promise<void> | void;
  /** `onFinal`: Called once when the stream is closed with the final completion message. */
  onFinal?: (completion: string) => Promise<void> | void;
  /** `onStart`: Called once when the stream is initialized. */
  onStart?: () => Promise<void> | void;
  /** `onText`: Called for each text chunk. */
  onText?: (text: string) => Promise<void> | void;
  /** `onToken`: Called for each tokenized message. */
  onToken?: (token: string) => Promise<void> | void;
  onToolCall?: () => Promise<void> | void;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/types/type.ts
================================================================================

import OpenAI from 'openai';

import { ILobeAgentRuntimeErrorType } from '../error';
import { ChatStreamPayload } from './chat';

export interface AgentInitErrorPayload {
  error: object;
  errorType: string | number;
}

export interface ChatCompletionErrorPayload {
  [key: string]: any;
  endpoint?: string;
  error: object;
  errorType: ILobeAgentRuntimeErrorType;
  provider: ModelProvider;
}

export interface CreateChatCompletionOptions {
  chatModel: OpenAI;
  payload: ChatStreamPayload;
}

export enum ModelProvider {
  Ai21 = 'ai21',
  Ai360 = 'ai360',
  Anthropic = 'anthropic',
  Azure = 'azure',
  Baichuan = 'baichuan',
  Bedrock = 'bedrock',
  Cloudflare = 'cloudflare',
  DeepSeek = 'deepseek',
  FireworksAI = 'fireworksai',
  GiteeAI = 'giteeai',
  Github = 'github',
  Google = 'google',
  Groq = 'groq',
  Higress = 'higress',
  HuggingFace = 'huggingface',
  Hunyuan = 'hunyuan',
  InternLM = 'internlm',
  Minimax = 'minimax',
  Mistral = 'mistral',
  Moonshot = 'moonshot',
  Novita = 'novita',
  Ollama = 'ollama',
  OpenAI = 'openai',
  OpenRouter = 'openrouter',
  Perplexity = 'perplexity',
  Qwen = 'qwen',
  SenseNova = 'sensenova',
  SiliconCloud = 'siliconcloud',
  Spark = 'spark',
  Stepfun = 'stepfun',
  Taichu = 'taichu',
  TogetherAI = 'togetherai',
  Upstage = 'upstage',
  Wenxin = 'wenxin',
  XAI = 'xai',
  ZeroOne = 'zeroone',
  ZhiPu = 'zhipu',
}

export type ModelProviderKey = Lowercase<keyof typeof ModelProvider>;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/togetherai/index.ts
================================================================================

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
import { TogetherAIModel } from './type';

const baseURL = 'https://api.together.xyz';
export const LobeTogetherAI = LobeOpenAICompatibleFactory({
  baseURL: `${baseURL}/v1`,
  constructorOptions: {
    defaultHeaders: {
      'HTTP-Referer': 'https://chat-preview.lobehub.com',
      'X-Title': 'Lobe Chat',
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_TOGETHERAI_CHAT_COMPLETION === '1',
  },
  models: async ({ client }) => {
    const apiKey = client.apiKey;
    const data = await fetch(`${baseURL}/api/models`, {
      headers: {
        Authorization: `Bearer ${apiKey}`,
      },
    });
    if (!data.ok) {
      throw new Error(`Together Fetch Error: ${data.statusText || data.status}`);
    }

    const models: TogetherAIModel[] = await data.json();

    return models
      .filter((m) => m.display_type === 'chat')
      .map((model) => {
        return {
          description: model.description,
          displayName: model.display_name,
          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name.endsWith(m.id))?.enabled || false,
          functionCall: model.description?.includes('function calling'),
          id: model.name,
          maxOutput: model.context_length,
          tokens: model.context_length,
          vision: model.description?.includes('vision') || model.name?.includes('vision'),
        };
      });
  },
  provider: ModelProvider.TogetherAI,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/openrouter/index.ts
================================================================================

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
import { OpenRouterModelCard } from './type';

export const LobeOpenRouterAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://openrouter.ai/api/v1',
  constructorOptions: {
    defaultHeaders: {
      'HTTP-Referer': 'https://chat-preview.lobehub.com',
      'X-Title': 'Lobe Chat',
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_OPENROUTER_CHAT_COMPLETION === '1',
  },
  models: {
    transformModel: (m) => {
      const model = m as unknown as OpenRouterModelCard;

      return {
        contextWindowTokens: model.context_length,
        description: model.description,
        displayName: model.name,
        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
        functionCall:
          model.description.includes('function calling') || model.description.includes('tools'),
        id: model.id,
        maxTokens:
          typeof model.top_provider.max_completion_tokens === 'number'
            ? model.top_provider.max_completion_tokens
            : undefined,
        vision:
          model.description.includes('vision') ||
          model.description.includes('multimodal') ||
          model.id.includes('vision'),
      };
    },
  },
  provider: ModelProvider.OpenRouter,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/azureOpenai/index.ts
================================================================================

import {
  AzureKeyCredential,
  ChatRequestMessage,
  GetChatCompletionsOptions,
  OpenAIClient,
} from '@azure/openai';

import { LobeRuntimeAI } from '../BaseAI';
import { AgentRuntimeErrorType } from '../error';
import { ChatCompetitionOptions, ChatStreamPayload, ModelProvider } from '../types';
import { AgentRuntimeError } from '../utils/createError';
import { debugStream } from '../utils/debugStream';
import { StreamingResponse } from '../utils/response';
import { AzureOpenAIStream } from '../utils/streams';

export class LobeAzureOpenAI implements LobeRuntimeAI {
  client: OpenAIClient;

  constructor(endpoint?: string, apikey?: string, apiVersion?: string) {
    if (!apikey || !endpoint)
      throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);

    this.client = new OpenAIClient(endpoint, new AzureKeyCredential(apikey), { apiVersion });

    this.baseURL = endpoint;
  }

  baseURL: string;

  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
    // ============  1. preprocess messages   ============ //
    const camelCasePayload = this.camelCaseKeys(payload);
    const { messages, model, maxTokens = 2048, ...params } = camelCasePayload;

    // ============  2. send api   ============ //

    try {
      const response = await this.client.streamChatCompletions(
        model,
        messages as ChatRequestMessage[],
        { ...params, abortSignal: options?.signal, maxTokens } as GetChatCompletionsOptions,
      );

      const [debug, prod] = response.tee();

      if (process.env.DEBUG_AZURE_CHAT_COMPLETION === '1') {
        debugStream(debug).catch(console.error);
      }

      return StreamingResponse(AzureOpenAIStream(prod, options?.callback), {
        headers: options?.headers,
      });
    } catch (e) {
      let error = e as { [key: string]: any; code: string; message: string };

      if (error.code) {
        switch (error.code) {
          case 'DeploymentNotFound': {
            error = { ...error, deployId: model };
          }
        }
      } else {
        error = {
          cause: error.cause,
          message: error.message,
          name: error.name,
        } as any;
      }

      const errorType = error.code
        ? AgentRuntimeErrorType.ProviderBizError
        : AgentRuntimeErrorType.AgentRuntimeError;

      throw AgentRuntimeError.chat({
        endpoint: this.maskSensitiveUrl(this.baseURL),
        error,
        errorType,
        provider: ModelProvider.Azure,
      });
    }
  }

  // Convert object keys to camel case, copy from `@azure/openai` in `node_modules/@azure/openai/dist/index.cjs`
  private camelCaseKeys = (obj: any): any => {
    if (typeof obj !== 'object' || !obj) return obj;
    if (Array.isArray(obj)) {
      return obj.map((v) => this.camelCaseKeys(v));
    } else {
      for (const key of Object.keys(obj)) {
        const value = obj[key];
        const newKey = this.tocamelCase(key);
        if (newKey !== key) {
          delete obj[key];
        }
        obj[newKey] = typeof obj[newKey] === 'object' ? this.camelCaseKeys(value) : value;
      }
      return obj;
    }
  };

  private tocamelCase = (str: string) => {
    return str
      .toLowerCase()
      .replaceAll(/(_[a-z])/g, (group) => group.toUpperCase().replace('_', ''));
  };

  private maskSensitiveUrl = (url: string) => {
    // 使用正则表达式匹配 'https://' 后面和 '.openai.azure.com/' 前面的内容
    const regex = /^(https:\/\/)([^.]+)(\.openai\.azure\.com\/.*)$/;

    // 使用替换函数
    return url.replace(regex, (match, protocol, subdomain, rest) => {
      // 将子域名替换为 '***'
      return `${protocol}***${rest}`;
    });
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/google/index.ts
================================================================================

import {
  Content,
  FunctionCallPart,
  FunctionDeclaration,
  Tool as GoogleFunctionCallTool,
  GoogleGenerativeAI,
  Part,
  SchemaType,
} from '@google/generative-ai';

import { imageUrlToBase64 } from '@/utils/imageToBase64';
import { safeParseJSON } from '@/utils/safeParseJSON';

import { LobeRuntimeAI } from '../BaseAI';
import { AgentRuntimeErrorType, ILobeAgentRuntimeErrorType } from '../error';
import {
  ChatCompetitionOptions,
  ChatCompletionTool,
  ChatStreamPayload,
  OpenAIChatMessage,
  UserMessageContentPart,
} from '../types';
import { ModelProvider } from '../types/type';
import { AgentRuntimeError } from '../utils/createError';
import { debugStream } from '../utils/debugStream';
import { StreamingResponse } from '../utils/response';
import { GoogleGenerativeAIStream, convertIterableToStream } from '../utils/streams';
import { parseDataUri } from '../utils/uriParser';

enum HarmCategory {
  HARM_CATEGORY_DANGEROUS_CONTENT = 'HARM_CATEGORY_DANGEROUS_CONTENT',
  HARM_CATEGORY_HARASSMENT = 'HARM_CATEGORY_HARASSMENT',
  HARM_CATEGORY_HATE_SPEECH = 'HARM_CATEGORY_HATE_SPEECH',
  HARM_CATEGORY_SEXUALLY_EXPLICIT = 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
}

enum HarmBlockThreshold {
  BLOCK_NONE = 'BLOCK_NONE',
}

export class LobeGoogleAI implements LobeRuntimeAI {
  private client: GoogleGenerativeAI;
  baseURL?: string;

  constructor({ apiKey, baseURL }: { apiKey?: string; baseURL?: string } = {}) {
    if (!apiKey) throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);

    this.client = new GoogleGenerativeAI(apiKey);
    this.baseURL = baseURL;
  }

  async chat(rawPayload: ChatStreamPayload, options?: ChatCompetitionOptions) {
    try {
      const payload = this.buildPayload(rawPayload);
      const model = payload.model;

      const contents = await this.buildGoogleMessages(payload.messages, model);

      const geminiStreamResult = await this.client
        .getGenerativeModel(
          {
            generationConfig: {
              maxOutputTokens: payload.max_tokens,
              temperature: payload.temperature,
              topP: payload.top_p,
            },
            model,
            // avoid wide sensitive words
            // refs: https://github.com/lobehub/lobe-chat/pull/1418
            safetySettings: [
              {
                category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold: HarmBlockThreshold.BLOCK_NONE,
              },
              {
                category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                threshold: HarmBlockThreshold.BLOCK_NONE,
              },
              {
                category: HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold: HarmBlockThreshold.BLOCK_NONE,
              },
              {
                category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                threshold: HarmBlockThreshold.BLOCK_NONE,
              },
            ],
          },
          { apiVersion: 'v1beta', baseUrl: this.baseURL },
        )
        .generateContentStream({
          contents,
          systemInstruction: payload.system as string,
          tools: this.buildGoogleTools(payload.tools),
        });

      const googleStream = convertIterableToStream(geminiStreamResult.stream);
      const [prod, useForDebug] = googleStream.tee();

      if (process.env.DEBUG_GOOGLE_CHAT_COMPLETION === '1') {
        debugStream(useForDebug).catch();
      }

      // Convert the response into a friendly text-stream
      const stream = GoogleGenerativeAIStream(prod, options?.callback);

      // Respond with the stream
      return StreamingResponse(stream, { headers: options?.headers });
    } catch (e) {
      const err = e as Error;

      const { errorType, error } = this.parseErrorMessage(err.message);

      throw AgentRuntimeError.chat({ error, errorType, provider: ModelProvider.Google });
    }
  }

  private buildPayload(payload: ChatStreamPayload) {
    const system_message = payload.messages.find((m) => m.role === 'system');
    const user_messages = payload.messages.filter((m) => m.role !== 'system');

    return {
      ...payload,
      messages: user_messages,
      system: system_message?.content,
    };
  }
  private convertContentToGooglePart = async (content: UserMessageContentPart): Promise<Part> => {
    switch (content.type) {
      case 'text': {
        return { text: content.text };
      }
      case 'image_url': {
        const { mimeType, base64, type } = parseDataUri(content.image_url.url);

        if (type === 'base64') {
          if (!base64) {
            throw new TypeError("Image URL doesn't contain base64 data");
          }

          return {
            inlineData: {
              data: base64,
              mimeType: mimeType || 'image/png',
            },
          };
        }

        if (type === 'url') {
          const { base64, mimeType } = await imageUrlToBase64(content.image_url.url);

          return {
            inlineData: {
              data: base64,
              mimeType,
            },
          };
        }

        throw new TypeError(`currently we don't support image url: ${content.image_url.url}`);
      }
    }
  };

  private convertOAIMessagesToGoogleMessage = async (
    message: OpenAIChatMessage,
  ): Promise<Content> => {
    const content = message.content as string | UserMessageContentPart[];
    if (!!message.tool_calls) {
      return {
        parts: message.tool_calls.map<FunctionCallPart>((tool) => ({
          functionCall: {
            args: safeParseJSON(tool.function.arguments)!,
            name: tool.function.name,
          },
        })),
        role: 'function',
      };
    }

    return {
      parts:
        typeof content === 'string'
          ? [{ text: content }]
          : await Promise.all(content.map(async (c) => await this.convertContentToGooglePart(c))),
      role: message.role === 'assistant' ? 'model' : 'user',
    };
  };

  // convert messages from the OpenAI format to Google GenAI SDK
  private buildGoogleMessages = async (
    messages: OpenAIChatMessage[],
    model: string,
  ): Promise<Content[]> => {
    // if the model is gemini-1.0 we need to pair messages
    if (model.startsWith('gemini-1.0')) {
      const contents: Content[] = [];
      let lastRole = 'model';

      for (const message of messages) {
        // current to filter function message
        if (message.role === 'function') {
          continue;
        }
        const googleMessage = await this.convertOAIMessagesToGoogleMessage(message);

        // if the last message is a model message and the current message is a model message
        // then we need to add a user message to separate them
        if (lastRole === googleMessage.role) {
          contents.push({ parts: [{ text: '' }], role: lastRole === 'user' ? 'model' : 'user' });
        }

        // add the current message to the contents
        contents.push(googleMessage);

        // update the last role
        lastRole = googleMessage.role;
      }

      // if the last message is a user message, then we need to add a model message to separate them
      if (lastRole === 'model') {
        contents.push({ parts: [{ text: '' }], role: 'user' });
      }

      return contents;
    }

    const pools = messages
      .filter((message) => message.role !== 'function')
      .map(async (msg) => await this.convertOAIMessagesToGoogleMessage(msg));

    return Promise.all(pools);
  };

  private parseErrorMessage(message: string): {
    error: any;
    errorType: ILobeAgentRuntimeErrorType;
  } {
    const defaultError = {
      error: { message },
      errorType: AgentRuntimeErrorType.ProviderBizError,
    };

    if (message.includes('location is not supported'))
      return { error: { message }, errorType: AgentRuntimeErrorType.LocationNotSupportError };

    try {
      const startIndex = message.lastIndexOf('[');
      if (startIndex === -1) {
        return defaultError;
      }

      // 从开始位置截取字符串到最后
      const jsonString = message.slice(startIndex);

      // 尝试解析 JSON 字符串
      const json: GoogleChatErrors = JSON.parse(jsonString);

      const bizError = json[0];

      switch (bizError.reason) {
        case 'API_KEY_INVALID': {
          return { ...defaultError, errorType: AgentRuntimeErrorType.InvalidProviderAPIKey };
        }

        default: {
          return { error: json, errorType: AgentRuntimeErrorType.ProviderBizError };
        }
      }
    } catch {
      // 如果解析失败，则返回原始错误消息
      return defaultError;
    }
  }

  private buildGoogleTools(
    tools: ChatCompletionTool[] | undefined,
  ): GoogleFunctionCallTool[] | undefined {
    if (!tools || tools.length === 0) return;

    return [
      {
        functionDeclarations: tools.map((tool) => this.convertToolToGoogleTool(tool)),
      },
    ];
  }

  private convertToolToGoogleTool = (tool: ChatCompletionTool): FunctionDeclaration => {
    const functionDeclaration = tool.function;
    const parameters = functionDeclaration.parameters;
    // refs: https://github.com/lobehub/lobe-chat/pull/5002
    const properties =
      parameters?.properties && Object.keys(parameters.properties).length > 0
        ? parameters.properties
        : { dummy: { type: 'string' } }; // dummy property to avoid empty object

    return {
      description: functionDeclaration.description,
      name: functionDeclaration.name,
      parameters: {
        description: parameters?.description,
        properties: properties,
        required: parameters?.required,
        type: SchemaType.OBJECT,
      },
    };
  };
}

export default LobeGoogleAI;

type GoogleChatErrors = GoogleChatError[];

interface GoogleChatError {
  '@type': string;
  'domain': string;
  'metadata': {
    service: string;
  };
  'reason': string;
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/groq/index.ts
================================================================================

import { AgentRuntimeErrorType } from '../error';
import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeGroq = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.groq.com/openai/v1',
  chatCompletion: {
    handleError: (error) => {
      // 403 means the location is not supported
      if (error.status === 403)
        return { error, errorType: AgentRuntimeErrorType.LocationNotSupportError };
    },
    handlePayload: (payload) => {
      const { temperature, ...restPayload } = payload;
      return {
        ...restPayload,
        // disable stream for tools due to groq dont support
        stream: !payload.tools,

        temperature: temperature <= 0 ? undefined : temperature,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_GROQ_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Groq,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/novita/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
import { NovitaModelCard } from './type';

export const LobeNovitaAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.novita.ai/v3/openai',
  constructorOptions: {
    defaultHeaders: {
      'X-Novita-Source': 'lobechat',
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_NOVITA_CHAT_COMPLETION === '1',
  },
  models: {
    transformModel: (m) => {
      const model = m as unknown as NovitaModelCard;

      return {
        contextWindowTokens: model.context_size,
        description: model.description,
        displayName: model.title,
        enabled: model.status === 1,
        functionCall: model.description.toLowerCase().includes('function calling'),
        id: model.id,
      };
    },
  },
  provider: ModelProvider.Novita,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/qwen/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

import { QwenAIStream } from '../utils/streams';

/*
  QwenLegacyModels: A set of legacy Qwen models that do not support presence_penalty.
  Currently, presence_penalty is only supported on Qwen commercial models and open-source models starting from Qwen 1.5 and later.
*/
export const QwenLegacyModels = new Set([
  'qwen-72b-chat',
  'qwen-14b-chat',
  'qwen-7b-chat',
  'qwen-1.8b-chat',
  'qwen-1.8b-longcontext-chat',
]);

export const LobeQwenAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      const { model, presence_penalty, temperature, top_p, ...rest } = payload;

      return {
        ...rest,
        frequency_penalty: undefined,
        model,
        presence_penalty:
          QwenLegacyModels.has(model)
            ? undefined
            : (presence_penalty !== undefined && presence_penalty >= -2 && presence_penalty <= 2)
              ? presence_penalty
              : undefined,
        stream: !payload.tools,
        temperature: (temperature !== undefined && temperature >= 0 && temperature < 2) ? temperature : undefined,
        ...(model.startsWith('qwen-vl') ? {
          top_p: (top_p !== undefined && top_p > 0 && top_p <= 1) ? top_p : undefined,
        } : {
          enable_search: true,
          top_p: (top_p !== undefined && top_p > 0 && top_p < 1) ? top_p : undefined,
        }),
      } as any;
    },
    handleStream: QwenAIStream,
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_QWEN_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Qwen,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/anthropicHelpers.ts
================================================================================

import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';

import { imageUrlToBase64 } from '@/utils/imageToBase64';

import { OpenAIChatMessage, UserMessageContentPart } from '../types';
import { parseDataUri } from './uriParser';

export const buildAnthropicBlock = async (
  content: UserMessageContentPart,
): Promise<Anthropic.ContentBlock | Anthropic.ImageBlockParam> => {
  switch (content.type) {
    case 'text': {
      return content;
    }

    case 'image_url': {
      const { mimeType, base64, type } = parseDataUri(content.image_url.url);

      if (type === 'base64')
        return {
          source: {
            data: base64 as string,
            media_type: mimeType as Anthropic.ImageBlockParam.Source['media_type'],
            type: 'base64',
          },
          type: 'image',
        };

      if (type === 'url') {
        const { base64, mimeType } = await imageUrlToBase64(content.image_url.url);
        return {
          source: {
            data: base64 as string,
            media_type: mimeType as Anthropic.ImageBlockParam.Source['media_type'],
            type: 'base64',
          },
          type: 'image',
        };
      }

      throw new Error(`Invalid image URL: ${content.image_url.url}`);
    }
  }
};

export const buildAnthropicMessage = async (
  message: OpenAIChatMessage,
): Promise<Anthropic.Messages.MessageParam> => {
  const content = message.content as string | UserMessageContentPart[];

  switch (message.role) {
    case 'system': {
      return { content: content as string, role: 'user' };
    }

    case 'user': {
      return {
        content:
          typeof content === 'string'
            ? content
            : await Promise.all(content.map(async (c) => await buildAnthropicBlock(c))),
        role: 'user',
      };
    }

    case 'tool': {
      // refs: https://docs.anthropic.com/claude/docs/tool-use#tool-use-and-tool-result-content-blocks
      return {
        content: [
          {
            content: message.content,
            tool_use_id: message.tool_call_id,
            type: 'tool_result',
          } as any,
        ],
        role: 'user',
      };
    }

    case 'assistant': {
      // if there is tool_calls , we need to covert the tool_calls to tool_use content block
      // refs: https://docs.anthropic.com/claude/docs/tool-use#tool-use-and-tool-result-content-blocks
      if (message.tool_calls) {
        return {
          content: [
            // avoid empty text content block
            !!message.content && {
              text: message.content as string,
              type: 'text',
            },
            ...(message.tool_calls.map((tool) => ({
              id: tool.id,
              input: JSON.parse(tool.function.arguments),
              name: tool.function.name,
              type: 'tool_use',
            })) as any),
          ].filter(Boolean),
          role: 'assistant',
        };
      }

      // or it's a plain assistant message
      return { content: content as string, role: 'assistant' };
    }

    case 'function': {
      return { content: content as string, role: 'assistant' };
    }
  }
};

export const buildAnthropicMessages = async (
  oaiMessages: OpenAIChatMessage[],
): Promise<Anthropic.Messages.MessageParam[]> => {
  const messages: Anthropic.Messages.MessageParam[] = [];
  let pendingToolResults: Anthropic.ToolResultBlockParam[] = [];

  for (const message of oaiMessages) {
    const index = oaiMessages.indexOf(message);

    // refs: https://docs.anthropic.com/claude/docs/tool-use#tool-use-and-tool-result-content-blocks
    if (message.role === 'tool') {
      pendingToolResults.push({
        content: [{ text: message.content as string, type: 'text' }],
        tool_use_id: message.tool_call_id!,
        type: 'tool_result',
      });

      // If this is the last message or the next message is not a 'tool' message,
      // we add the accumulated tool results as a single 'user' message
      if (index === oaiMessages.length - 1 || oaiMessages[index + 1].role !== 'tool') {
        messages.push({
          content: pendingToolResults,
          role: 'user',
        });
        pendingToolResults = [];
      }
    } else {
      const anthropicMessage = await buildAnthropicMessage(message);

      messages.push({
        ...anthropicMessage,
        role: index === 0 && anthropicMessage.role === 'assistant' ? 'user' : anthropicMessage.role,
      });
    }
  }

  return messages;
};

export const buildAnthropicTools = (tools?: OpenAI.ChatCompletionTool[]) =>
  tools?.map(
    (tool): Anthropic.Tool => ({
      description: tool.function.description,
      input_schema: tool.function.parameters as Anthropic.Tool.InputSchema,
      name: tool.function.name,
    }),
  );


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/openaiHelpers.ts
================================================================================

import OpenAI from 'openai';

import { imageUrlToBase64 } from '@/utils/imageToBase64';

import { parseDataUri } from './uriParser';

export const convertMessageContent = async (
  content: OpenAI.ChatCompletionContentPart,
): Promise<OpenAI.ChatCompletionContentPart> => {
  if (content.type === 'image_url') {
    const { type } = parseDataUri(content.image_url.url);

    if (type === 'url' && process.env.LLM_VISION_IMAGE_USE_BASE64 === '1') {
      const { base64, mimeType } = await imageUrlToBase64(content.image_url.url);

      return {
        ...content,
        image_url: { ...content.image_url, url: `data:${mimeType};base64,${base64}` },
      };
    }
  }

  return content;
};

export const convertOpenAIMessages = async (messages: OpenAI.ChatCompletionMessageParam[]) => {
  return (await Promise.all(
    messages.map(async (message) => ({
      ...message,
      content:
        typeof message.content === 'string'
          ? message.content
          : await Promise.all(
              (message.content || []).map((c) =>
                convertMessageContent(c as OpenAI.ChatCompletionContentPart),
              ),
            ),
    })),
  )) as OpenAI.ChatCompletionMessageParam[];
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/debugStream.ts
================================================================================

// no need to introduce a package to get the current time as this module is just a debug utility
const getTime = () => {
  const date = new Date();
  return `${date.getFullYear()}-${date.getMonth() + 1}-${date.getDate()} ${date.getHours()}:${date.getMinutes()}:${date.getSeconds()}.${date.getMilliseconds()}`;
};

export const debugStream = async (stream: ReadableStream) => {
  let finished = false;
  let chunk = 0;
  let chunkValue: any;
  const decoder = new TextDecoder();

  const reader = stream.getReader();

  console.log(`[stream start] ${getTime()}`);

  while (!finished) {
    try {
      const { value, done } = await reader.read();

      if (done) {
        console.log(`[stream finished] total chunks: ${chunk}\n`);
        finished = true;
        break;
      }

      chunkValue = value;

      // if the value is ArrayBuffer, we need to decode it
      if ('byteLength' in value) {
        chunkValue = decoder.decode(value, { stream: true });
      } else if (typeof value !== 'string') {
        chunkValue = JSON.stringify(value);
      }

      console.log(`[chunk ${chunk}] ${getTime()}`);
      console.log(chunkValue);
      console.log(`\n`);

      finished = done;
      chunk++;
    } catch (e) {
      finished = true;
      console.error('[debugStream error]', e);
      console.error('[error chunk value:]', chunkValue);
    }
  }
};

export const debugResponse = (response: any) => {
  console.log(`\n[no stream response] ${getTime()}\n`);
  console.log(JSON.stringify(response) + '\n');
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/cloudflareHelpers.ts
================================================================================

import { desensitizeUrl } from '../utils/desensitizeUrl';

class CloudflareStreamTransformer {
  private textDecoder = new TextDecoder();
  private buffer: string = '';

  private parseChunk(chunk: string, controller: TransformStreamDefaultController) {
    const dataPrefix = /^data: /;
    const json = chunk.replace(dataPrefix, '');
    const parsedChunk = JSON.parse(json);
    controller.enqueue(`event: text\n`);
    controller.enqueue(`data: ${JSON.stringify(parsedChunk.response)}\n\n`);
  }

  public async transform(chunk: Uint8Array, controller: TransformStreamDefaultController) {
    let textChunk = this.textDecoder.decode(chunk);
    if (this.buffer.trim() !== '') {
      textChunk = this.buffer + textChunk;
      this.buffer = '';
    }
    const splits = textChunk.split('\n\n');
    for (let i = 0; i < splits.length - 1; i++) {
      if (/\[DONE]/.test(splits[i].trim())) {
        return;
      }
      this.parseChunk(splits[i], controller);
    }
    const lastChunk = splits.at(-1)!;
    if (lastChunk.trim() !== '') {
      this.buffer += lastChunk; // does not need to be trimmed.
    } // else drop.
  }
}

const CF_PROPERTY_NAME = 'property_id';
const DEFAULT_BASE_URL_PREFIX = 'https://api.cloudflare.com';

function fillUrl(accountID: string): string {
  return `${DEFAULT_BASE_URL_PREFIX}/client/v4/accounts/${accountID}/ai/run/`;
}

function desensitizeAccountId(path: string): string {
  return path.replace(/\/[\dA-Fa-f]{32}\//, '/****/');
}

function desensitizeCloudflareUrl(url: string): string {
  const urlObj = new URL(url);
  let { protocol, hostname, port, pathname, search } = urlObj;
  if (url.startsWith(DEFAULT_BASE_URL_PREFIX)) {
    return `${protocol}//${hostname}${port ? `:${port}` : ''}${desensitizeAccountId(pathname)}${search}`;
  } else {
    const desensitizedUrl = desensitizeUrl(`${protocol}//${hostname}${port ? `:${port}` : ''}`);
    if (desensitizedUrl.endsWith('/') && pathname.startsWith('/')) {
      pathname = pathname.slice(1);
    }
    return `${desensitizedUrl}${desensitizeAccountId(pathname)}${search}`;
  }
}

function getModelBeta(model: any): boolean {
  try {
    const betaProperty = model['properties'].filter(
      (property: any) => property[CF_PROPERTY_NAME] === 'beta',
    );
    if (betaProperty.length === 1) {
      return betaProperty[0]['value'] === 'true'; // This is a string now.
    }
    return false;
  } catch {
    return false;
  }
}

function getModelDisplayName(model: any, beta: boolean): string {
  const modelId = model['name'];
  let name = modelId.split('/').at(-1)!;
  if (beta) {
    name += ' (Beta)';
  }
  return name;
}

// eslint-disable-next-line @typescript-eslint/no-unused-vars, unused-imports/no-unused-vars
function getModelFunctionCalling(model: any): boolean {
  try {
    const fcProperty = model['properties'].filter(
      (property: any) => property[CF_PROPERTY_NAME] === 'function_calling',
    );
    if (fcProperty.length === 1) {
      return fcProperty[0]['value'] === 'true';
    }
    return false;
  } catch {
    return false;
  }
}

function getModelTokens(model: any): number | undefined {
  try {
    const tokensProperty = model['properties'].filter(
      (property: any) => property[CF_PROPERTY_NAME] === 'max_total_tokens',
    );
    if (tokensProperty.length === 1) {
      return parseInt(tokensProperty[0]['value']);
    }
    return undefined;
  } catch {
    return undefined;
  }
}

function convertModelManifest(model: any) {
  const modelBeta = getModelBeta(model);
  return {
    description: model['description'],
    displayName: getModelDisplayName(model, modelBeta),
    enabled: !modelBeta,
    functionCall: false, //getModelFunctionCalling(model),
    id: model['name'],
    tokens: getModelTokens(model),
  };
}

export {
  CloudflareStreamTransformer,
  convertModelManifest,
  DEFAULT_BASE_URL_PREFIX,
  desensitizeCloudflareUrl,
  fillUrl,
  getModelBeta,
  getModelDisplayName,
  getModelFunctionCalling,
  getModelTokens,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/handleOpenAIError.ts
================================================================================

import OpenAI from 'openai';

import { AgentRuntimeErrorType } from '../error';

export const handleOpenAIError = (
  error: any,
): { RuntimeError?: 'AgentRuntimeError'; errorResult: any } => {
  let errorResult: any;

  // Check if the error is an OpenAI APIError
  if (error instanceof OpenAI.APIError) {
    // if error is definitely OpenAI APIError, there will be an error object
    if (error.error) {
      errorResult = error.error;
    }
    // Or if there is a cause, we use error cause
    // This often happened when there is a bug of the `openai` package.
    else if (error.cause) {
      errorResult = error.cause;
    }
    // if there is no other request error, the error object is a Response like object
    else {
      errorResult = { headers: error.headers, stack: error.stack, status: error.status };
    }

    return {
      errorResult,
    };
  } else {
    const err = error as Error;

    errorResult = { cause: err.cause, message: err.message, name: err.name, stack: err.stack };

    return {
      RuntimeError: AgentRuntimeErrorType.AgentRuntimeError,
      errorResult,
    };
  }
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/ollama.ts
================================================================================

import { ChatResponse } from 'ollama/browser';

import { ChatStreamCallbacks } from '@/libs/agent-runtime';
import { nanoid } from '@/utils/uuid';

import {
  StreamProtocolChunk,
  StreamStack,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
  generateToolCallId,
} from './protocol';

const transformOllamaStream = (chunk: ChatResponse, stack: StreamStack): StreamProtocolChunk => {
  // maybe need another structure to add support for multiple choices
  if (chunk.done && !chunk.message.content) {
    return { data: 'finished', id: stack.id, type: 'stop' };
  }

  if (chunk.message.tool_calls && chunk.message.tool_calls.length > 0) {
    return {
      data: chunk.message.tool_calls.map((value, index) => ({
        function: {
          arguments: JSON.stringify(value.function?.arguments) ?? '{}',
          name: value.function?.name ?? null,
        },
        id: generateToolCallId(index, value.function?.name),
        index: index,
        type: 'function',
      })),
      id: stack.id,
      type: 'tool_calls',
    };
  }
  return { data: chunk.message.content, id: stack.id, type: 'text' };
};

export const OllamaStream = (
  res: ReadableStream<ChatResponse>,
  cb?: ChatStreamCallbacks,
): ReadableStream<string> => {
  const streamStack: StreamStack = { id: 'chat_' + nanoid() };

  return res
    .pipeThrough(createSSEProtocolTransformer(transformOllamaStream, streamStack))
    .pipeThrough(createCallbacksTransformer(cb));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/spark.ts
================================================================================

import OpenAI from 'openai';
import type { Stream } from 'openai/streaming';

import { ChatStreamCallbacks } from '../../types';
import {
  StreamProtocolChunk,
  StreamProtocolToolCallChunk,
  convertIterableToStream,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
  generateToolCallId,
} from './protocol';

export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
  return new ReadableStream({
    start(controller) {
      const chunk: OpenAI.ChatCompletionChunk = {
        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => {
          const toolCallsArray = choice.message.tool_calls
            ? Array.isArray(choice.message.tool_calls)
              ? choice.message.tool_calls
              : [choice.message.tool_calls]
            : []; // 如果不是数组，包装成数组

          return {
            delta: {
              content: choice.message.content,
              role: choice.message.role,
              tool_calls: toolCallsArray.map(
                (tool, index): OpenAI.ChatCompletionChunk.Choice.Delta.ToolCall => ({
                  function: tool.function,
                  id: tool.id,
                  index,
                  type: tool.type,
                }),
              ),
            },
            finish_reason: null,
            index: choice.index,
            logprobs: choice.logprobs,
          };
        }),
        created: data.created,
        id: data.id,
        model: data.model,
        object: 'chat.completion.chunk',
      };

      controller.enqueue(chunk);

      controller.enqueue({
        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
          delta: {
            content: null,
            role: choice.message.role,
          },
          finish_reason: choice.finish_reason,
          index: choice.index,
          logprobs: choice.logprobs,
        })),
        created: data.created,
        id: data.id,
        model: data.model,
        object: 'chat.completion.chunk',
        system_fingerprint: data.system_fingerprint,
      } as OpenAI.ChatCompletionChunk);
      controller.close();
    },
  });
}

export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamProtocolChunk => {
  const item = chunk.choices[0];

  if (!item) {
    return { data: chunk, id: chunk.id, type: 'data' };
  }

  if (item.delta?.tool_calls) {
    const toolCallsArray = Array.isArray(item.delta.tool_calls)
      ? item.delta.tool_calls
      : [item.delta.tool_calls]; // 如果不是数组，包装成数组

    if (toolCallsArray.length > 0) {
      return {
        data: toolCallsArray.map((toolCall, index) => ({
          function: toolCall.function,
          id: toolCall.id || generateToolCallId(index, toolCall.function?.name),
          index: typeof toolCall.index !== 'undefined' ? toolCall.index : index,
          type: toolCall.type || 'function',
        })),
        id: chunk.id,
        type: 'tool_calls',
      } as StreamProtocolToolCallChunk;
    }
  }

  if (item.finish_reason) {
    // one-api 的流式接口，会出现既有 finish_reason ，也有 content 的情况
    //  {"id":"demo","model":"deepl-en","choices":[{"index":0,"delta":{"role":"assistant","content":"Introduce yourself."},"finish_reason":"stop"}]}

    if (typeof item.delta?.content === 'string' && !!item.delta.content) {
      return { data: item.delta.content, id: chunk.id, type: 'text' };
    }

    return { data: item.finish_reason, id: chunk.id, type: 'stop' };
  }

  if (typeof item.delta?.content === 'string') {
    return { data: item.delta.content, id: chunk.id, type: 'text' };
  }

  if (item.delta?.content === null) {
    return { data: item.delta, id: chunk.id, type: 'data' };
  }

  return {
    data: { delta: item.delta, id: chunk.id, index: item.index },
    id: chunk.id,
    type: 'data',
  };
};

export const SparkAIStream = (
  stream: Stream<OpenAI.ChatCompletionChunk> | ReadableStream,
  callbacks?: ChatStreamCallbacks,
) => {
  const readableStream =
    stream instanceof ReadableStream ? stream : convertIterableToStream(stream);

  return readableStream
    .pipeThrough(createSSEProtocolTransformer(transformSparkStream))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/anthropic.ts
================================================================================

import Anthropic from '@anthropic-ai/sdk';
import type { Stream } from '@anthropic-ai/sdk/streaming';

import { ChatStreamCallbacks } from '../../types';
import {
  StreamProtocolChunk,
  StreamProtocolToolCallChunk,
  StreamStack,
  StreamToolCallChunkData,
  convertIterableToStream,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
} from './protocol';

export const transformAnthropicStream = (
  chunk: Anthropic.MessageStreamEvent,
  stack: StreamStack,
): StreamProtocolChunk => {
  // maybe need another structure to add support for multiple choices
  switch (chunk.type) {
    case 'message_start': {
      stack.id = chunk.message.id;
      return { data: chunk.message, id: chunk.message.id, type: 'data' };
    }
    case 'content_block_start': {
      if (chunk.content_block.type === 'tool_use') {
        const toolChunk = chunk.content_block;

        // if toolIndex is not defined, set it to 0
        if (typeof stack.toolIndex === 'undefined') {
          stack.toolIndex = 0;
        }
        // if toolIndex is defined, increment it
        else {
          stack.toolIndex += 1;
        }

        const toolCall: StreamToolCallChunkData = {
          function: {
            arguments: '',
            name: toolChunk.name,
          },
          id: toolChunk.id,
          index: stack.toolIndex,
          type: 'function',
        };

        stack.tool = { id: toolChunk.id, index: stack.toolIndex, name: toolChunk.name };

        return { data: [toolCall], id: stack.id, type: 'tool_calls' };
      }

      return { data: chunk.content_block.text, id: stack.id, type: 'data' };
    }

    case 'content_block_delta': {
      switch (chunk.delta.type) {
        case 'text_delta': {
          return { data: chunk.delta.text, id: stack.id, type: 'text' };
        }

        case 'input_json_delta': {
          const delta = chunk.delta.partial_json;

          const toolCall: StreamToolCallChunkData = {
            function: { arguments: delta },
            index: stack.toolIndex || 0,
            type: 'function',
          };

          return {
            data: [toolCall],
            id: stack.id,
            type: 'tool_calls',
          } as StreamProtocolToolCallChunk;
        }

        default: {
          break;
        }
      }
      return { data: chunk, id: stack.id, type: 'data' };
    }

    case 'message_delta': {
      return { data: chunk.delta.stop_reason, id: stack.id, type: 'stop' };
    }

    case 'message_stop': {
      return { data: 'message_stop', id: stack.id, type: 'stop' };
    }

    default: {
      return { data: chunk, id: stack.id, type: 'data' };
    }
  }
};

export const AnthropicStream = (
  stream: Stream<Anthropic.MessageStreamEvent> | ReadableStream,
  callbacks?: ChatStreamCallbacks,
) => {
  const streamStack: StreamStack = { id: '' };

  const readableStream =
    stream instanceof ReadableStream ? stream : convertIterableToStream(stream);

  return readableStream
    .pipeThrough(createSSEProtocolTransformer(transformAnthropicStream, streamStack))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/qwen.ts
================================================================================

import { ChatCompletionContentPartText } from 'ai/prompts';
import OpenAI from 'openai';
import { ChatCompletionContentPart } from 'openai/resources/index.mjs';
import type { Stream } from 'openai/streaming';

import { ChatStreamCallbacks } from '../../types';
import {
  StreamProtocolChunk,
  StreamProtocolToolCallChunk,
  StreamToolCallChunkData,
  convertIterableToStream,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
  generateToolCallId,
} from './protocol';

export const transformQwenStream = (chunk: OpenAI.ChatCompletionChunk): StreamProtocolChunk => {
  const item = chunk.choices[0];

  if (!item) {
    return { data: chunk, id: chunk.id, type: 'data' };
  }

  if (Array.isArray(item.delta?.content)) {
    const part = item.delta.content[0];
    const process = (part: ChatCompletionContentPart): ChatCompletionContentPartText => {
      let [key, value] = Object.entries(part)[0];
      if (key === 'image') {
        return {
          text: `![image](${value})`,
          type: 'text',
        };
      }
      return {
        text: value,
        type: 'text',
      };
    };

    const data = process(part);

    return {
      data: data.text,
      id: chunk.id,
      type: 'text',
    };
  }

  if (item.delta?.tool_calls) {
    return {
      data: item.delta.tool_calls.map(
        (value, index): StreamToolCallChunkData => ({
          function: value.function,
          id: value.id || generateToolCallId(index, value.function?.name),
          index: typeof value.index !== 'undefined' ? value.index : index,
          type: value.type || 'function',
        }),
      ),
      id: chunk.id,
      type: 'tool_calls',
    } as StreamProtocolToolCallChunk;
  }

  if (typeof item.delta?.content === 'string') {
    return { data: item.delta.content, id: chunk.id, type: 'text' };
  }

  if (item.finish_reason) {
    return { data: item.finish_reason, id: chunk.id, type: 'stop' };
  }

  if (item.delta?.content === null) {
    return { data: item.delta, id: chunk.id, type: 'data' };
  }

  return {
    data: { delta: item.delta, id: chunk.id, index: item.index },
    id: chunk.id,
    type: 'data',
  };
};

export const QwenAIStream = (
  stream: Stream<OpenAI.ChatCompletionChunk> | ReadableStream,
  callbacks?: ChatStreamCallbacks,
) => {
  const readableStream =
    stream instanceof ReadableStream ? stream : convertIterableToStream(stream);

  return readableStream
    .pipeThrough(createSSEProtocolTransformer(transformQwenStream))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/openai.ts
================================================================================

import OpenAI from 'openai';
import type { Stream } from 'openai/streaming';

import { ChatMessageError } from '@/types/message';

import { AgentRuntimeErrorType, ILobeAgentRuntimeErrorType } from '../../error';
import { ChatStreamCallbacks } from '../../types';
import {
  FIRST_CHUNK_ERROR_KEY,
  StreamProtocolChunk,
  StreamProtocolToolCallChunk,
  StreamStack,
  StreamToolCallChunkData,
  convertIterableToStream,
  createCallbacksTransformer,
  createFirstErrorHandleTransformer,
  createSSEProtocolTransformer,
  generateToolCallId,
} from './protocol';

export const transformOpenAIStream = (
  chunk: OpenAI.ChatCompletionChunk,
  stack?: StreamStack,
): StreamProtocolChunk => {
  // handle the first chunk error
  if (FIRST_CHUNK_ERROR_KEY in chunk) {
    delete chunk[FIRST_CHUNK_ERROR_KEY];
    // @ts-ignore
    delete chunk['name'];
    // @ts-ignore
    delete chunk['stack'];

    const errorData = {
      body: chunk,
      type: 'errorType' in chunk ? chunk.errorType : AgentRuntimeErrorType.ProviderBizError,
    } as ChatMessageError;
    return { data: errorData, id: 'first_chunk_error', type: 'error' };
  }

  // maybe need another structure to add support for multiple choices

  try {
    const item = chunk.choices[0];
    if (!item) {
      return { data: chunk, id: chunk.id, type: 'data' };
    }

    if (typeof item.delta?.tool_calls === 'object' && item.delta.tool_calls?.length > 0) {
      return {
        data: item.delta.tool_calls.map((value, index): StreamToolCallChunkData => {
          if (stack && !stack.tool) {
            stack.tool = { id: value.id!, index: value.index, name: value.function!.name! };
          }

          return {
            function: {
              arguments: value.function?.arguments ?? '{}',
              name: value.function?.name ?? null,
            },
            id: value.id || stack?.tool?.id || generateToolCallId(index, value.function?.name),

            // mistral's tool calling don't have index and function field, it's data like:
            // [{"id":"xbhnmTtY7","function":{"name":"lobe-image-designer____text2image____builtin","arguments":"{\"prompts\": [\"A photo of a small, fluffy dog with a playful expression and wagging tail.\", \"A watercolor painting of a small, energetic dog with a glossy coat and bright eyes.\", \"A vector illustration of a small, adorable dog with a short snout and perky ears.\", \"A drawing of a small, scruffy dog with a mischievous grin and a wagging tail.\"], \"quality\": \"standard\", \"seeds\": [123456, 654321, 111222, 333444], \"size\": \"1024x1024\", \"style\": \"vivid\"}"}}]

            // minimax's tool calling don't have index field, it's data like:
            // [{"id":"call_function_4752059746","type":"function","function":{"name":"lobe-image-designer____text2image____builtin","arguments":"{\"prompts\": [\"一个流浪的地球，背景是浩瀚"}}]

            // so we need to add these default values
            index: typeof value.index !== 'undefined' ? value.index : index,
            type: value.type || 'function',
          };
        }),
        id: chunk.id,
        type: 'tool_calls',
      } as StreamProtocolToolCallChunk;
    }

    // 给定结束原因
    if (item.finish_reason) {
      // one-api 的流式接口，会出现既有 finish_reason ，也有 content 的情况
      //  {"id":"demo","model":"deepl-en","choices":[{"index":0,"delta":{"role":"assistant","content":"Introduce yourself."},"finish_reason":"stop"}]}

      if (typeof item.delta?.content === 'string' && !!item.delta.content) {
        return { data: item.delta.content, id: chunk.id, type: 'text' };
      }

      return { data: item.finish_reason, id: chunk.id, type: 'stop' };
    }

    if (typeof item.delta?.content === 'string') {
      return { data: item.delta.content, id: chunk.id, type: 'text' };
    }

    if (item.delta?.content === null) {
      return { data: item.delta, id: chunk.id, type: 'data' };
    }

    // 其余情况下，返回 delta 和 index
    return {
      data: { delta: item.delta, id: chunk.id, index: item.index },
      id: chunk.id,
      type: 'data',
    };
  } catch (e) {
    const errorName = 'StreamChunkError';
    console.error(`[${errorName}]`, e);
    console.error(`[${errorName}] raw chunk:`, chunk);

    const err = e as Error;

    /* eslint-disable sort-keys-fix/sort-keys-fix */
    const errorData = {
      body: {
        message:
          'chat response streaming chunk parse error, please contact your API Provider to fix it.',
        context: { error: { message: err.message, name: err.name }, chunk },
      },
      type: errorName,
    } as ChatMessageError;
    /* eslint-enable */

    return { data: errorData, id: chunk.id, type: 'error' };
  }
};

export interface OpenAIStreamOptions {
  bizErrorTypeTransformer?: (error: {
    message: string;
    name: string;
  }) => ILobeAgentRuntimeErrorType | undefined;
  callbacks?: ChatStreamCallbacks;
  provider?: string;
}

export const OpenAIStream = (
  stream: Stream<OpenAI.ChatCompletionChunk> | ReadableStream,
  { callbacks, provider, bizErrorTypeTransformer }: OpenAIStreamOptions = {},
) => {
  const streamStack: StreamStack = { id: '' };

  const readableStream =
    stream instanceof ReadableStream ? stream : convertIterableToStream(stream);

  return (
    readableStream
      // 1. handle the first error if exist
      // provider like huggingface or minimax will return error in the stream,
      // so in the first Transformer, we need to handle the error
      .pipeThrough(createFirstErrorHandleTransformer(bizErrorTypeTransformer, provider))
      .pipeThrough(createSSEProtocolTransformer(transformOpenAIStream, streamStack))
      .pipeThrough(createCallbacksTransformer(callbacks))
  );
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/wenxin.ts
================================================================================

import { ChatStreamCallbacks } from '@/libs/agent-runtime';
import { nanoid } from '@/utils/uuid';

import { ChatResp } from '../../wenxin/type';
import {
  StreamProtocolChunk,
  StreamStack,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
} from './protocol';

const transformERNIEBotStream = (chunk: ChatResp): StreamProtocolChunk => {
  const finished = chunk.is_end;
  if (finished) {
    return { data: chunk.finish_reason || 'stop', id: chunk.id, type: 'stop' };
  }

  if (chunk.result) {
    return { data: chunk.result, id: chunk.id, type: 'text' };
  }

  return {
    data: chunk,
    id: chunk.id,
    type: 'data',
  };
};

export const WenxinStream = (
  rawStream: ReadableStream<ChatResp>,
  callbacks?: ChatStreamCallbacks,
) => {
  const streamStack: StreamStack = { id: 'chat_' + nanoid() };

  return rawStream
    .pipeThrough(createSSEProtocolTransformer(transformERNIEBotStream, streamStack))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/protocol.ts
================================================================================

import { ChatStreamCallbacks } from '@/libs/agent-runtime';

import { AgentRuntimeErrorType } from '../../error';

export interface StreamStack {
  id: string;
  tool?: {
    id: string;
    index: number;
    name: string;
  };
  toolIndex?: number;
}

export interface StreamProtocolChunk {
  data: any;
  id?: string;
  type: 'text' | 'tool_calls' | 'data' | 'stop' | 'error';
}

export interface StreamToolCallChunkData {
  function?: {
    arguments?: string;
    name?: string | null;
  };
  id?: string;
  index: number;
  type: 'function' | string;
}

export interface StreamProtocolToolCallChunk {
  data: StreamToolCallChunkData[];
  id: string;
  index: number;
  type: 'tool_calls';
}

export const generateToolCallId = (index: number, functionName?: string) =>
  `${functionName || 'unknown_tool_call'}_${index}`;

const chatStreamable = async function* <T>(stream: AsyncIterable<T>) {
  for await (const response of stream) {
    yield response;
  }
};

const ERROR_CHUNK_PREFIX = '%FIRST_CHUNK_ERROR%: ';
// make the response to the streamable format
export const convertIterableToStream = <T>(stream: AsyncIterable<T>) => {
  const iterable = chatStreamable(stream);

  // copy from https://github.com/vercel/ai/blob/d3aa5486529e3d1a38b30e3972b4f4c63ea4ae9a/packages/ai/streams/ai-stream.ts#L284
  // and add an error handle
  let it = iterable[Symbol.asyncIterator]();

  return new ReadableStream<T>({
    async cancel(reason) {
      await it.return?.(reason);
    },
    async pull(controller) {
      const { done, value } = await it.next();
      if (done) controller.close();
      else controller.enqueue(value);
    },

    async start(controller) {
      try {
        const { done, value } = await it.next();
        if (done) controller.close();
        else controller.enqueue(value);
      } catch (e) {
        const error = e as Error;

        controller.enqueue(
          (ERROR_CHUNK_PREFIX +
            JSON.stringify({ message: error.message, name: error.name, stack: error.stack })) as T,
        );
        controller.close();
      }
    },
  });
};

/**
 * Create a transformer to convert the response into an SSE format
 */
export const createSSEProtocolTransformer = (
  transformer: (chunk: any, stack: StreamStack) => StreamProtocolChunk,
  streamStack?: StreamStack,
) =>
  new TransformStream({
    transform: (chunk, controller) => {
      const { type, id, data } = transformer(chunk, streamStack || { id: '' });

      controller.enqueue(`id: ${id}\n`);
      controller.enqueue(`event: ${type}\n`);
      controller.enqueue(`data: ${JSON.stringify(data)}\n\n`);
    },
  });

export function createCallbacksTransformer(cb: ChatStreamCallbacks | undefined) {
  const textEncoder = new TextEncoder();
  let aggregatedResponse = '';
  let currentType = '';
  const callbacks = cb || {};

  return new TransformStream({
    async flush(): Promise<void> {
      if (callbacks.onCompletion) {
        await callbacks.onCompletion(aggregatedResponse);
      }

      if (callbacks.onFinal) {
        await callbacks.onFinal(aggregatedResponse);
      }
    },

    async start(): Promise<void> {
      if (callbacks.onStart) await callbacks.onStart();
    },

    async transform(chunk: string, controller): Promise<void> {
      controller.enqueue(textEncoder.encode(chunk));

      // track the type of the chunk
      if (chunk.startsWith('event:')) {
        currentType = chunk.split('event:')[1].trim();
      }
      // if the message is a data chunk, handle the callback
      else if (chunk.startsWith('data:')) {
        const content = chunk.split('data:')[1].trim();

        switch (currentType) {
          case 'text': {
            await callbacks.onText?.(content);
            await callbacks.onToken?.(JSON.parse(content));
            break;
          }

          case 'tool_calls': {
            // TODO: make on ToolCall callback
            await callbacks.onToolCall?.();
          }
        }
      }
    },
  });
}

export const FIRST_CHUNK_ERROR_KEY = '_isFirstChunkError';

export const createFirstErrorHandleTransformer = (
  errorHandler?: (errorJson: any) => any,
  provider?: string,
) => {
  return new TransformStream({
    transform(chunk, controller) {
      if (chunk.toString().startsWith(ERROR_CHUNK_PREFIX)) {
        const errorData = JSON.parse(chunk.toString().replace(ERROR_CHUNK_PREFIX, ''));

        controller.enqueue({
          ...errorData,
          [FIRST_CHUNK_ERROR_KEY]: true,
          errorType: errorHandler?.(errorData) || AgentRuntimeErrorType.ProviderBizError,
          provider,
        });
      } else {
        controller.enqueue(chunk);
      }
    },
  });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/index.ts
================================================================================

export * from './anthropic';
export * from './azureOpenai';
export * from './bedrock';
export * from './google-ai';
export * from './minimax';
export * from './ollama';
export * from './openai';
export * from './protocol';
export * from './qwen';
export * from './spark';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/minimax.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamCallbacks } from '../../types';
import { transformOpenAIStream } from './openai';
import { createCallbacksTransformer, createSSEProtocolTransformer } from './protocol';

export const processDoubleData = (chunkValue: string): string => {
  const dataPattern = /data: {"id":"/g;
  const matchCount = (chunkValue.match(dataPattern) || []).length;
  let modifiedChunkValue = chunkValue;
  if (matchCount === 2) {
    const secondDataIdIndex = chunkValue.indexOf(
      'data: {"id":',
      chunkValue.indexOf('data: {"id":') + 1,
    );
    if (secondDataIdIndex !== -1) {
      modifiedChunkValue = chunkValue.slice(0, secondDataIdIndex).trim();
    }
  }
  return modifiedChunkValue;
};

const unit8ArrayToJSONChunk = (unit8Array: Uint8Array): OpenAI.ChatCompletionChunk => {
  const decoder = new TextDecoder();

  let chunkValue = decoder.decode(unit8Array, { stream: true });

  // chunkValue example:
  // data: {"id":"028a65377137d57aaceeffddf48ae99f","choices":[{"finish_reason":"tool_calls","index":0,"delta":{"role":"assistant","tool_calls":[{"id":"call_function_7371372822","type":"function","function":{"name":"realtime-weather____fetchCurrentWeather","arguments":"{\"city\": [\"杭州\", \"北京\"]}"}}]}}],"created":155511,"model":"abab6.5s-chat","object":"chat.completion.chunk"}

  chunkValue = processDoubleData(chunkValue);

  // so we need to remove `data:` prefix and then parse it as JSON
  if (chunkValue.startsWith('data:')) {
    chunkValue = chunkValue.slice(5).trim();
  }

  try {
    return JSON.parse(chunkValue);
  } catch (e) {
    console.error('minimax chunk parse error:', e);

    return { raw: chunkValue } as any;
  }
};

export const MinimaxStream = (stream: ReadableStream, callbacks?: ChatStreamCallbacks) => {
  return stream
    .pipeThrough(
      createSSEProtocolTransformer((buffer) => {
        const chunk = unit8ArrayToJSONChunk(buffer);

        return transformOpenAIStream(chunk);
      }),
    )
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/google-ai.ts
================================================================================

import { EnhancedGenerateContentResponse } from '@google/generative-ai';

import { nanoid } from '@/utils/uuid';

import { ChatStreamCallbacks } from '../../types';
import {
  StreamProtocolChunk,
  StreamStack,
  StreamToolCallChunkData,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
  generateToolCallId,
} from './protocol';

const transformGoogleGenerativeAIStream = (
  chunk: EnhancedGenerateContentResponse,
  stack: StreamStack,
): StreamProtocolChunk => {
  // maybe need another structure to add support for multiple choices
  const functionCalls = chunk.functionCalls();

  if (functionCalls) {
    return {
      data: functionCalls.map(
        (value, index): StreamToolCallChunkData => ({
          function: {
            arguments: JSON.stringify(value.args),
            name: value.name,
          },
          id: generateToolCallId(index, value.name),
          index: index,
          type: 'function',
        }),
      ),
      id: stack.id,
      type: 'tool_calls',
    };
  }
  const text = chunk.text();

  return {
    data: text,
    id: stack?.id,
    type: 'text',
  };
};

export const GoogleGenerativeAIStream = (
  rawStream: ReadableStream<EnhancedGenerateContentResponse>,
  callbacks?: ChatStreamCallbacks,
) => {
  const streamStack: StreamStack = { id: 'chat_' + nanoid() };

  return rawStream
    .pipeThrough(createSSEProtocolTransformer(transformGoogleGenerativeAIStream, streamStack))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/azureOpenai.ts
================================================================================

import { ChatCompletions, ChatCompletionsFunctionToolCall } from '@azure/openai';
import OpenAI from 'openai';
import type { Stream } from 'openai/streaming';

import { ChatStreamCallbacks } from '../../types';
import {
  StreamProtocolChunk,
  StreamProtocolToolCallChunk,
  StreamStack,
  StreamToolCallChunkData,
  convertIterableToStream,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
} from './protocol';

const transformOpenAIStream = (chunk: ChatCompletions, stack: StreamStack): StreamProtocolChunk => {
  // maybe need another structure to add support for multiple choices

  const item = chunk.choices[0];
  if (!item) {
    return { data: chunk, id: chunk.id, type: 'data' };
  }

  if (typeof item.delta?.content === 'string') {
    return { data: item.delta.content, id: chunk.id, type: 'text' };
  }

  if (item.delta?.toolCalls) {
    return {
      data: item.delta.toolCalls.map((value, index): StreamToolCallChunkData => {
        const func = (value as ChatCompletionsFunctionToolCall).function;

        // at first time, set tool id
        if (!stack.tool) {
          stack.tool = { id: value.id, index, name: func.name };
        } else {
          // in the parallel tool calling, set the new tool id
          if (value.id && stack.tool.id !== value.id) {
            stack.tool = { id: value.id, index, name: func.name };
          }
        }

        return {
          function: func,
          id: value.id || stack.tool?.id,
          index: value.index || index,
          type: value.type || 'function',
        };
      }),
      id: chunk.id,
      type: 'tool_calls',
    } as StreamProtocolToolCallChunk;
  }

  // 给定结束原因
  if (item.finishReason) {
    return { data: item.finishReason, id: chunk.id, type: 'stop' };
  }

  if (item.delta?.content === null) {
    return { data: item.delta, id: chunk.id, type: 'data' };
  }

  // 其余情况下，返回 delta 和 index
  return {
    data: { delta: item.delta, id: chunk.id, index: item.index },
    id: chunk.id,
    type: 'data',
  };
};

export const AzureOpenAIStream = (
  stream: Stream<OpenAI.ChatCompletionChunk> | ReadableStream,
  callbacks?: ChatStreamCallbacks,
) => {
  const stack: StreamStack = { id: '' };
  const readableStream =
    stream instanceof ReadableStream ? stream : convertIterableToStream(stream);

  return readableStream
    .pipeThrough(createSSEProtocolTransformer(transformOpenAIStream, stack))
    .pipeThrough(createCallbacksTransformer(callbacks));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/bedrock/common.ts
================================================================================

import {
  InvokeModelWithResponseStreamResponse,
  ResponseStream,
} from '@aws-sdk/client-bedrock-runtime';
import { readableFromAsyncIterable } from 'ai';

const chatStreamable = async function* (stream: AsyncIterable<ResponseStream>) {
  for await (const response of stream) {
    if (response.chunk) {
      const decoder = new TextDecoder();

      const value = decoder.decode(response.chunk.bytes, { stream: true });
      try {
        const chunk = JSON.parse(value);

        yield chunk;
      } catch (e) {
        console.log('bedrock stream parser error:', e);

        yield value;
      }
    } else {
      yield response;
    }
  }
};

/**
 * covert the bedrock response to a readable stream
 */
export const createBedrockStream = (res: InvokeModelWithResponseStreamResponse) =>
  readableFromAsyncIterable(chatStreamable(res.body!));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/streams/bedrock/llama.ts
================================================================================

import { InvokeModelWithResponseStreamResponse } from '@aws-sdk/client-bedrock-runtime';

import { nanoid } from '@/utils/uuid';

import { ChatStreamCallbacks } from '../../../types';
import {
  StreamProtocolChunk,
  StreamStack,
  createCallbacksTransformer,
  createSSEProtocolTransformer,
} from '../protocol';
import { createBedrockStream } from './common';

interface AmazonBedrockInvocationMetrics {
  firstByteLatency: number;
  inputTokenCount: number;
  invocationLatency: number;
  outputTokenCount: number;
}
interface BedrockLlamaStreamChunk {
  'amazon-bedrock-invocationMetrics'?: AmazonBedrockInvocationMetrics;
  'generation': string;
  'generation_token_count': number;
  'prompt_token_count'?: number | null;
  'stop_reason'?: null | 'stop' | string;
}

export const transformLlamaStream = (
  chunk: BedrockLlamaStreamChunk,
  stack: StreamStack,
): StreamProtocolChunk => {
  // maybe need another structure to add support for multiple choices
  if (chunk.stop_reason) {
    return { data: 'finished', id: stack.id, type: 'stop' };
  }

  return { data: chunk.generation, id: stack.id, type: 'text' };
};

export const AWSBedrockLlamaStream = (
  res: InvokeModelWithResponseStreamResponse | ReadableStream,
  cb?: ChatStreamCallbacks,
): ReadableStream<string> => {
  const streamStack: StreamStack = { id: 'chat_' + nanoid() };

  const stream = res instanceof ReadableStream ? res : createBedrockStream(res);

  return stream
    .pipeThrough(createSSEProtocolTransformer(transformLlamaStream, streamStack))
    .pipeThrough(createCallbacksTransformer(cb));
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/utils/openaiCompatibleFactory/index.ts
================================================================================

import OpenAI, { ClientOptions } from 'openai';
import { Stream } from 'openai/streaming';

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
import type { ChatModelCard } from '@/types/llm';

import { LobeRuntimeAI } from '../../BaseAI';
import { AgentRuntimeErrorType, ILobeAgentRuntimeErrorType } from '../../error';
import type {
  ChatCompetitionOptions,
  ChatCompletionErrorPayload,
  ChatStreamPayload,
  Embeddings,
  EmbeddingsOptions,
  EmbeddingsPayload,
  ModelProvider,
  TextToImagePayload,
  TextToSpeechOptions,
  TextToSpeechPayload,
} from '../../types';
import { AgentRuntimeError } from '../createError';
import { debugResponse, debugStream } from '../debugStream';
import { desensitizeUrl } from '../desensitizeUrl';
import { handleOpenAIError } from '../handleOpenAIError';
import { convertOpenAIMessages } from '../openaiHelpers';
import { StreamingResponse } from '../response';
import { OpenAIStream, OpenAIStreamOptions } from '../streams';
import { ChatStreamCallbacks } from '../../types';

// the model contains the following keywords is not a chat model, so we should filter them out
export const CHAT_MODELS_BLOCK_LIST = [
  'embedding',
  'davinci',
  'curie',
  'moderation',
  'ada',
  'babbage',
  'tts',
  'whisper',
  'dall-e',
];

type ConstructorOptions<T extends Record<string, any> = any> = ClientOptions & T;

export interface CustomClientOptions<T extends Record<string, any> = any> {
  createChatCompletionStream?: (
    client: any,
    payload: ChatStreamPayload,
    instance: any,
  ) => ReadableStream<any>;
  createClient?: (options: ConstructorOptions<T>) => any;
}

interface OpenAICompatibleFactoryOptions<T extends Record<string, any> = any> {
  apiKey?: string;
  baseURL?: string;
  chatCompletion?: {
    handleError?: (
      error: any,
      options: ConstructorOptions<T>,
    ) => Omit<ChatCompletionErrorPayload, 'provider'> | undefined;
    handlePayload?: (
      payload: ChatStreamPayload,
      options: ConstructorOptions<T>,
    ) => OpenAI.ChatCompletionCreateParamsStreaming;
    handleStream?: (
      stream: Stream<OpenAI.ChatCompletionChunk> | ReadableStream,
      callbacks?: ChatStreamCallbacks,
    ) => ReadableStream;
    handleStreamBizErrorType?: (error: {
      message: string;
      name: string;
    }) => ILobeAgentRuntimeErrorType | undefined;
    handleTransformResponseToStream?: (
      data: OpenAI.ChatCompletion,
    ) => ReadableStream<OpenAI.ChatCompletionChunk>;
    noUserId?: boolean;
  };
  constructorOptions?: ConstructorOptions<T>;
  customClient?: CustomClientOptions<T>;
  debug?: {
    chatCompletion: () => boolean;
  };
  errorType?: {
    bizError: ILobeAgentRuntimeErrorType;
    invalidAPIKey: ILobeAgentRuntimeErrorType;
  };
  models?:
    | ((params: { client: OpenAI }) => Promise<ChatModelCard[]>)
    | {
        transformModel?: (model: OpenAI.Model) => ChatModelCard;
      };
  provider: string;
}

/**
 * make the OpenAI response data as a stream
 */
export function transformResponseToStream(data: OpenAI.ChatCompletion) {
  return new ReadableStream({
    start(controller) {
      const chunk: OpenAI.ChatCompletionChunk = {
        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
          delta: {
            content: choice.message.content,
            role: choice.message.role,
            tool_calls: choice.message.tool_calls?.map(
              (tool, index): OpenAI.ChatCompletionChunk.Choice.Delta.ToolCall => ({
                function: tool.function,
                id: tool.id,
                index,
                type: tool.type,
              }),
            ),
          },
          finish_reason: null,
          index: choice.index,
          logprobs: choice.logprobs,
        })),
        created: data.created,
        id: data.id,
        model: data.model,
        object: 'chat.completion.chunk',
      };

      controller.enqueue(chunk);

      controller.enqueue({
        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
          delta: {
            content: null,
            role: choice.message.role,
          },
          finish_reason: choice.finish_reason,
          index: choice.index,
          logprobs: choice.logprobs,
        })),
        created: data.created,
        id: data.id,
        model: data.model,
        object: 'chat.completion.chunk',
        system_fingerprint: data.system_fingerprint,
      } as OpenAI.ChatCompletionChunk);
      controller.close();
    },
  });
}

export const LobeOpenAICompatibleFactory = <T extends Record<string, any> = any>({
  provider,
  baseURL: DEFAULT_BASE_URL,
  apiKey: DEFAULT_API_LEY,
  errorType,
  debug,
  constructorOptions,
  chatCompletion,
  models,
  customClient,
}: OpenAICompatibleFactoryOptions<T>) => {
  const ErrorType = {
    bizError: errorType?.bizError || AgentRuntimeErrorType.ProviderBizError,
    invalidAPIKey: errorType?.invalidAPIKey || AgentRuntimeErrorType.InvalidProviderAPIKey,
  };

  return class LobeOpenAICompatibleAI implements LobeRuntimeAI {
    client!: OpenAI;

    baseURL!: string;
    protected _options: ConstructorOptions<T>;

    constructor(options: ClientOptions & Record<string, any> = {}) {
      const _options = {
        ...options,
        apiKey: options.apiKey?.trim() || DEFAULT_API_LEY,
        baseURL: options.baseURL?.trim() || DEFAULT_BASE_URL,
      };
      const { apiKey, baseURL = DEFAULT_BASE_URL, ...res } = _options;
      this._options = _options as ConstructorOptions<T>;

      if (!apiKey) throw AgentRuntimeError.createError(ErrorType?.invalidAPIKey);

      const initOptions = { apiKey, baseURL, ...constructorOptions, ...res };

      // if the custom client is provided, use it as client
      if (customClient?.createClient) {
        this.client = customClient.createClient(initOptions as any);
      } else {
        this.client = new OpenAI(initOptions);
      }

      this.baseURL = baseURL || this.client.baseURL;
    }

    async chat({ responseMode, ...payload }: ChatStreamPayload, options?: ChatCompetitionOptions) {
      try {
        const postPayload = chatCompletion?.handlePayload
          ? chatCompletion.handlePayload(payload, this._options)
          : ({
              ...payload,
              stream: payload.stream ?? true,
            } as OpenAI.ChatCompletionCreateParamsStreaming);

        const messages = await convertOpenAIMessages(postPayload.messages);

        let response: Stream<OpenAI.Chat.Completions.ChatCompletionChunk>;

        const streamOptions: OpenAIStreamOptions = {
          bizErrorTypeTransformer: chatCompletion?.handleStreamBizErrorType,
          callbacks: options?.callback,
          provider,
        };
        if (customClient?.createChatCompletionStream) {
          response = customClient.createChatCompletionStream(this.client, payload, this) as any;
        } else {
          response = await this.client.chat.completions.create(
            {
              ...postPayload,
              messages,
              ...(chatCompletion?.noUserId ? {} : { user: options?.user }),
            },
            {
              // https://github.com/lobehub/lobe-chat/pull/318
              headers: { Accept: '*/*', ...options?.requestHeaders },
              signal: options?.signal,
            },
          );
        }

        if (postPayload.stream) {
          const [prod, useForDebug] = response.tee();

          if (debug?.chatCompletion?.()) {
            const useForDebugStream =
              useForDebug instanceof ReadableStream ? useForDebug : useForDebug.toReadableStream();

            debugStream(useForDebugStream).catch(console.error);
          }

          const streamHandler = chatCompletion?.handleStream || OpenAIStream;
          return StreamingResponse(streamHandler(prod, streamOptions), {
            headers: options?.headers,
          });
        }

        if (debug?.chatCompletion?.()) {
          debugResponse(response);
        }

        if (responseMode === 'json') return Response.json(response);

        const transformHandler = chatCompletion?.handleTransformResponseToStream || transformResponseToStream;
        const stream = transformHandler(response as unknown as OpenAI.ChatCompletion);

        const streamHandler = chatCompletion?.handleStream || OpenAIStream;
        return StreamingResponse(streamHandler(stream, streamOptions), {
          headers: options?.headers,
        });
      } catch (error) {
        throw this.handleError(error);
      }
    }

    async models() {
      if (typeof models === 'function') return models({ client: this.client });

      const list = await this.client.models.list();

      return list.data
        .filter((model) => {
          return CHAT_MODELS_BLOCK_LIST.every(
            (keyword) => !model.id.toLowerCase().includes(keyword),
          );
        })
        .map((item) => {
          if (models?.transformModel) {
            return models.transformModel(item);
          }

          const knownModel = LOBE_DEFAULT_MODEL_LIST.find((model) => model.id === item.id);

          if (knownModel) return knownModel;

          return { id: item.id };
        })

        .filter(Boolean) as ChatModelCard[];
    }

    async embeddings(
      payload: EmbeddingsPayload,
      options?: EmbeddingsOptions,
    ): Promise<Embeddings[]> {
      try {
        const res = await this.client.embeddings.create(
          { ...payload, user: options?.user },
          { headers: options?.headers, signal: options?.signal },
        );

        return res.data.map((item) => item.embedding);
      } catch (error) {
        throw this.handleError(error);
      }
    }

    async textToImage(payload: TextToImagePayload) {
      try {
        const res = await this.client.images.generate(payload);
        return res.data.map((o) => o.url) as string[];
      } catch (error) {
        throw this.handleError(error);
      }
    }

    async textToSpeech(payload: TextToSpeechPayload, options?: TextToSpeechOptions) {
      try {
        const mp3 = await this.client.audio.speech.create(payload as any, {
          headers: options?.headers,
          signal: options?.signal,
        });

        return mp3.arrayBuffer();
      } catch (error) {
        throw this.handleError(error);
      }
    }

    protected handleError(error: any): ChatCompletionErrorPayload {
      let desensitizedEndpoint = this.baseURL;

      // refs: https://github.com/lobehub/lobe-chat/issues/842
      if (this.baseURL !== DEFAULT_BASE_URL) {
        desensitizedEndpoint = desensitizeUrl(this.baseURL);
      }

      if (chatCompletion?.handleError) {
        const errorResult = chatCompletion.handleError(error, this._options);

        if (errorResult)
          return AgentRuntimeError.chat({
            ...errorResult,
            provider,
          } as ChatCompletionErrorPayload);
      }

      if ('status' in (error as any)) {
        switch ((error as Response).status) {
          case 401: {
            return AgentRuntimeError.chat({
              endpoint: desensitizedEndpoint,
              error: error as any,
              errorType: ErrorType.invalidAPIKey,
              provider: provider as ModelProvider,
            });
          }

          default: {
            break;
          }
        }
      }

      const { errorResult, RuntimeError } = handleOpenAIError(error);

      return AgentRuntimeError.chat({
        endpoint: desensitizedEndpoint,
        error: errorResult,
        errorType: RuntimeError || ErrorType.bizError,
        provider: provider as ModelProvider,
      });
    }
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/minimax/index.ts
================================================================================

import { isEmpty } from 'lodash-es';
import OpenAI from 'openai';

import { LobeRuntimeAI } from '../BaseAI';
import { AgentRuntimeErrorType } from '../error';
import {
  ChatCompetitionOptions,
  ChatCompletionErrorPayload,
  ChatStreamPayload,
  ModelProvider,
} from '../types';
import { AgentRuntimeError } from '../utils/createError';
import { debugStream } from '../utils/debugStream';
import { StreamingResponse } from '../utils/response';
import { MinimaxStream } from '../utils/streams';

interface MinimaxBaseResponse {
  base_resp?: {
    status_code?: number;
    status_msg?: string;
  };
}

type MinimaxResponse = Partial<OpenAI.ChatCompletionChunk> & MinimaxBaseResponse;

function throwIfErrorResponse(data: MinimaxResponse) {
  // error status code
  // https://www.minimaxi.com/document/guides/chat-model/pro/api?id=6569c85948bc7b684b30377e#3.1.3%20%E8%BF%94%E5%9B%9E(response)%E5%8F%82%E6%95%B0
  if (!data.base_resp?.status_code || data.base_resp?.status_code < 1000) {
    return;
  }
  if (data.base_resp?.status_code === 1004) {
    throw AgentRuntimeError.chat({
      error: {
        code: data.base_resp.status_code,
        message: data.base_resp.status_msg,
      },
      errorType: AgentRuntimeErrorType.InvalidProviderAPIKey,
      provider: ModelProvider.Minimax,
    });
  }
  throw AgentRuntimeError.chat({
    error: {
      code: data.base_resp.status_code,
      message: data.base_resp.status_msg,
    },
    errorType: AgentRuntimeErrorType.ProviderBizError,
    provider: ModelProvider.Minimax,
  });
}

function parseMinimaxResponse(chunk: string): MinimaxResponse | undefined {
  let body = chunk;
  if (body.startsWith('data:')) {
    body = body.slice(5).trim();
  }
  if (isEmpty(body)) {
    return;
  }
  return JSON.parse(body) as MinimaxResponse;
}

export class LobeMinimaxAI implements LobeRuntimeAI {
  apiKey: string;

  constructor({ apiKey }: { apiKey?: string } = {}) {
    if (!apiKey) throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);

    this.apiKey = apiKey;
  }

  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions): Promise<Response> {
    try {
      const response = await fetch('https://api.minimax.chat/v1/text/chatcompletion_v2', {
        body: JSON.stringify(this.buildCompletionsParams(payload)),
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        method: 'POST',
      });
      if (!response.body || !response.ok) {
        throw AgentRuntimeError.chat({
          error: {
            status: response.status,
            statusText: response.statusText,
          },
          errorType: AgentRuntimeErrorType.ProviderBizError,
          provider: ModelProvider.Minimax,
        });
      }

      const [prod, body2] = response.body.tee();
      const [prod2, debug] = body2.tee();

      if (process.env.DEBUG_MINIMAX_CHAT_COMPLETION === '1') {
        debugStream(debug).catch(console.error);
      }

      // wait for the first response, and throw error if minix returns an error
      await this.parseFirstResponse(prod2.getReader());

      return StreamingResponse(MinimaxStream(prod), { headers: options?.headers });
    } catch (error) {
      console.log('error', error);
      const err = error as Error | ChatCompletionErrorPayload;
      if ('provider' in err) {
        throw error;
      }
      const errorResult = {
        cause: err.cause,
        message: err.message,
        name: err.name,
        stack: err.stack,
      };
      throw AgentRuntimeError.chat({
        error: errorResult,
        errorType: AgentRuntimeErrorType.ProviderBizError,
        provider: ModelProvider.Minimax,
      });
    }
  }

  // the document gives the default value of max tokens, but abab6.5 and abab6.5s
  // will meet length finished error, and output is truncationed
  // so here fill the max tokens number to fix it
  // https://www.minimaxi.com/document/guides/chat-model/V2
  private getMaxTokens(model: string): number | undefined {
    switch (model) {
      case 'abab6.5t-chat':
      case 'abab6.5g-chat':
      case 'abab5.5s-chat':
      case 'abab5.5-chat': {
        return 4096;
      }
      case 'abab6.5s-chat': {
        return 8192;
      }
    }
  }

  private buildCompletionsParams(payload: ChatStreamPayload) {
    const { temperature, top_p, ...params } = payload;

    return {
      ...params,
      frequency_penalty: undefined,
      max_tokens:
        payload.max_tokens !== undefined ? payload.max_tokens : this.getMaxTokens(payload.model),
      presence_penalty: undefined,
      stream: true,
      temperature: temperature === undefined || temperature <= 0 ? undefined : temperature / 2,

      tools: params.tools?.map((tool) => ({
        function: {
          description: tool.function.description,
          name: tool.function.name,
          parameters: JSON.stringify(tool.function.parameters),
        },
        type: 'function',
      })),
      top_p: top_p === 0 ? undefined : top_p,
    };
  }

  private async parseFirstResponse(reader: ReadableStreamDefaultReader<Uint8Array>) {
    const decoder = new TextDecoder();

    const { value } = await reader.read();
    const chunkValue = decoder.decode(value, { stream: true });
    let data;
    try {
      data = parseMinimaxResponse(chunkValue);
    } catch {
      // parse error, skip it
      return;
    }
    if (data) {
      throwIfErrorResponse(data);
    }
  }
}

export default LobeMinimaxAI;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/deepseek/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeDeepSeekAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.deepseek.com/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_DEEPSEEK_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.DeepSeek,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/xai/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeXAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.x.ai/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_XAI_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.XAI,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/zeroone/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeZeroOneAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.lingyiwanwu.com/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_ZEROONE_CHAT_COMPLETION === '1',
  },

  provider: ModelProvider.ZeroOne,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/stepfun/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeStepfunAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.stepfun.com/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      return {
        ...payload,
        stream: !payload.tools,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_STEPFUN_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Stepfun,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/hunyuan/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeHunyuanAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.hunyuan.cloud.tencent.com/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_HUNYUAN_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Hunyuan,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/perplexity/index.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamPayload, ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobePerplexityAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.perplexity.ai',
  chatCompletion: {
    handlePayload: (payload: ChatStreamPayload) => {
      // Set a default frequency penalty value greater than 0
      const { presence_penalty, frequency_penalty, stream = true, temperature, ...res } = payload;

      let param;

      // Ensure we are only have one frequency_penalty or frequency_penalty
      if (presence_penalty !== 0) {
        param = { presence_penalty };
      } else {
        const defaultFrequencyPenalty = 1;

        param = { frequency_penalty: frequency_penalty || defaultFrequencyPenalty };
      }

      return {
        ...res,
        ...param,
        stream,
        temperature: temperature >= 2 ? undefined : temperature,
      } as OpenAI.ChatCompletionCreateParamsStreaming;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_PERPLEXITY_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Perplexity,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/baichuan/index.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamPayload, ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeBaichuanAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.baichuan-ai.com/v1',
  chatCompletion: {
    handlePayload: (payload: ChatStreamPayload) => {
      const { temperature, ...rest } = payload;

      return {
        ...rest,
        // [baichuan] frequency_penalty must be between 1 and 2.
        frequency_penalty: undefined,
        temperature: temperature !== undefined ? temperature / 2 : undefined,
      } as OpenAI.ChatCompletionCreateParamsStreaming;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_BAICHUAN_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Baichuan,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/github/index.ts
================================================================================

import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
import type { ChatModelCard } from '@/types/llm';

import { AgentRuntimeErrorType } from '../error';
import { o1Models, pruneO1Payload } from '../openai';
import { ModelProvider } from '../types';
import {
  CHAT_MODELS_BLOCK_LIST,
  LobeOpenAICompatibleFactory,
} from '../utils/openaiCompatibleFactory';

enum Task {
  'chat-completion',
  'embeddings',
}

/* eslint-disable typescript-sort-keys/interface */
type Model = {
  id: string;
  name: string;
  friendly_name: string;
  model_version: number;
  publisher: string;
  model_family: string;
  model_registry: string;
  license: string;
  task: Task;
  description: string;
  summary: string;
  tags: string[];
};
/* eslint-enable typescript-sort-keys/interface */

export const LobeGithubAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://models.inference.ai.azure.com',
  chatCompletion: {
    handlePayload: (payload) => {
      const { model } = payload;

      if (o1Models.has(model)) {
        return { ...pruneO1Payload(payload), stream: false } as any;
      }

      return { ...payload, stream: payload.stream ?? true };
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_GITHUB_CHAT_COMPLETION === '1',
  },
  errorType: {
    bizError: AgentRuntimeErrorType.ProviderBizError,
    invalidAPIKey: AgentRuntimeErrorType.InvalidGithubToken,
  },
  models: async ({ client }) => {
    const modelsPage = (await client.models.list()) as any;
    const modelList: Model[] = modelsPage.body;
    return modelList
      .filter((model) => {
        return CHAT_MODELS_BLOCK_LIST.every(
          (keyword) => !model.name.toLowerCase().includes(keyword),
        );
      })
      .map((model) => {
        const knownModel = LOBE_DEFAULT_MODEL_LIST.find((m) => m.id === model.name);

        if (knownModel) return knownModel;

        return {
          description: model.description,
          displayName: model.friendly_name,
          id: model.name,
        };
      })
      .filter(Boolean) as ChatModelCard[];
  },
  provider: ModelProvider.Github,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/sensenova/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeSenseNovaAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.sensenova.cn/compatible-mode/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      const { frequency_penalty, temperature, top_p, ...rest } = payload;

      return {
        ...rest,
        frequency_penalty:
          frequency_penalty !== undefined && frequency_penalty > 0 && frequency_penalty <= 2
            ? frequency_penalty
            : undefined,
        stream: true,
        temperature:
          temperature !== undefined && temperature > 0 && temperature <= 2
            ? temperature
            : undefined,
        top_p: top_p !== undefined && top_p > 0 && top_p < 1 ? top_p : undefined,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_SENSENOVA_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.SenseNova,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/ai360/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeAi360AI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.360.cn/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      return {
        ...payload,
        stream: !payload.tools,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_AI360_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Ai360,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/anthropic/index.ts
================================================================================

// sort-imports-ignore
import '@anthropic-ai/sdk/shims/web';
import Anthropic from '@anthropic-ai/sdk';
import { ClientOptions } from 'openai';

import { LobeRuntimeAI } from '../BaseAI';
import { AgentRuntimeErrorType } from '../error';
import { ChatCompetitionOptions, ChatStreamPayload, ModelProvider } from '../types';
import { AgentRuntimeError } from '../utils/createError';
import { debugStream } from '../utils/debugStream';
import { desensitizeUrl } from '../utils/desensitizeUrl';
import { buildAnthropicMessages, buildAnthropicTools } from '../utils/anthropicHelpers';
import { StreamingResponse } from '../utils/response';
import { AnthropicStream } from '../utils/streams';

const DEFAULT_BASE_URL = 'https://api.anthropic.com';

export class LobeAnthropicAI implements LobeRuntimeAI {
  private client: Anthropic;

  baseURL: string;

  constructor({ apiKey, baseURL = DEFAULT_BASE_URL, ...res }: ClientOptions = {}) {
    if (!apiKey) throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);

    this.client = new Anthropic({ apiKey, baseURL, ...res });
    this.baseURL = this.client.baseURL;
  }

  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
    try {
      const anthropicPayload = await this.buildAnthropicPayload(payload);

      const response = await this.client.messages.create(
        { ...anthropicPayload, stream: true },
        {
          signal: options?.signal,
        },
      );

      const [prod, debug] = response.tee();

      if (process.env.DEBUG_ANTHROPIC_CHAT_COMPLETION === '1') {
        debugStream(debug.toReadableStream()).catch(console.error);
      }

      return StreamingResponse(AnthropicStream(prod, options?.callback), {
        headers: options?.headers,
      });
    } catch (error) {
      let desensitizedEndpoint = this.baseURL;

      if (this.baseURL !== DEFAULT_BASE_URL) {
        desensitizedEndpoint = desensitizeUrl(this.baseURL);
      }

      if ('status' in (error as any)) {
        switch ((error as Response).status) {
          case 401: {
            throw AgentRuntimeError.chat({
              endpoint: desensitizedEndpoint,
              error: error as any,
              errorType: AgentRuntimeErrorType.InvalidProviderAPIKey,
              provider: ModelProvider.Anthropic,
            });
          }

          case 403: {
            throw AgentRuntimeError.chat({
              endpoint: desensitizedEndpoint,
              error: error as any,
              errorType: AgentRuntimeErrorType.LocationNotSupportError,
              provider: ModelProvider.Anthropic,
            });
          }
          default: {
            break;
          }
        }
      }
      throw AgentRuntimeError.chat({
        endpoint: desensitizedEndpoint,
        error: error as any,
        errorType: AgentRuntimeErrorType.ProviderBizError,
        provider: ModelProvider.Anthropic,
      });
    }
  }

  private async buildAnthropicPayload(payload: ChatStreamPayload) {
    const { messages, model, max_tokens = 4096, temperature, top_p, tools } = payload;
    const system_message = messages.find((m) => m.role === 'system');
    const user_messages = messages.filter((m) => m.role !== 'system');

    return {
      max_tokens,
      messages: await buildAnthropicMessages(user_messages),
      model,
      system: system_message?.content as string,
      temperature: payload.temperature !== undefined ? temperature / 2 : undefined,
      tools: buildAnthropicTools(tools),
      top_p,
    } satisfies Anthropic.MessageCreateParams;
  }
}

export default LobeAnthropicAI;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/giteeai/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeGiteeAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://ai.gitee.com/v1',
  debug: {
    chatCompletion: () => process.env.DEBUG_GITEE_AI_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.GiteeAI,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/taichu/index.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamPayload, ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeTaichuAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://ai-maas.wair.ac.cn/maas/v1',
  chatCompletion: {
    handlePayload: (payload: ChatStreamPayload) => {
      const { temperature, top_p, ...rest } = payload;

      return {
        ...rest,
        temperature: temperature !== undefined ? Math.max(temperature / 2, 0.01) : undefined,
        top_p: top_p !== undefined ? Math.min(9.9, Math.max(top_p / 2, 0.1)) : undefined,
      } as OpenAI.ChatCompletionCreateParamsStreaming;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_TAICHU_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Taichu,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/zhipu/index.ts
================================================================================

import OpenAI from 'openai';

import { ChatStreamPayload, ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeZhipuAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://open.bigmodel.cn/api/paas/v4',
  chatCompletion: {
    handlePayload: ({ model, temperature, top_p, ...payload }: ChatStreamPayload) =>
      ({
        ...payload,
        model,
        stream: true,
        ...(model === 'glm-4-alltools'
          ? {
              temperature:
                temperature !== undefined
                  ? Math.max(0.01, Math.min(0.99, temperature / 2))
                  : undefined,
              top_p: top_p !== undefined ? Math.max(0.01, Math.min(0.99, top_p)) : undefined,
            }
          : {
              temperature: temperature !== undefined ? temperature / 2 : undefined,
              top_p,
            }),
      }) as OpenAI.ChatCompletionCreateParamsStreaming,
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_ZHIPU_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.ZhiPu,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/huggingface/index.ts
================================================================================

import { HfInference } from '@huggingface/inference';
import urlJoin from 'url-join';

import { AgentRuntimeErrorType } from '../error';
import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
import { convertIterableToStream } from '../utils/streams';

export const LobeHuggingFaceAI = LobeOpenAICompatibleFactory({
  chatCompletion: {
    handleStreamBizErrorType: (error) => {
      // e.g.: Server meta-llama/Meta-Llama-3.1-8B-Instruct does not seem to support chat completion. Error: Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.
      if (error.message?.includes('Model requires a Pro subscription')) {
        return AgentRuntimeErrorType.PermissionDenied;
      }

      // e.g.: Server meta-llama/Meta-Llama-3.1-8B-Instruct does not seem to support chat completion. Error: Authorization header is correct, but the token seems invalid
      if (error.message?.includes('the token seems invalid')) {
        return AgentRuntimeErrorType.InvalidProviderAPIKey;
      }
    },
  },
  customClient: {
    createChatCompletionStream: (client: HfInference, payload, instance) => {
      const { max_tokens = 4096 } = payload;
      const hfRes = client.chatCompletionStream({
        endpointUrl: instance.baseURL ? urlJoin(instance.baseURL, payload.model) : instance.baseURL,
        max_tokens: max_tokens,
        messages: payload.messages,
        model: payload.model,
        stream: true,
        temperature: payload.temperature,
        //  `top_p` must be > 0.0 and < 1.0
        top_p: payload?.top_p
          ? payload?.top_p >= 1
            ? 0.99
            : payload?.top_p <= 0
              ? 0.01
              : payload?.top_p
          : undefined,
      });

      return convertIterableToStream(hfRes);
    },
    createClient: (options) => new HfInference(options.apiKey),
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_HUGGINGFACE_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.HuggingFace,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/ollama/index.ts
================================================================================

import { Ollama, Tool } from 'ollama/browser';
import { ClientOptions } from 'openai';

import { OpenAIChatMessage } from '@/libs/agent-runtime';
import { ChatModelCard } from '@/types/llm';

import { LobeRuntimeAI } from '../BaseAI';
import { AgentRuntimeErrorType } from '../error';
import { ChatCompetitionOptions, ChatStreamPayload, ModelProvider } from '../types';
import { AgentRuntimeError } from '../utils/createError';
import { debugStream } from '../utils/debugStream';
import { StreamingResponse } from '../utils/response';
import { OllamaStream, convertIterableToStream } from '../utils/streams';
import { parseDataUri } from '../utils/uriParser';
import { OllamaMessage } from './type';

export class LobeOllamaAI implements LobeRuntimeAI {
  private client: Ollama;

  baseURL?: string;

  constructor({ baseURL }: ClientOptions = {}) {
    try {
      if (baseURL) new URL(baseURL);
    } catch {
      throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidOllamaArgs);
    }

    this.client = new Ollama(!baseURL ? undefined : { host: baseURL });

    if (baseURL) this.baseURL = baseURL;
  }

  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
    try {
      const abort = () => {
        this.client.abort();
        options?.signal?.removeEventListener('abort', abort);
      };

      options?.signal?.addEventListener('abort', abort);

      const response = await this.client.chat({
        messages: this.buildOllamaMessages(payload.messages),
        model: payload.model,
        options: {
          frequency_penalty: payload.frequency_penalty,
          presence_penalty: payload.presence_penalty,
          temperature: payload.temperature !== undefined ? payload.temperature / 2 : undefined,
          top_p: payload.top_p,
        },
        stream: true,
        tools: payload.tools as Tool[],
      });

      const stream = convertIterableToStream(response);
      const [prod, debug] = stream.tee();

      if (process.env.DEBUG_OLLAMA_CHAT_COMPLETION === '1') {
        debugStream(debug).catch(console.error);
      }

      return StreamingResponse(OllamaStream(prod, options?.callback), {
        headers: options?.headers,
      });
    } catch (error) {
      const e = error as {
        error: any;
        message: string;
        name: string;
        status_code: number;
      };

      throw AgentRuntimeError.chat({
        error: {
          ...e.error,
          message: String(e.error?.message || e.message),
          name: e.name,
          status_code: e.status_code,
        },
        errorType: AgentRuntimeErrorType.OllamaBizError,
        provider: ModelProvider.Ollama,
      });
    }
  }

  async models(): Promise<ChatModelCard[]> {
    const list = await this.client.list();
    return list.models.map((model) => ({
      id: model.name,
    }));
  }

  private buildOllamaMessages(messages: OpenAIChatMessage[]) {
    return messages.map((message) => this.convertContentToOllamaMessage(message));
  }

  private convertContentToOllamaMessage = (message: OpenAIChatMessage): OllamaMessage => {
    if (typeof message.content === 'string') {
      return { content: message.content, role: message.role };
    }

    const ollamaMessage: OllamaMessage = {
      content: '',
      role: message.role,
    };

    for (const content of message.content) {
      switch (content.type) {
        case 'text': {
          // keep latest text input
          ollamaMessage.content = content.text;
          break;
        }
        case 'image_url': {
          const { base64 } = parseDataUri(content.image_url.url);
          if (base64) {
            ollamaMessage.images ??= [];
            ollamaMessage.images.push(base64);
          }
          break;
        }
      }
    }

    return ollamaMessage;
  };
}

export default LobeOllamaAI;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/upstage/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeUpstageAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.upstage.ai/v1/solar',
  debug: {
    chatCompletion: () => process.env.DEBUG_UPSTAGE_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Upstage,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/ai21/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeAi21AI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.ai21.com/studio/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      return {
        ...payload,
        stream: !payload.tools,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_AI21_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Ai21,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/siliconcloud/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeSiliconCloudAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.siliconflow.cn/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      return {
        ...payload,
        stream: !payload.tools,
      } as any;
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_SILICONCLOUD_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.SiliconCloud,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/mistral/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

export const LobeMistralAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.mistral.ai/v1',
  chatCompletion: {
    handlePayload: (payload) => ({
      ...(payload.max_tokens !== undefined && { max_tokens: payload.max_tokens }),
      messages: payload.messages as any,
      model: payload.model,
      stream: true,
      temperature: payload.temperature !== undefined ? payload.temperature / 2 : undefined,
      ...(payload.tools && { tools: payload.tools }),
      top_p: payload.top_p,
    }),
    noUserId: true,
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_MISTRAL_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Mistral,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/spark/index.ts
================================================================================

import { ModelProvider } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams';

export const LobeSparkAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://spark-api-open.xf-yun.com/v1',
  chatCompletion: {
    handleStream: SparkAIStream,
    handleTransformResponseToStream: transformSparkResponseToStream,
    noUserId: true,
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_SPARK_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.Spark,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/libs/agent-runtime/openai/index.ts
================================================================================

import { ChatStreamPayload, ModelProvider, OpenAIChatMessage } from '../types';
import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';

// TODO: 临时写法，后续要重构成 model card 展示配置
export const o1Models = new Set([
  'o1-preview',
  'o1-preview-2024-09-12',
  'o1-mini',
  'o1-mini-2024-09-12',
  'o1',
  'o1-2024-12-17',
]);

export const pruneO1Payload = (payload: ChatStreamPayload) => ({
  ...payload,
  frequency_penalty: 0,
  messages: payload.messages.map((message: OpenAIChatMessage) => ({
    ...message,
    role: message.role === 'system' ? 'user' : message.role,
  })),
  presence_penalty: 0,
  temperature: 1,
  top_p: 1,
});

export const LobeOpenAI = LobeOpenAICompatibleFactory({
  baseURL: 'https://api.openai.com/v1',
  chatCompletion: {
    handlePayload: (payload) => {
      const { model } = payload;

      if (o1Models.has(model)) {
        return pruneO1Payload(payload) as any;
      }

      return { ...payload, stream: payload.stream ?? true };
    },
  },
  debug: {
    chatCompletion: () => process.env.DEBUG_OPENAI_CHAT_COMPLETION === '1',
  },
  provider: ModelProvider.OpenAI,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/guide.ts
================================================================================

import urlJoin from 'url-join';

import {
  BLOG,
  DOCKER_IMAGE,
  EMAIL_BUSINESS,
  EMAIL_SUPPORT,
  GITHUB,
  OFFICIAL_PREVIEW_URL,
  OFFICIAL_SITE,
  OFFICIAL_URL,
  SELF_HOSTING_DOCUMENTS,
  USAGE_DOCUMENTS,
  WIKI,
} from '@/const/url';

export const INBOX_GUIDE_SYSTEMROLE = `# Role: LobeChat Support Assistant

## About [LobeHub](${OFFICIAL_SITE})

LobeHub is an organization of design-engineers dedicated to providing advanced design components and tools for AI-generated content (AIGC).
It aims to create a technology-driven community platform that enables the sharing of knowledge and ideas, fostering inspiration and collaboration.

Adopting a Bootstrapping approach, LobeHub is committed to delivering an open, transparent, and user-friendly product ecosystem for both casual users and professional developers.
LobeHub serves as an AI Agent playground, where creativity and innovation meet.

## About [LobeChat](${OFFICIAL_URL})

LobeChat, a product of LobeHub, is an open-source ChatGPT/LLMs UI/Framework designed for modern LLMs/AI applications.
Supports Multi AI Providers( OpenAI / Claude 3 / Gemini / Perplexity / Bedrock / Azure / Mistral / Ollama ), Multi-Modals (Vision/TTS) and plugin system.
and offers a one-click FREE deployment for a private ChatGPT chat application, making it accessible and customizable for a wide range of users.

### Features

- [Multi-Model Service Provider Support](${urlJoin(USAGE_DOCUMENTS, '/features/multi-ai-providers')})
- [Local Large Language Model (LLM) Support](${urlJoin(USAGE_DOCUMENTS, '/features/local-llm')})
- [Model Visual Recognition](${urlJoin(USAGE_DOCUMENTS, '/features/vision')})
- [TTS & STT Voice Conversation](${urlJoin(USAGE_DOCUMENTS, '/features/tts')})
- [Text to Image Generation](${urlJoin(USAGE_DOCUMENTS, '/features/text-to-image')})
- [Plugin System (Function Calling)](${urlJoin(USAGE_DOCUMENTS, '/features/plugin-system')})
- [Agent Market (GPTs)](${urlJoin(USAGE_DOCUMENTS, '/features/agent-market')})

### Community Edition and Cloud Version

LobeChat is currently available as a community preview version, completely open-source and free of charge.

In the LobeChat Cloud version, we provide 500,000 free computing credits to all registered users. It is ready to use without complex configurations.
If you require more usage, you can subscribe to the Basic, Advanced, or Professional versions for a fee.

### Self Hosting

LobeChat provides Self-Hosted Version with [Vercel](${urlJoin(SELF_HOSTING_DOCUMENTS, '/platform/vercel')}) and [Docker Image](${DOCKER_IMAGE}).
This allows you to deploy your own chatbot within a few minutes without any prior knowledge.

**IMPORTANT**

When users ask about usage or deployment, DO NOT MAKE UP ANSWERS. Instead, guide them to the relevant documentation!!!

Learn more about [Build your own LobeChat](${SELF_HOSTING_DOCUMENTS}) by checking it out.

## Resources Links

In the response, please try to pick and include the relevant links below, and if a relevant answer cannot be provided, also offer the user these related links:

- Official Website: ${OFFICIAL_SITE}
- Cloud Version: ${OFFICIAL_URL}
- Community Edition: ${OFFICIAL_PREVIEW_URL}
- GitHub Repository: ${GITHUB}
- Latest News: ${BLOG}
- Usage Documentation: ${USAGE_DOCUMENTS}
- Self-Hosting Documentation: ${SELF_HOSTING_DOCUMENTS}
- Development Guide: ${WIKI}
- Email Support: ${EMAIL_SUPPORT}
- Business Inquiries: ${EMAIL_BUSINESS}

## Workflow

1. Greet users and introduce the role and purpose of LobeHub LobeChat Support Assistant.
2. Understand and address user inquiries related to the LobeHub ecosystem and LobeChat application.
3. If unable to resolve user queries, pick and guide them to appropriate resources listed above.

## Initialization

As the role <Role>, I will adhere to the following guidelines:
- Provide accurate and helpful information to users.
- Maintain a friendly and professional demeanor.
- Direct users to the appropriate resources when necessary.
- Keep the language of the response consistent with the language of the user input; if they are not consistent, then translate.

Welcome users to LobeChat, introduce myself as the <Role>, and inform them about the services and support available. Then, guide users through the <Workflow> for assistance.`;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/fetch.ts
================================================================================

export const OPENAI_END_POINT = 'X-openai-end-point';
export const OPENAI_API_KEY_HEADER_KEY = 'X-openai-api-key';
export const LOBE_USER_ID = 'X-lobe-user-id';

export const USE_AZURE_OPENAI = 'X-use-azure-openai';

export const AZURE_OPENAI_API_VERSION = 'X-azure-openai-api-version';

export const LOBE_CHAT_ACCESS_CODE = 'X-lobe-chat-access-code';

export const OAUTH_AUTHORIZED = 'X-oauth-authorized';

/**
 * @deprecated
 */
export const getOpenAIAuthFromRequest = (req: Request) => {
  const apiKey = req.headers.get(OPENAI_API_KEY_HEADER_KEY);
  const endpoint = req.headers.get(OPENAI_END_POINT);
  const accessCode = req.headers.get(LOBE_CHAT_ACCESS_CODE);
  const useAzureStr = req.headers.get(USE_AZURE_OPENAI);
  const apiVersion = req.headers.get(AZURE_OPENAI_API_VERSION);
  const oauthAuthorizedStr = req.headers.get(OAUTH_AUTHORIZED);
  const userId = req.headers.get(LOBE_USER_ID);

  const oauthAuthorized = !!oauthAuthorizedStr;
  const useAzure = !!useAzureStr;

  return { accessCode, apiKey, apiVersion, endpoint, oauthAuthorized, useAzure, userId };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/settings/systemAgent.ts
================================================================================

import {
  QueryRewriteSystemAgent,
  SystemAgentItem,
  UserSystemAgentConfig,
} from '@/types/user/settings';

import { DEFAULT_SMALL_MODEL, DEFAULT_SMALL_PROVIDER } from './llm';

export const DEFAULT_REWRITE_QUERY =
  'Given the following conversation and a follow-up question, rephrase the follow up question to be a standalone question, in its original language. Keep as much details as possible from previous messages. Keep entity names and all.';

export const DEFAULT_SYSTEM_AGENT_ITEM: SystemAgentItem = {
  model: DEFAULT_SMALL_MODEL,
  provider: DEFAULT_SMALL_PROVIDER,
};

export const DEFAULT_QUERY_REWRITE_SYSTEM_AGENT_ITEM: QueryRewriteSystemAgent = {
  enabled: true,
  model: DEFAULT_SMALL_MODEL,
  provider: DEFAULT_SMALL_PROVIDER,
};

export const DEFAULT_SYSTEM_AGENT_CONFIG: UserSystemAgentConfig = {
  agentMeta: DEFAULT_SYSTEM_AGENT_ITEM,
  historyCompress: DEFAULT_SYSTEM_AGENT_ITEM,
  queryRewrite: DEFAULT_QUERY_REWRITE_SYSTEM_AGENT_ITEM,
  thread: DEFAULT_SYSTEM_AGENT_ITEM,
  topic: DEFAULT_SYSTEM_AGENT_ITEM,
  translation: DEFAULT_SYSTEM_AGENT_ITEM,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/settings/agent.ts
================================================================================

import { DEFAULT_AGENT_META } from '@/const/meta';
import { DEFAULT_CHAT_MODEL, DEFAULT_CHAT_PROVIDER } from '@/const/settings/llm';
import { LobeAgentChatConfig, LobeAgentConfig, LobeAgentTTSConfig } from '@/types/agent';
import { UserDefaultAgent } from '@/types/user/settings';

export const DEFAUTT_AGENT_TTS_CONFIG: LobeAgentTTSConfig = {
  showAllLocaleVoice: false,
  sttLocale: 'auto',
  ttsService: 'openai',
  voice: {
    openai: 'nova',
  },
};

export const DEFAULT_AGENT_CHAT_CONFIG: LobeAgentChatConfig = {
  autoCreateTopicThreshold: 2,
  displayMode: 'chat',
  enableAutoCreateTopic: true,
  enableCompressHistory: true,
  enableHistoryCount: true,
  historyCount: 20,
};

export const DEFAULT_AGENT_CONFIG: LobeAgentConfig = {
  chatConfig: DEFAULT_AGENT_CHAT_CONFIG,
  model: DEFAULT_CHAT_MODEL,
  params: {
    frequency_penalty: 0,
    presence_penalty: 0,
    temperature: 1,
    top_p: 1,
  },
  plugins: [],
  provider: DEFAULT_CHAT_PROVIDER,
  systemRole: '',
  tts: DEFAUTT_AGENT_TTS_CONFIG,
};

export const DEFAULT_AGENT: UserDefaultAgent = {
  config: DEFAULT_AGENT_CONFIG,
  meta: DEFAULT_AGENT_META,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/settings/llm.ts
================================================================================

import { ModelProvider } from '@/libs/agent-runtime';
import { genUserLLMConfig } from '@/utils/genUserLLMConfig';

export const DEFAULT_LLM_CONFIG = genUserLLMConfig({
  ollama: {
    enabled: true,
    fetchOnClient: true,
  },
  openai: {
    enabled: true,
  },
});

export const DEFAULT_CHAT_MODEL = 'claude-3-5-sonnet-20241022';
export const DEFAULT_CHAT_PROVIDER = ModelProvider.Anthropic;

export const DEFAULT_EMBEDDING_MODEL = 'text-embedding-3-large';
export const DEFAULT_EMBEDDING_PROVIDER = ModelProvider.OpenAI;

export const DEFAULT_SMALL_MODEL = 'claude-3-5-haiku-20241022';
export const DEFAULT_SMALL_PROVIDER = ModelProvider.Anthropic;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/settings/tts.ts
================================================================================

import { UserTTSConfig } from '@/types/user/settings';

export const DEFAULT_TTS_CONFIG: UserTTSConfig = {
  openAI: {
    sttModel: 'whisper-1',
    ttsModel: 'tts-1-hd',
  },
  sttAutoStop: true,
  sttServer: 'openai',
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/const/settings/index.ts
================================================================================

import { UserSettings } from '@/types/user/settings';

import { DEFAULT_AGENT } from './agent';
import { DEFAULT_COMMON_SETTINGS } from './common';
import { DEFAULT_LLM_CONFIG } from './llm';
import { DEFAULT_SYNC_CONFIG } from './sync';
import { DEFAULT_SYSTEM_AGENT_CONFIG } from './systemAgent';
import { DEFAULT_TOOL_CONFIG } from './tool';
import { DEFAULT_TTS_CONFIG } from './tts';

export const COOKIE_CACHE_DAYS = 30;

export * from './agent';
export * from './llm';
export * from './systemAgent';
export * from './tool';
export * from './tts';

export const DEFAULT_SETTINGS: UserSettings = {
  defaultAgent: DEFAULT_AGENT,
  general: DEFAULT_COMMON_SETTINGS,
  keyVaults: {},
  languageModel: DEFAULT_LLM_CONFIG,
  sync: DEFAULT_SYNC_CONFIG,
  systemAgent: DEFAULT_SYSTEM_AGENT_CONFIG,
  tool: DEFAULT_TOOL_CONFIG,
  tts: DEFAULT_TTS_CONFIG,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/Loading/SkeletonLoading/index.tsx
================================================================================

'use client';

import { Skeleton, SkeletonProps } from 'antd';
import { createStyles } from 'antd-style';
import { memo } from 'react';

const useStyles = createStyles(
  ({ css, responsive }) => css`
    ${responsive.mobile} {
      padding: 16px;
    }
  `,
);

const SkeletonLoading = memo<SkeletonProps>(({ className, ...rest }) => {
  const { cx, styles } = useStyles();

  return <Skeleton active className={cx(styles, className)} paragraph={{ rows: 8 }} {...rest} />;
});

export default SkeletonLoading;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/ModelSelect/index.tsx
================================================================================

import { IconAvatarProps, ModelIcon, ProviderIcon } from '@lobehub/icons';
import { Icon, Tooltip } from '@lobehub/ui';
import { Typography } from 'antd';
import { createStyles } from 'antd-style';
import { Infinity, LucideEye, LucidePaperclip, ToyBrick } from 'lucide-react';
import numeral from 'numeral';
import { rgba } from 'polished';
import { FC, memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import { ChatModelCard } from '@/types/llm';
import { formatTokenNumber } from '@/utils/format';

const useStyles = createStyles(({ css, token }) => ({
  custom: css`
    width: 36px;
    height: 20px;

    font-family: ${token.fontFamilyCode};
    font-size: 12px;
    color: ${rgba(token.colorWarning, 0.75)};

    background: ${token.colorWarningBg};
    border-radius: 4px;
  `,
  tag: css`
    cursor: default;

    display: flex;
    align-items: center;
    justify-content: center;

    width: 20px;
    height: 20px;

    border-radius: 4px;
  `,
  tagBlue: css`
    color: ${token.geekblue};
    background: ${token.geekblue1};
  `,
  tagGreen: css`
    color: ${token.green};
    background: ${token.green1};
  `,
  token: css`
    width: 36px;
    height: 20px;

    font-family: ${token.fontFamilyCode};
    font-size: 11px;
    color: ${token.colorTextSecondary};

    background: ${token.colorFillTertiary};
    border-radius: 4px;
  `,
}));

interface ModelInfoTagsProps extends ChatModelCard {
  directionReverse?: boolean;
  placement?: 'top' | 'right';
}

export const ModelInfoTags = memo<ModelInfoTagsProps>(
  ({ directionReverse, placement = 'right', ...model }) => {
    const { t } = useTranslation('components');
    const { styles, cx } = useStyles();

    return (
      <Flexbox direction={directionReverse ? 'horizontal-reverse' : 'horizontal'} gap={4}>
        {model.files && (
          <Tooltip
            overlayStyle={{ pointerEvents: 'none' }}
            placement={placement}
            title={t('ModelSelect.featureTag.file')}
          >
            <div className={cx(styles.tag, styles.tagGreen)} style={{ cursor: 'pointer' }} title="">
              <Icon icon={LucidePaperclip} />
            </div>
          </Tooltip>
        )}
        {model.vision && (
          <Tooltip
            overlayStyle={{ pointerEvents: 'none' }}
            placement={placement}
            title={t('ModelSelect.featureTag.vision')}
          >
            <div className={cx(styles.tag, styles.tagGreen)} style={{ cursor: 'pointer' }} title="">
              <Icon icon={LucideEye} />
            </div>
          </Tooltip>
        )}
        {model.functionCall && (
          <Tooltip
            overlayStyle={{ maxWidth: 'unset', pointerEvents: 'none' }}
            placement={placement}
            title={t('ModelSelect.featureTag.functionCall')}
          >
            <div className={cx(styles.tag, styles.tagBlue)} style={{ cursor: 'pointer' }} title="">
              <Icon icon={ToyBrick} />
            </div>
          </Tooltip>
        )}
        {model.contextWindowTokens !== undefined && (
          <Tooltip
            overlayStyle={{ maxWidth: 'unset', pointerEvents: 'none' }}
            placement={placement}
            title={t('ModelSelect.featureTag.tokens', {
              tokens:
                model.contextWindowTokens === 0
                  ? '∞'
                  : numeral(model.contextWindowTokens).format('0,0'),
            })}
          >
            <Center className={styles.token} title="">
              {model.contextWindowTokens === 0 ? (
                <Infinity size={17} strokeWidth={1.6} />
              ) : (
                formatTokenNumber(model.contextWindowTokens)
              )}
            </Center>
          </Tooltip>
        )}
      </Flexbox>
    );
  },
);

interface ModelItemRenderProps extends ChatModelCard {
  showInfoTag?: boolean;
}

export const ModelItemRender = memo<ModelItemRenderProps>(({ showInfoTag = true, ...model }) => {
  return (
    <Flexbox align={'center'} gap={32} horizontal justify={'space-between'}>
      <Flexbox align={'center'} gap={8} horizontal>
        <ModelIcon model={model.id} size={20} />
        <Typography.Paragraph ellipsis={false} style={{ marginBottom: 0 }}>
          {model.displayName || model.id}
        </Typography.Paragraph>
      </Flexbox>

      {showInfoTag && <ModelInfoTags {...model} />}
    </Flexbox>
  );
});

interface ProviderItemRenderProps {
  name: string;
  provider: string;
}

export const ProviderItemRender = memo<ProviderItemRenderProps>(({ provider, name }) => (
  <Flexbox align={'center'} gap={4} horizontal>
    <ProviderIcon provider={provider} size={20} type={'mono'} />
    {name}
  </Flexbox>
));

interface LabelRendererProps {
  Icon: FC<IconAvatarProps>;
  label: string;
}

export const LabelRenderer = memo<LabelRendererProps>(({ Icon, label }) => (
  <Flexbox align={'center'} gap={8} horizontal>
    <Icon size={20} />
    <span>{label}</span>
  </Flexbox>
));


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/DragUpload/index.tsx
================================================================================

/* eslint-disable no-undef */
import { Icon } from '@lobehub/ui';
import { createStyles } from 'antd-style';
import { FileImage, FileText, FileUpIcon } from 'lucide-react';
import { darken, lighten } from 'polished';
import { memo } from 'react';
import { createPortal } from 'react-dom';
import { useTranslation } from 'react-i18next';
import { Center, Flexbox } from 'react-layout-kit';

import { getContainer, useDragUpload } from './useDragUpload';

const BLOCK_SIZE = 64;
const ICON_SIZE = 36;

const useStyles = createStyles(({ css, token }) => {
  return {
    container: css`
      width: 320px;
      height: 200px;
      padding: ${token.borderRadiusLG + 4}px;

      background: ${token.geekblue};
      border-radius: 16px;
    `,
    content: css`
      width: 100%;
      height: 100%;
      padding: 16px;

      border: 1.5px dashed #fff;
      border-radius: ${token.borderRadiusLG}px;
    `,
    desc: css`
      font-size: 14px;
      line-height: 22px;
      color: #fff;
    `,
    icon: css`
      color: ${darken(0.05, token.geekblue)};
      background: ${lighten(0.38, token.geekblue)};
      border-radius: ${token.borderRadiusLG}px;
    `,
    iconGroup: css`
      margin-block-start: -44px;
    `,
    title: css`
      font-size: 20px;
      font-weight: bold;
      color: #fff;
    `,
    wrapper: css`
      position: fixed;
      z-index: 9999;
      inset: 0;

      width: 100%;
      height: 100%;

      background: ${token.colorBgMask};

      transition: all 0.3s ease-in-out;
    `,
  };
});

interface DragUploadProps {
  enabledFiles?: boolean;
  onUploadFiles: (files: File[]) => Promise<void>;
}

const DragUpload = memo<DragUploadProps>(({ enabledFiles = true, onUploadFiles }) => {
  const { styles, theme } = useStyles();
  const { t } = useTranslation('components');

  const isDragging = useDragUpload(onUploadFiles);

  if (!isDragging) return;

  return createPortal(
    <Center className={styles.wrapper}>
      <div className={styles.container}>
        <Center className={styles.content} gap={12}>
          <Flexbox className={styles.iconGroup} horizontal>
            <Center
              className={styles.icon}
              height={BLOCK_SIZE * 1.25}
              style={{
                background: lighten(0.32, theme.geekblue),
                transform: 'rotateZ(-20deg) translateX(10px)',
              }}
              width={BLOCK_SIZE}
            >
              <Icon icon={FileImage} size={{ fontSize: ICON_SIZE, strokeWidth: 1.5 }} />
            </Center>
            <Center
              className={styles.icon}
              height={BLOCK_SIZE * 1.25}
              style={{
                transform: 'translateY(-12px)',
                zIndex: 1,
              }}
              width={BLOCK_SIZE}
            >
              <Icon icon={FileUpIcon} size={{ fontSize: ICON_SIZE, strokeWidth: 1.5 }} />
            </Center>
            <Center
              className={styles.icon}
              height={BLOCK_SIZE * 1.25}
              style={{
                background: lighten(0.32, theme.geekblue),
                transform: 'rotateZ(20deg) translateX(-10px)',
              }}
              width={BLOCK_SIZE}
            >
              <Icon icon={FileText} size={{ fontSize: ICON_SIZE, strokeWidth: 1.5 }} />
            </Center>
          </Flexbox>
          <Flexbox align={'center'} gap={8} style={{ textAlign: 'center' }}>
            <Flexbox className={styles.title}>
              {t(enabledFiles ? 'DragUpload.dragFileTitle' : 'DragUpload.dragTitle')}
            </Flexbox>
            <Flexbox className={styles.desc}>
              {t(enabledFiles ? 'DragUpload.dragFileDesc' : 'DragUpload.dragDesc')}
            </Flexbox>
          </Flexbox>
        </Center>
      </div>
    </Center>,
    getContainer()!,
  );
});

export default DragUpload;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/DragUpload/useDragUpload.tsx
================================================================================

/* eslint-disable no-undef */
import { useEffect, useRef, useState } from 'react';

const DRAGGING_ROOT_ID = 'dragging-root';
export const getContainer = () => document.querySelector(`#${DRAGGING_ROOT_ID}`);

const handleDragOver = (e: DragEvent) => {
  if (!e.dataTransfer?.items || e.dataTransfer.items.length === 0) return;

  const isFile = e.dataTransfer.types.includes('Files');
  if (isFile) {
    e.preventDefault();
  }
};

const processEntry = async (entry: FileSystemEntry): Promise<File[]> => {
  return new Promise((resolve) => {
    if (entry.isFile) {
      (entry as FileSystemFileEntry).file((file) => {
        resolve([file]);
      });
    } else if (entry.isDirectory) {
      const dirReader = (entry as FileSystemDirectoryEntry).createReader();
      dirReader.readEntries(async (entries) => {
        const filesPromises = entries.map((element) => processEntry(element));
        const fileArrays = await Promise.all(filesPromises);
        resolve(fileArrays.flat());
      });
    } else {
      resolve([]);
    }
  });
};

const getFileListFromDataTransferItems = async (items: DataTransferItem[]) => {
  // get filesList
  const filePromises: Promise<File[]>[] = [];
  for (const item of items) {
    if (item.kind === 'file') {
      const entry = item.webkitGetAsEntry();
      if (entry) {
        filePromises.push(processEntry(entry));
      } else {
        const file = item.getAsFile();

        if (file)
          filePromises.push(
            new Promise((resolve) => {
              resolve([file]);
            }),
          );
      }
    }
  }

  const fileArrays = await Promise.all(filePromises);
  return fileArrays.flat();
};

export const useDragUpload = (onUploadFiles: (files: File[]) => Promise<void>) => {
  const [isDragging, setIsDragging] = useState(false);
  // When a file is dragged to a different area, the 'dragleave' event may be triggered,
  // causing isDragging to be mistakenly set to false.
  // to fix this issue, use a counter to ensure the status change only when drag event left the browser window .
  const dragCounter = useRef(0);

  const handleDragEnter = (e: DragEvent) => {
    if (!e.dataTransfer?.items || e.dataTransfer.items.length === 0) return;

    const isFile = e.dataTransfer.types.includes('Files');
    if (isFile) {
      dragCounter.current += 1;
      e.preventDefault();
      setIsDragging(true);
    }
  };

  const handleDragLeave = (e: DragEvent) => {
    if (!e.dataTransfer?.items || e.dataTransfer.items.length === 0) return;

    const isFile = e.dataTransfer.types.includes('Files');
    if (isFile) {
      e.preventDefault();

      // reset counter
      dragCounter.current -= 1;

      if (dragCounter.current === 0) {
        setIsDragging(false);
      }
    }
  };

  const handleDrop = async (e: DragEvent) => {
    if (!e.dataTransfer?.items || e.dataTransfer.items.length === 0) return;

    const isFile = e.dataTransfer.types.includes('Files');
    if (!isFile) return;

    e.preventDefault();

    // reset counter
    dragCounter.current = 0;

    setIsDragging(false);
    const items = Array.from(e.dataTransfer?.items);

    const files = await getFileListFromDataTransferItems(items);

    if (files.length === 0) return;

    // upload files
    onUploadFiles(files);
  };

  const handlePaste = async (event: ClipboardEvent) => {
    // get files from clipboard
    if (!event.clipboardData) return;
    const items = Array.from(event.clipboardData?.items);

    const files = await getFileListFromDataTransferItems(items);
    if (files.length === 0) return;

    onUploadFiles(files);
  };

  useEffect(() => {
    if (getContainer()) return;
    const root = document.createElement('div');
    root.id = DRAGGING_ROOT_ID;
    document.body.append(root);

    return () => {
      root.remove();
    };
  }, []);

  useEffect(() => {
    window.addEventListener('dragenter', handleDragEnter);
    window.addEventListener('dragover', handleDragOver);
    window.addEventListener('dragleave', handleDragLeave);
    window.addEventListener('drop', handleDrop);
    window.addEventListener('paste', handlePaste);

    return () => {
      window.removeEventListener('dragenter', handleDragEnter);
      window.removeEventListener('dragover', handleDragOver);
      window.removeEventListener('dragleave', handleDragLeave);
      window.removeEventListener('drop', handleDrop);
      window.removeEventListener('paste', handlePaste);
    };
  }, [handleDragEnter, handleDragOver, handleDragLeave, handleDrop, handlePaste]);

  return isDragging;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/FileParsingStatus/index.tsx
================================================================================

import { Icon, Tooltip } from '@lobehub/ui';
import { Badge, Button, Tag } from 'antd';
import { createStyles } from 'antd-style';
import { BoltIcon, Loader2Icon, RotateCwIcon } from 'lucide-react';
import { darken, lighten } from 'polished';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { AsyncTaskStatus, FileParsingTask } from '@/types/asyncTask';

import EmbeddingStatus from './EmbeddingStatus';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  errorReason: css`
    padding: 4px;

    font-family: monospace;
    font-size: 12px;

    background: ${isDarkMode ? darken(0.1, token.colorText) : lighten(0.1, token.colorText)};
    border-radius: 4px;
  `,
}));

interface FileParsingStatusProps extends FileParsingTask {
  className?: string;
  hideEmbeddingButton?: boolean;
  onClick?: (status: AsyncTaskStatus) => void;
  onEmbeddingClick?: () => void;
  onErrorClick?: (task: 'chunking' | 'embedding') => void;
  preparingEmbedding?: boolean;
}

const FileParsingStatus = memo<FileParsingStatusProps>(
  ({
    chunkingStatus,
    onEmbeddingClick,
    chunkingError,
    finishEmbedding,
    chunkCount,
    embeddingStatus,
    embeddingError,
    onClick,
    preparingEmbedding,
    onErrorClick,
    className,
    hideEmbeddingButton,
  }) => {
    const { t } = useTranslation(['components', 'common']);
    const { styles, cx } = useStyles();

    switch (chunkingStatus) {
      case AsyncTaskStatus.Processing: {
        return (
          <Tooltip
            overlayStyle={{ pointerEvents: 'none' }}
            title={t('FileParsingStatus.chunks.status.processingTip')}
          >
            <Tag
              bordered={false}
              className={className}
              color={'processing'}
              icon={<Badge status={'processing'} />}
              style={{ display: 'flex', gap: 4 }}
            >
              {t('FileParsingStatus.chunks.status.processing')}
            </Tag>
          </Tooltip>
        );
      }

      case AsyncTaskStatus.Error: {
        return (
          <Tooltip
            overlayStyle={{ maxWidth: 340, pointerEvents: 'none' }}
            title={
              <Flexbox gap={4}>
                {t('FileParsingStatus.chunks.status.errorResult')}
                {chunkingError && (
                  <Flexbox className={styles.errorReason}>
                    [{chunkingError.name}]:{' '}
                    {chunkingError.body && typeof chunkingError.body !== 'string'
                      ? chunkingError.body.detail
                      : chunkingError.body}
                  </Flexbox>
                )}
              </Flexbox>
            }
          >
            <Tag bordered={false} className={className} color={'error'}>
              {t('FileParsingStatus.chunks.status.error')}{' '}
              <Icon
                icon={RotateCwIcon}
                onClick={() => {
                  onErrorClick?.('chunking');
                }}
                style={{ cursor: 'pointer' }}
                title={t('retry', { ns: 'common' })}
              />
            </Tag>
          </Tooltip>
        );
      }

      case AsyncTaskStatus.Success: {
        // if no embedding status, it means that the embedding is not started
        if (!embeddingStatus || preparingEmbedding)
          return (
            <Flexbox horizontal>
              <Tooltip
                overlayStyle={{ pointerEvents: 'none' }}
                title={t('FileParsingStatus.chunks.embeddingStatus.empty')}
              >
                <Tag
                  bordered={false}
                  className={cx('chunk-tag', className)}
                  icon={
                    preparingEmbedding ? <Icon icon={Loader2Icon} spin /> : <Icon icon={BoltIcon} />
                  }
                  onClick={() => {
                    onClick?.(AsyncTaskStatus.Success);
                  }}
                  style={{ cursor: 'pointer' }}
                >
                  {chunkCount}
                  {
                    // if want to hide button
                    hideEmbeddingButton ||
                    // or if preparing the embedding
                    preparingEmbedding ? null : (
                      <Button
                        onClick={(e) => {
                          e.stopPropagation();
                          onEmbeddingClick?.();
                        }}
                        style={{
                          fontSize: 12,
                          height: 'auto',
                          paddingBlock: 0,
                          paddingInline: '8px 0',
                        }}
                        type={'link'}
                      >
                        {t('FileParsingStatus.chunks.embeddings')}
                      </Button>
                    )
                  }
                </Tag>
              </Tooltip>
            </Flexbox>
          );

        return (
          <EmbeddingStatus
            chunkCount={chunkCount}
            className={className}
            embeddingError={embeddingError}
            embeddingStatus={embeddingStatus}
            finishEmbedding={finishEmbedding}
            onClick={onClick}
            onErrorClick={onErrorClick}
          />
        );
      }
    }
  },
);

export default FileParsingStatus;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/components/FileParsingStatus/EmbeddingStatus.tsx
================================================================================

import { Icon, Tooltip } from '@lobehub/ui';
import { Tag } from 'antd';
import { createStyles } from 'antd-style';
import { BoltIcon, RotateCwIcon } from 'lucide-react';
import { darken, lighten } from 'polished';
import { memo } from 'react';
import { useTranslation } from 'react-i18next';
import { Flexbox } from 'react-layout-kit';

import { AsyncTaskStatus, FileParsingTask } from '@/types/asyncTask';

const useStyles = createStyles(({ css, token, isDarkMode }) => ({
  errorReason: css`
    padding: 4px;

    font-family: monospace;
    font-size: 12px;

    background: ${isDarkMode ? darken(0.1, token.colorText) : lighten(0.1, token.colorText)};
    border-radius: 4px;
  `,
}));

interface EmbeddingStatusProps extends FileParsingTask {
  className?: string;
  onClick?: (status: AsyncTaskStatus) => void;
  onErrorClick?: (task: 'chunking' | 'embedding') => void;
}

const EmbeddingStatus = memo<EmbeddingStatusProps>(
  ({ chunkCount, embeddingStatus, embeddingError, onClick, onErrorClick, className }) => {
    const { t } = useTranslation(['components', 'common']);
    const { styles, cx } = useStyles();

    switch (embeddingStatus) {
      case AsyncTaskStatus.Processing: {
        return (
          <Flexbox horizontal>
            <Tooltip
              overlayStyle={{ pointerEvents: 'none' }}
              title={t('FileParsingStatus.chunks.embeddingStatus.processing')}
            >
              <Tag
                bordered={false}
                className={cx('chunk-tag', className)}
                color={'processing'}
                icon={<Icon icon={BoltIcon} spin />}
                style={{ cursor: 'pointer' }}
              >
                {chunkCount}
              </Tag>
            </Tooltip>
          </Flexbox>
        );
      }

      case AsyncTaskStatus.Error: {
        return (
          <Tooltip
            overlayStyle={{ maxWidth: 340, pointerEvents: 'none' }}
            title={
              <Flexbox gap={4}>
                {t('FileParsingStatus.chunks.embeddingStatus.errorResult')}
                {embeddingError && (
                  <Flexbox className={styles.errorReason}>
                    [{embeddingError.name}]:{' '}
                    {embeddingError.body && typeof embeddingError.body !== 'string'
                      ? embeddingError.body.detail
                      : embeddingError.body}
                  </Flexbox>
                )}
              </Flexbox>
            }
          >
            <Tag bordered={false} className={className} color={'error'}>
              {t('FileParsingStatus.chunks.embeddingStatus.error')}{' '}
              <Icon
                icon={RotateCwIcon}
                onClick={() => {
                  onErrorClick?.('embedding');
                }}
                style={{ cursor: 'pointer' }}
                title={t('retry', { ns: 'common' })}
              />
            </Tag>
          </Tooltip>
        );
      }

      case AsyncTaskStatus.Success: {
        return (
          <Flexbox horizontal>
            <Tooltip
              overlayStyle={{ pointerEvents: 'none' }}
              title={t('FileParsingStatus.chunks.embeddingStatus.success')}
            >
              <Tag
                bordered={false}
                className={cx('chunk-tag', className)}
                color={'purple'}
                icon={<Icon icon={BoltIcon} />}
                onClick={() => {
                  onClick?.(AsyncTaskStatus.Success);
                }}
                style={{ cursor: 'pointer' }}
              >
                {chunkCount}
              </Tag>
            </Tooltip>
          </Flexbox>
        );
      }
    }
  },
);

export default EmbeddingStatus;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/hooks/_header.ts
================================================================================

import { LOBE_CHAT_ACCESS_CODE, OPENAI_API_KEY_HEADER_KEY, OPENAI_END_POINT } from '@/const/fetch';
import { useUserStore } from '@/store/user';
import { keyVaultsConfigSelectors } from '@/store/user/selectors';

/**
 * TODO: Need to be removed after tts refactor
 * @deprecated
 */
// eslint-disable-next-line no-undef
export const createHeaderWithOpenAI = (header?: HeadersInit): HeadersInit => {
  const openai = keyVaultsConfigSelectors.openAIConfig(useUserStore.getState());

  const apiKey = openai.apiKey || '';
  const endpoint = openai.baseURL || '';

  // eslint-disable-next-line no-undef
  return {
    ...header,
    [LOBE_CHAT_ACCESS_CODE]: keyVaultsConfigSelectors.password(useUserStore.getState()),
    [OPENAI_API_KEY_HEADER_KEY]: apiKey,
    [OPENAI_END_POINT]: endpoint,
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/hooks/useTTS.ts
================================================================================

import {
  EdgeSpeechOptions,
  MicrosoftSpeechOptions,
  OpenAITTSOptions,
  TTSOptions,
  useEdgeSpeech,
  useMicrosoftSpeech,
  useOpenAITTS,
} from '@lobehub/tts/react';
import isEqual from 'fast-deep-equal';

import { createHeaderWithOpenAI } from '@/services/_header';
import { API_ENDPOINTS } from '@/services/_url';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/slices/chat';
import { useUserStore } from '@/store/user';
import { settingsSelectors, userGeneralSettingsSelectors } from '@/store/user/selectors';
import { TTSServer } from '@/types/agent';

interface TTSConfig extends TTSOptions {
  onUpload?: (currentVoice: string, arraybuffers: ArrayBuffer[]) => void;
  server?: TTSServer;
  voice?: string;
}

export const useTTS = (content: string, config?: TTSConfig) => {
  const ttsSettings = useUserStore(settingsSelectors.currentTTS, isEqual);
  const ttsAgentSettings = useAgentStore(agentSelectors.currentAgentTTS, isEqual);
  const lang = useUserStore(userGeneralSettingsSelectors.currentLanguage);
  const voice = useAgentStore(agentSelectors.currentAgentTTSVoice(lang));
  let useSelectedTTS;
  let options: any = {};
  switch (config?.server || ttsAgentSettings.ttsService) {
    case 'openai': {
      useSelectedTTS = useOpenAITTS;
      options = {
        api: {
          headers: createHeaderWithOpenAI(),
          serviceUrl: API_ENDPOINTS.tts,
        },
        options: {
          model: ttsSettings.openAI.ttsModel,
          voice: config?.voice || voice,
        },
      } as OpenAITTSOptions;
      break;
    }
    case 'edge': {
      useSelectedTTS = useEdgeSpeech;
      options = {
        api: {
          /**
           * @description client fetch
           * serviceUrl: TTS_URL.edge,
           */
        },
        options: {
          voice: config?.voice || voice,
        },
      } as EdgeSpeechOptions;
      break;
    }
    case 'microsoft': {
      useSelectedTTS = useMicrosoftSpeech;
      options = {
        api: {
          serviceUrl: API_ENDPOINTS.microsoft,
        },
        options: {
          voice: config?.voice || voice,
        },
      } as MicrosoftSpeechOptions;
      break;
    }
  }

  return useSelectedTTS(content, {
    ...config,
    ...options,
    onFinish: (arraybuffers) => {
      config?.onUpload?.(options.voice || 'alloy', arraybuffers);
    },
  });
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/hooks/useProviderName.ts
================================================================================

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';

export const useProviderName = (provider: string) => {
  // const { t } = useTranslation('modelProvider');
  const providerCard = DEFAULT_MODEL_PROVIDER_LIST.find((p) => p.id === provider);

  return providerCard?.name || provider;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/prompts/knowledgeBaseQA/knowledge.ts
================================================================================

import { KnowledgeItem } from '@/types/knowledgeBase';

const knowledgePrompt = (item: KnowledgeItem) =>
  `<knowledge id="${item.id}" name="${item.name}" type="${item.type}"${item.fileType ? ` fileType="${item.fileType}" ` : ''}>${item.description || ''}</knowledge>`;

export const knowledgePrompts = (list?: KnowledgeItem[]) => {
  if ((list || []).length === 0) return '';

  const prompt = `<knowledge_bases>
<knowledge_bases_docstring>here are the knowledge base scope we retrieve chunks from:</knowledge_bases_docstring>
${list?.map((item) => knowledgePrompt(item)).join('\n')}
</knowledge_bases>`;

  return prompt.trim();
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/prompts/knowledgeBaseQA/chunk.ts
================================================================================

import { ChatSemanticSearchChunk } from '@/types/chunk';

const chunkPrompt = (item: ChatSemanticSearchChunk) =>
  `<chunk fileId="${item.fileId}" fileName="${item.fileName}" similarity="${item.similarity}" ${item.pageNumber ? ` pageNumber="${item.pageNumber}" ` : ''}>${item.text}</chunk>`;

export const chunkPrompts = (fileList: ChatSemanticSearchChunk[]) => {
  if (fileList.length === 0) return '';

  const prompt = `<retrieved_chunks>
<retrieved_chunks_docstring>here are retrived chunks you can refer to:</retrieved_chunks_docstring>
${fileList.map((item) => chunkPrompt(item)).join('\n')}
</retrieved_chunks>`;

  return prompt.trim();
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/prompts/knowledgeBaseQA/index.ts
================================================================================

import { chunkPrompts } from '@/prompts/knowledgeBaseQA/chunk';
import { knowledgePrompts } from '@/prompts/knowledgeBaseQA/knowledge';
import { userQueryPrompt } from '@/prompts/knowledgeBaseQA/userQuery';
import { ChatSemanticSearchChunk } from '@/types/chunk';
import { KnowledgeItem } from '@/types/knowledgeBase';

export const knowledgeBaseQAPrompts = ({
  chunks,
  knowledge,
  userQuery,
  rewriteQuery,
}: {
  chunks?: ChatSemanticSearchChunk[];
  knowledge?: KnowledgeItem[];
  rewriteQuery?: string;
  userQuery: string;
}) => {
  if ((chunks || [])?.length === 0) return '';

  const domains = (knowledge || []).map((v) => v.name).join('/');

  return `<knowledge_base_qa_info>
You are also a helpful assistant good answering questions related to ${domains}. And you'll be provided with a question and several passages that might be relevant. And currently your task is to provide answer based on the question and passages.
<knowledge_base_anwser_instruction>
- Note that passages might not be relevant to the question, please only use the passages that are relevant.
- if there is no relevant passage, please answer using your knowledge.
- Answer should use the same original language as the question and follow markdown syntax.
</knowledge_base_anwser_instruction>
${knowledgePrompts(knowledge)}
${chunks ? chunkPrompts(chunks) : ''}
${userQueryPrompt(userQuery, rewriteQuery)}
</knowledge_base_qa_info>`;
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/answerWithContext.ts
================================================================================

import { ChatStreamPayload } from '@/types/openai/chat';

export const chainAnswerWithContext = ({
  context,
  knowledge,
  question,
}: {
  context: string[];
  knowledge: string[];
  question: string;
}): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: `You are also a helpful assistant good at answering questions related to ${knowledge.join('/')}. And you'll be provided with a question and several passages that might be relevant. And currently your task is to provide an answer based on the question and passages.

Note that passages might not be relevant to the question, please only use the passages that are relevant. Or if there is no relevant passage, please answer using your knowledge.

Answer should use the same original language as the question and follow markdown syntax.

The provided passages as context:

<Context>
${context.join('\n')}
</Context>

The question to answer is:

${question}
`,
      role: 'user',
    },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/summaryHistory.ts
================================================================================

import { chatHistoryPrompts } from '@/prompts/chatMessages';
import { ChatMessage } from '@/types/message';
import { ChatStreamPayload } from '@/types/openai/chat';

export const chainSummaryHistory = (messages: ChatMessage[]): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: `You're an assistant who's good at extracting key takeaways and important pieces of data from conversations and summarizing them. Please summarize according to the user's needs. The content you need to summarize is located in the <chat_history> </chat_history> group of xml tags. The summary needs to maintain the original language.  If you are working with the user to solve a problem include any information you would need to keep working.`,
      role: 'system',
    },
    {
      content: `${chatHistoryPrompts(messages)}

Please summarize the above conversation and retain key information. The summarized content will be used as context for subsequent prompts, so include anything pertinent. The summary should be long and detailed.`,

      role: 'user',
    },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/pickEmoji.ts
================================================================================

import { ChatStreamPayload } from '@/types/openai/chat';

/**
 * pick emoji for user prompt
 * @param content
 */
export const chainPickEmoji = (content: string): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content:
        'You are a designer and Emoji expert skilled in concept abstraction. You need to abstract a physical entity concept Emoji as a role avatar based on the role capability description. The format requirements are as follows:\nInput: {text as JSON quoted string}\nOutput: {one Emoji}.',
      role: 'system',
    },
    {
      content: `Input: {You are a copywriting master who helps name design/art works. The names need to have literary connotations, focus on refinement and artistic conception, express the scene atmosphere of the works, making the names both concise and poetic.}`,
      role: 'user',
    },
    { content: '✒️', role: 'assistant' },
    {
      content: `Input: {You are a code wizard. Please convert the following code to TypeScript without modifying the implementation. If there are global variables not defined in the original JavaScript, you need to add declare type declarations.}`,
      role: 'user',
    },
    { content: '🧙‍♂️', role: 'assistant' },
    {
      content: `Input: {You are a startup plan writing expert who can provide plan generation including creative names, short slogans, target user profiles, user pain points, main value propositions, sales/marketing channels, revenue streams, and cost structures.}`,
      role: 'user',
    },
    { content: '🚀', role: 'assistant' },
    { content: `Input: {${content}}`, role: 'user' },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/langDetect.ts
================================================================================

import { ChatStreamPayload } from '@/types/openai/chat';

export const chainLangDetect = (content: string): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content:
        'You are a language expert proficient in languages worldwide. You need to identify the language of user input and output it as a standard locale code.',
      role: 'system',
    },
    {
      content: '{你好}',
      role: 'user',
    },
    {
      content: 'zh-CN',
      role: 'assistant',
    },
    {
      content: '{hello}',
      role: 'user',
    },
    {
      content: 'en-US',
      role: 'assistant',
    },
    {
      content: `{${content}}`,
      role: 'user',
    },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/summaryTitle.ts
================================================================================

import { globalHelpers } from '@/store/user/helpers';
import { ChatStreamPayload, OpenAIChatMessage } from '@/types/openai/chat';

export const chainSummaryTitle = (messages: OpenAIChatMessage[]): Partial<ChatStreamPayload> => {
  const lang = globalHelpers.getCurrentLanguage();

  return {
    messages: [
      {
        content: 'You are an assistant skilled in conversation who needs to summarize user conversations into titles within 10 characters.',
        role: 'system',
      },
      {
        content: `${messages.map((message) => `${message.role}: ${message.content}`).join('\n')}

Please summarize the above conversation into a title within 10 characters, without punctuation marks, output language: ${lang}`,
        role: 'user',
      },
    ],
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/abstractChunk.ts
================================================================================

import { DEFAULT_SMALL_MODEL } from '@/const/settings';
import { ChatStreamPayload } from '@/types/openai/chat';

export const chainAbstractChunkText = (text: string): Partial<ChatStreamPayload> => {
  return {
    messages: [
      {
        content:
          'You are an assistant skilled at extracting summaries from chunks. You need to summarize the user\'s conversation into 1-2 sentences as an abstract, and output it in the same language used in the chunk',
        role: 'system',
      },
      {
        content: `chunk: ${text}`,
        role: 'user',
      },
    ],
    model: DEFAULT_SMALL_MODEL,
  };
};



================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/summaryDescription.ts
================================================================================

import { globalHelpers } from '@/store/user/helpers';
import { ChatStreamPayload } from '@/types/openai/chat';

export const chainSummaryDescription = (content: string): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: `You are an assistant skilled in skill summarization. You need to summarize the user's input into a role skill profile, not exceeding 20 characters. The content needs to ensure clear information, clear logic, and effectively convey the role's skills and experience, and needs to be translated into the target language:${globalHelpers.getCurrentLanguage()}. Format requirements are as follows:\nInput: {text as JSON quoted string} [locale]\nOutput: {profile}.`,
      role: 'system',
    },
    {
      content: `Input: {You are a copywriting master who helps name design/art works. The names need to have literary connotations, focus on refinement and artistic conception, express the scene atmosphere of the works, making the names both concise and poetic.} [zh-CN]`,
      role: 'user',
    },
    { content: '擅长文创艺术作品起名', role: 'assistant' },
    {
      content: `Input: {You are a startup plan writing expert who can provide plan generation including creative names, short slogans, target user profiles, user pain points, main value propositions, sales/marketing channels, revenue streams, and cost structures.} [en-US]`,
      role: 'user',
    },
    { content: 'Good at business plan writing and consulting', role: 'assistant' },
    {
      content: `Input: {You are a frontend expert. Please convert the code below to TS without modifying the implementation. If there are global variables not defined in the original JS, you need to add type declarations using declare.} [zh-CN]`,
      role: 'user',
    },
    { content: '擅长 ts 转换和补充类型声明', role: 'assistant' },
    {
      content: `Input: {
Users write API user documentation for developers normally. You need to provide more user-friendly and readable documentation content from the user's perspective.\n\nA standard API documentation example is as follows:\n\n\`\`\`markdown
---
title: useWatchPluginMessage
description: Monitor and receive plugin messages from LobeChat
nav: API
---\n\n\`useWatchPluginMessage\` is a React Hook encapsulated by Chat Plugin SDK for monitoring plugin messages from LobeChat.
} [ru-RU]`,
      role: 'user',
    },
    {
      content:
        'Специализируется на создании хорошо структурированной и профессиональной документации README для GitHub с точными техническими терминами',
      role: 'assistant',
    },
    {
      content: `Input: {You are a startup plan writing expert who can provide plan generation including creative names, short slogans, target user profiles, user pain points, main value propositions, sales/marketing channels, revenue streams, and cost structures.} [zh-CN]`,
      role: 'user',
    },
    { content: '擅长创业计划撰写与咨询', role: 'assistant' },
    { content: `Input: {${content}} [${globalHelpers.getCurrentLanguage()}]`, role: 'user' },
  ],
  temperature: 0,
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/summaryAgentName.ts
================================================================================

import { globalHelpers } from '@/store/user/helpers';
import { ChatStreamPayload } from '@/types/openai/chat';

/**
 * summary agent name for user prompt
 * @param content
 */
export const chainSummaryAgentName = (content: string): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: `You are a naming expert skilled in creating names. Names should have literary connotations, focusing on refinement and artistic conception. You need to summarize the user's description into a role name within 10 characters and translate it to the target language. Format requirements are as follows:\nInput: {text as JSON quoted string} [locale]\nOutput: {role name}.`,
      role: 'system',
    },
    {
      content: `Input: {You are a copywriting master who helps name design/art works. The names need to have literary connotations, focus on refinement and artistic conception, express the scene atmosphere of the works, making the names both concise and poetic.} [zh-CN]`,
      role: 'user',
    },
    {
      content: `Input: {You are a UX Writer skilled at converting ordinary descriptions into exquisite expressions. For the following text input from users, you need to transform it into a better expression not exceeding 40 characters.} [ru-RU]`,
      role: 'user',
    },
    { content: 'Творческий редактор UX', role: 'assistant' },
    {
      content: `Input: {You are a frontend code expert. Please convert the following code to TypeScript without modifying the implementation. If there are global variables not defined in the original JavaScript, you need to add declare type declarations.} [en-US]`,
      role: 'user',
    },
    { content: 'TS Transformer', role: 'assistant' },
    {
      content: `Input: {Improve my English language use by replacing basic A0-level expressions with more sophisticated, advanced-level phrases while maintaining the conversation's essence. Your responses should focus solely on corrections and enhancements, avoiding additional explanations.} [zh-CN]`,
      role: 'user',
    },
    { content: '邮件优化助理', role: 'assistant' },
    { content: `Input: {${content}} [${globalHelpers.getCurrentLanguage()}]`, role: 'user' },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/summaryTags.ts
================================================================================

import { globalHelpers } from '@/store/user/helpers';
import { ChatStreamPayload } from '@/types/openai/chat';

export const chainSummaryTags = (content: string): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content:
        'You are an assistant skilled in conversation tag summarization. You need to extract classification tags from user input, separated by `,`, no more than 5 tags, and translate them into the target language. Format requirements are as follows:\nInput: {text as JSON quoted string} [locale]\nOutput: {tags}.',
      role: 'system',
    },
    {
      content: `Input: {You are a copywriting master who helps name design/art works. The names need to have literary connotations, focus on refinement and artistic conception, express the scene atmosphere of the works, making the names both concise and poetic.} [zh-CN]`,
      role: 'user',
    },
    { content: '起名,写作,创意', role: 'assistant' },
    {
      content: `Input: {You are a professional translator proficient in Simplified Chinese, and have participated in the translation work of the Chinese versions of The New York Times and The Economist. Therefore, you have a deep understanding of translating news and current affairs articles. I hope you can help me translate the following English news paragraphs into Chinese, with a style similar to the Chinese versions of the aforementioned magazines.} [zh-CN]`,
      role: 'user',
    },
    { content: '翻译,写作,文案', role: 'assistant' },
    {
      content: `Input: {You are a startup plan writing expert who can provide plan generation including creative names, short slogans, target user profiles, user pain points, main value propositions, sales/marketing channels, revenue streams, and cost structures.} [en-US]`,
      role: 'user',
    },
    { content: 'entrepreneurship,planning,consulting', role: 'assistant' },
    { content: `Input: {${content}} [${globalHelpers.getCurrentLanguage()}]`, role: 'user' },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/translate.ts
================================================================================

import { ChatStreamPayload } from '@/types/openai/chat';

export const chainTranslate = (
  content: string,
  targetLang: string,
): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: 'You are an assistant skilled in translation who needs to translate the input language into the target language.',
      role: 'system',
    },
    {
      content: `Please translate the following content ${content} into ${targetLang}`,
      role: 'user',
    },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/chains/rewriteQuery.ts
================================================================================

import { DEFAULT_REWRITE_QUERY } from '@/const/settings';
import { ChatStreamPayload } from '@/types/openai/chat';

export const chainRewriteQuery = (
  query: string,
  context: string[],
  instruction: string = DEFAULT_REWRITE_QUERY,
): Partial<ChatStreamPayload> => ({
  messages: [
    {
      content: `${instruction}
<chatHistory>
${context.join('\n')}
</chatHistory>
`,
      role: 'system',
    },
    {
      content: `Follow Up Input: ${query}, it's a standalone query:`,
      role: 'user',
    },
  ],
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/tool.ts
================================================================================

import { globalHelpers } from '@/store/user/helpers';
import { DiscoverPlugintem } from '@/types/discover';
import { convertOpenAIManifestToLobeManifest, getToolManifest } from '@/utils/toolManifest';

import { API_ENDPOINTS } from './_url';

class ToolService {
  getToolList = async (): Promise<DiscoverPlugintem[]> => {
    const locale = globalHelpers.getCurrentLanguage();

    const res = await fetch(`${API_ENDPOINTS.pluginStore}?locale=${locale}`);

    const json = await res.json();

    return json.plugins;
  };

  getToolManifest = getToolManifest;
  convertOpenAIManifestToLobeManifest = convertOpenAIManifestToLobeManifest;
}

export const toolService = new ToolService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/_header.ts
================================================================================

import {
  LOBE_CHAT_ACCESS_CODE,
  LOBE_USER_ID,
  OPENAI_API_KEY_HEADER_KEY,
  OPENAI_END_POINT,
} from '@/const/fetch';
import { useUserStore } from '@/store/user';
import { keyVaultsConfigSelectors } from '@/store/user/selectors';

/**
 * TODO: Need to be removed after tts refactor
 * @deprecated
 */
// eslint-disable-next-line no-undef
export const createHeaderWithOpenAI = (header?: HeadersInit): HeadersInit => {
  const state = useUserStore.getState();
  const openAIConfig = keyVaultsConfigSelectors.openAIConfig(state);

  // eslint-disable-next-line no-undef
  return {
    ...header,
    [LOBE_CHAT_ACCESS_CODE]: keyVaultsConfigSelectors.password(state),
    [LOBE_USER_ID]: state.user?.id || '',
    [OPENAI_API_KEY_HEADER_KEY]: openAIConfig.apiKey || '',
    [OPENAI_END_POINT]: openAIConfig.baseURL || '',
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/rag.ts
================================================================================

import { lambdaClient } from '@/libs/trpc/client';
import { SemanticSearchSchemaType } from '@/types/rag';

class RAGService {
  createParseFileTask = async (id: string, skipExist?: boolean) => {
    return lambdaClient.chunk.createParseFileTask.mutate({ id, skipExist });
  };

  retryParseFile = async (id: string) => {
    return lambdaClient.chunk.retryParseFileTask.mutate({ id });
  };

  createEmbeddingChunksTask = async (id: string) => {
    return lambdaClient.chunk.createEmbeddingChunksTask.mutate({ id });
  };

  semanticSearch = async (query: string, fileIds?: string[]) => {
    return lambdaClient.chunk.semanticSearch.mutate({ fileIds, query });
  };

  semanticSearchForChat = async (params: SemanticSearchSchemaType) => {
    return lambdaClient.chunk.semanticSearchForChat.mutate(params);
  };

  deleteMessageRagQuery = async (id: string) => {
    return lambdaClient.message.removeMessageQuery.mutate({ id });
  };
}

export const ragService = new RAGService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/textToImage.ts
================================================================================

import { ModelProvider } from '@/libs/agent-runtime';
import { createHeaderWithAuth } from '@/services/_auth';
import { OpenAIImagePayload } from '@/types/openai/image';

import { API_ENDPOINTS } from './_url';

interface FetchOptions {
  signal?: AbortSignal | undefined;
}

class ImageGenerationService {
  generateImage = async (
    params: Omit<OpenAIImagePayload, 'model' | 'n'>,
    options?: FetchOptions,
  ) => {
    const payload: OpenAIImagePayload = { ...params, model: 'dall-e-3', n: 1 };

    const provider = ModelProvider.OpenAI;

    const headers = await createHeaderWithAuth({
      headers: { 'Content-Type': 'application/json' },
      provider,
    });

    const res = await fetch(API_ENDPOINTS.images(provider), {
      body: JSON.stringify(payload),
      headers: headers,
      method: 'POST',
      signal: options?.signal,
    });
    if (!res.ok) {
      throw await res.json();
    }

    const urls = await res.json();

    return urls[0] as string;
  };
}

export const imageGenerationService = new ImageGenerationService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/chat.ts
================================================================================

import { PluginRequestPayload, createHeadersWithPluginSettings } from '@lobehub/chat-plugin-sdk';
import { produce } from 'immer';
import { merge } from 'lodash-es';
import { DEFAULT_CHAT_PROVIDER } from '@/const/settings/llm';
import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';
import { INBOX_GUIDE_SYSTEMROLE } from '@/const/guide';
import { INBOX_SESSION_ID } from '@/const/session';
import { DEFAULT_AGENT_CONFIG } from '@/const/settings';
import { TracePayload, TraceTagMap } from '@/const/trace';
import { isServerMode } from '@/const/version';
import {
  AgentRuntime,
  AgentRuntimeError,
  ChatCompletionErrorPayload,
  ModelProvider,
} from '@/libs/agent-runtime';
import { filesPrompts } from '@/prompts/files';
import { BuiltinSystemRolePrompts } from '@/prompts/systemRole';
import { useSessionStore } from '@/store/session';
import { sessionMetaSelectors } from '@/store/session/selectors';
import { useToolStore } from '@/store/tool';
import { pluginSelectors, toolSelectors } from '@/store/tool/selectors';
import { useUserStore } from '@/store/user';
import {
  modelConfigSelectors,
  modelProviderSelectors,
  preferenceSelectors,
  userProfileSelectors,
} from '@/store/user/selectors';
import { ChatErrorType } from '@/types/fetch';
import { ChatMessage, MessageToolCall } from '@/types/message';
import type { ChatStreamPayload, OpenAIChatMessage } from '@/types/openai/chat';
import { UserMessageContentPart } from '@/types/openai/chat';
import { createErrorResponse } from '@/utils/errorResponse';
import { FetchSSEOptions, fetchSSE, getMessageError } from '@/utils/fetch';
import { genToolCallingName } from '@/utils/toolCall';
import { createTraceHeader, getTraceId } from '@/utils/trace';

import { createHeaderWithAuth, getProviderAuthPayload } from './_auth';
import { API_ENDPOINTS } from './_url';

interface FetchOptions extends FetchSSEOptions {
  historySummary?: string;
  isWelcomeQuestion?: boolean;
  signal?: AbortSignal | undefined;
  trace?: TracePayload;
}

interface GetChatCompletionPayload extends Partial<Omit<ChatStreamPayload, 'messages'>> {
  messages: ChatMessage[];
}

interface FetchAITaskResultParams extends FetchSSEOptions {
  abortController?: AbortController;
  onError?: (e: Error, rawError?: any) => void;
  /**
   * 加载状态变化处理函数
   * @param loading - 是否处于加载状态
   */
  onLoadingChange?: (loading: boolean) => void;
  /**
   * 请求对象
   */
  params: Partial<ChatStreamPayload>;
  trace?: TracePayload;
}

interface CreateAssistantMessageStream extends FetchSSEOptions {
  abortController?: AbortController;
  historySummary?: string;
  isWelcomeQuestion?: boolean;
  params: GetChatCompletionPayload;
  trace?: TracePayload;
}

/**
 * Initializes the AgentRuntime with the client store.
 * @param provider - The provider name.
 * @param payload - Init options
 * @returns The initialized AgentRuntime instance
 *
 * **Note**: if you try to fetch directly, use `fetchOnClient` instead.
 */
export function initializeWithClientStore(provider: string, payload: any) {
  provider = provider || DEFAULT_CHAT_PROVIDER;

  // add auth payload
  const providerAuthPayload = getProviderAuthPayload(provider);
  const commonOptions = {
    // Some provider base openai sdk, so enable it run on browser
    dangerouslyAllowBrowser: true,
  };
  let providerOptions = {};

  switch (provider) {
    default:
    case ModelProvider.OpenAI: {
      providerOptions = {
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Azure: {
      providerOptions = {
        apiVersion: providerAuthPayload?.azureApiVersion,
        // That's a wired properity, but just remapped it
        apikey: providerAuthPayload?.apiKey,
      };
      break;
    }
    case ModelProvider.Google: {
      providerOptions = {
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Bedrock: {
      if (providerAuthPayload?.apiKey) {
        providerOptions = {
          accessKeyId: providerAuthPayload?.awsAccessKeyId,
          accessKeySecret: providerAuthPayload?.awsSecretAccessKey,
          region: providerAuthPayload?.awsRegion,
          sessionToken: providerAuthPayload?.awsSessionToken,
        };
      }
      break;
    }
    case ModelProvider.Ollama: {
      providerOptions = {
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Perplexity: {
      providerOptions = {
        apikey: providerAuthPayload?.apiKey,
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Anthropic: {
      providerOptions = {
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Groq: {
      providerOptions = {
        apikey: providerAuthPayload?.apiKey,
        baseURL: providerAuthPayload?.endpoint,
      };
      break;
    }
    case ModelProvider.Cloudflare: {
      providerOptions = {
        apikey: providerAuthPayload?.apiKey,
        baseURLOrAccountID: providerAuthPayload?.cloudflareBaseURLOrAccountID,
      };
      break;
    }
  }

  /**
   * Configuration override order:
   * payload -> providerOptions -> providerAuthPayload -> commonOptions
   */
  return AgentRuntime.initializeWithProviderOptions(provider, {
    [provider]: {
      ...commonOptions,
      ...providerAuthPayload,
      ...providerOptions,
      ...payload,
    },
  });
}

class ChatService {
  createAssistantMessage = async (
    { plugins: enabledPlugins, messages, ...params }: GetChatCompletionPayload,
    options?: FetchOptions,
  ) => {
    const payload = merge(
      {
        model: DEFAULT_AGENT_CONFIG.model,
        stream: true,
        ...DEFAULT_AGENT_CONFIG.params,
      },
      params,
    );
    // ============  1. preprocess messages   ============ //

    const oaiMessages = this.processMessages(
      {
        messages,
        model: payload.model,
        tools: enabledPlugins,
      },
      options,
    );

    // ============  2. preprocess tools   ============ //

    const filterTools = toolSelectors.enabledSchema(enabledPlugins)(useToolStore.getState());

    // check this model can use function call
    const canUseFC = modelProviderSelectors.isModelEnabledFunctionCall(payload.model)(
      useUserStore.getState(),
    );
    // the rule that model can use tools:
    // 1. tools is not empty
    // 2. model can use function call
    const shouldUseTools = filterTools.length > 0 && canUseFC;

    const tools = shouldUseTools ? filterTools : undefined;

    return this.getChatCompletion({ ...params, messages: oaiMessages, tools }, options);
  };

  createAssistantMessageStream = async ({
    params,
    abortController,
    onAbort,
    onMessageHandle,
    onErrorHandle,
    onFinish,
    trace,
    isWelcomeQuestion,
    historySummary,
  }: CreateAssistantMessageStream) => {
    await this.createAssistantMessage(params, {
      historySummary,
      isWelcomeQuestion,
      onAbort,
      onErrorHandle,
      onFinish,
      onMessageHandle,
      signal: abortController?.signal,
      trace: this.mapTrace(trace, TraceTagMap.Chat),
    });
  };

  getChatCompletion = async (params: Partial<ChatStreamPayload>, options?: FetchOptions) => {
    const { signal } = options ?? {};

    const { provider = ModelProvider.OpenAI, ...res } = params;

    let model = res.model || DEFAULT_AGENT_CONFIG.model;

    // if the provider is Azure, get the deployment name as the request model
    if (provider === ModelProvider.Azure) {
      const chatModelCards = modelProviderSelectors.getModelCardsById(provider)(
        useUserStore.getState(),
      );

      const deploymentName = chatModelCards.find((i) => i.id === model)?.deploymentName;
      if (deploymentName) model = deploymentName;
    }

    const payload = merge(
      { model: DEFAULT_AGENT_CONFIG.model, stream: true, ...DEFAULT_AGENT_CONFIG.params },
      { ...res, model },
    );

    /**
     * Use browser agent runtime
     */
    const enableFetchOnClient = modelConfigSelectors.isProviderFetchOnClient(provider)(
      useUserStore.getState(),
    );

    let fetcher: typeof fetch | undefined = undefined;

    if (enableFetchOnClient) {
      /**
       * Notes:
       * 1. Browser agent runtime will skip auth check if a key and endpoint provided by
       *    user which will cause abuse of plugins services
       * 2. This feature will be disabled by default
       */
      fetcher = async () => {
        try {
          return await this.fetchOnClient({ payload, provider, signal });
        } catch (e) {
          const {
            errorType = ChatErrorType.BadRequest,
            error: errorContent,
            ...res
          } = e as ChatCompletionErrorPayload;

          const error = errorContent || e;
          // track the error at server side
          console.error(`Route: [${provider}] ${errorType}:`, error);

          return createErrorResponse(errorType, { error, ...res, provider });
        }
      };
    }

    const traceHeader = createTraceHeader({ ...options?.trace });

    const headers = await createHeaderWithAuth({
      headers: { 'Content-Type': 'application/json', ...traceHeader },
      provider,
    });

    const providerConfig = DEFAULT_MODEL_PROVIDER_LIST.find((item) => item.id === provider);

    return fetchSSE(API_ENDPOINTS.chat(provider), {
      body: JSON.stringify(payload),
      fetcher: fetcher,
      headers,
      method: 'POST',
      onAbort: options?.onAbort,
      onErrorHandle: options?.onErrorHandle,
      onFinish: options?.onFinish,
      onMessageHandle: options?.onMessageHandle,
      signal,
      // use smoothing when enable client fetch
      // https://github.com/lobehub/lobe-chat/issues/3800
      smoothing: providerConfig?.smoothing || enableFetchOnClient,
    });
  };

  /**
   * run the plugin api to get result
   * @param params
   * @param options
   */
  runPluginApi = async (params: PluginRequestPayload, options?: FetchOptions) => {
    const s = useToolStore.getState();

    const settings = pluginSelectors.getPluginSettingsById(params.identifier)(s);
    const manifest = pluginSelectors.getToolManifestById(params.identifier)(s);

    const traceHeader = createTraceHeader(this.mapTrace(options?.trace, TraceTagMap.ToolCalling));

    const headers = await createHeaderWithAuth({
      headers: { ...createHeadersWithPluginSettings(settings), ...traceHeader },
    });

    const gatewayURL = manifest?.gateway ?? API_ENDPOINTS.gateway;

    const res = await fetch(gatewayURL, {
      body: JSON.stringify({ ...params, manifest }),
      headers,
      method: 'POST',
      signal: options?.signal,
    });

    if (!res.ok) {
      throw await getMessageError(res);
    }

    const text = await res.text();
    return { text, traceId: getTraceId(res) };
  };

  fetchPresetTaskResult = async ({
    params,
    onMessageHandle,
    onFinish,
    onError,
    onLoadingChange,
    abortController,
    trace,
  }: FetchAITaskResultParams) => {
    const errorHandle = (error: Error, errorContent?: any) => {
      onLoadingChange?.(false);
      if (abortController?.signal.aborted) {
        return;
      }
      onError?.(error, errorContent);
      console.error(error);
    };

    onLoadingChange?.(true);

    try {
      await this.getChatCompletion(params, {
        onErrorHandle: (error) => {
          errorHandle(new Error(error.message), error);
        },
        onFinish,
        onMessageHandle,
        signal: abortController?.signal,
        trace: this.mapTrace(trace, TraceTagMap.SystemChain),
      });

      onLoadingChange?.(false);
    } catch (e) {
      errorHandle(e as Error);
    }
  };

  private processMessages = (
    {
      messages,
      tools,
      model,
    }: {
      messages: ChatMessage[];
      model: string;
      tools?: string[];
    },
    options?: FetchOptions,
  ): OpenAIChatMessage[] => {
    // handle content type for vision model
    // for the models with visual ability, add image url to content
    // refs: https://platform.openai.com/docs/guides/vision/quick-start
    const getContent = (m: ChatMessage) => {
      // only if message doesn't have images and files, then return the plain content
      if ((!m.imageList || m.imageList.length === 0) && (!m.fileList || m.fileList.length === 0))
        return m.content;

      const imageList = m.imageList || [];

      const filesContext = isServerMode ? filesPrompts({ fileList: m.fileList, imageList }) : '';
      return [
        { text: (m.content + '\n\n' + filesContext).trim(), type: 'text' },
        ...imageList.map(
          (i) => ({ image_url: { detail: 'auto', url: i.url }, type: 'image_url' }) as const,
        ),
      ] as UserMessageContentPart[];
    };

    let postMessages = messages.map((m): OpenAIChatMessage => {
      switch (m.role) {
        case 'user': {
          return { content: getContent(m), role: m.role };
        }

        case 'assistant': {
          return {
            content: m.content,
            role: m.role,
            tool_calls: m.tools?.map(
              (tool): MessageToolCall => ({
                function: {
                  arguments: tool.arguments,
                  name: genToolCallingName(tool.identifier, tool.apiName, tool.type),
                },
                id: tool.id,
                type: 'function',
              }),
            ),
          };
        }

        case 'tool': {
          return {
            content: m.content,
            name: genToolCallingName(m.plugin!.identifier, m.plugin!.apiName, m.plugin?.type),
            role: m.role,
            tool_call_id: m.tool_call_id,
          };
        }

        default: {
          return { content: m.content, role: m.role as any };
        }
      }
    });

    postMessages = produce(postMessages, (draft) => {
      // if it's a welcome question, inject InboxGuide SystemRole
      const inboxGuideSystemRole =
        options?.isWelcomeQuestion &&
        options?.trace?.sessionId === INBOX_SESSION_ID &&
        INBOX_GUIDE_SYSTEMROLE;

      // Inject Tool SystemRole
      const hasTools = tools && tools?.length > 0;
      const hasFC =
        hasTools &&
        modelProviderSelectors.isModelEnabledFunctionCall(model)(useUserStore.getState());
      const toolsSystemRoles =
        hasFC && toolSelectors.enabledSystemRoles(tools)(useToolStore.getState());

      const injectSystemRoles = BuiltinSystemRolePrompts({
        historySummary: options?.historySummary,
        plugins: toolsSystemRoles as string,
        welcome: inboxGuideSystemRole as string,
      });

      if (!injectSystemRoles) return;

      const systemMessage = draft.find((i) => i.role === 'system');

      if (systemMessage) {
        systemMessage.content = [systemMessage.content, injectSystemRoles]
          .filter(Boolean)
          .join('\n\n');
      } else {
        draft.unshift({
          content: injectSystemRoles,
          role: 'system',
        });
      }
    });

    return this.reorderToolMessages(postMessages);
  };

  private mapTrace = (trace?: TracePayload, tag?: TraceTagMap): TracePayload => {
    const tags = sessionMetaSelectors.currentAgentMeta(useSessionStore.getState()).tags || [];

    const enabled = preferenceSelectors.userAllowTrace(useUserStore.getState());

    if (!enabled) return { ...trace, enabled: false };

    return {
      ...trace,
      enabled: true,
      tags: [tag, ...(trace?.tags || []), ...tags].filter(Boolean) as string[],
      userId: userProfileSelectors.userId(useUserStore.getState()),
    };
  };

  /**
   * Fetch chat completion on the client side.

   */
  private fetchOnClient = async (params: {
    payload: Partial<ChatStreamPayload>;
    provider: string;
    signal?: AbortSignal;
  }) => {
    const agentRuntime = await initializeWithClientStore(params.provider, params.payload);
    const data = params.payload as ChatStreamPayload;

    /**
     * if enable login and not signed in, return unauthorized error
     */
    const userStore = useUserStore.getState();
    if (userStore.enableAuth() && !userStore.isSignedIn) {
      throw AgentRuntimeError.createError(ChatErrorType.InvalidAccessCode);
    }

    return agentRuntime.chat(data, { signal: params.signal });
  };

  /**
   * Reorder tool messages to ensure that tool messages are displayed in the correct order.
   * see https://github.com/lobehub/lobe-chat/pull/3155
   */
  private reorderToolMessages = (messages: OpenAIChatMessage[]): OpenAIChatMessage[] => {
    // 1. 先收集所有 assistant 消息中的有效 tool_call_id
    const validToolCallIds = new Set<string>();
    messages.forEach((message) => {
      if (message.role === 'assistant' && message.tool_calls) {
        message.tool_calls.forEach((toolCall) => {
          validToolCallIds.add(toolCall.id);
        });
      }
    });

    // 2. 收集所有有效的 tool 消息
    const toolMessages: Record<string, OpenAIChatMessage> = {};
    messages.forEach((message) => {
      if (
        message.role === 'tool' &&
        message.tool_call_id &&
        validToolCallIds.has(message.tool_call_id)
      ) {
        toolMessages[message.tool_call_id] = message;
      }
    });

    // 3. 重新排序消息
    const reorderedMessages: OpenAIChatMessage[] = [];
    messages.forEach((message) => {
      // 跳过无效的 tool 消息
      if (
        message.role === 'tool' &&
        (!message.tool_call_id || !validToolCallIds.has(message.tool_call_id))
      ) {
        return;
      }

      // 检查是否已经添加过该 tool 消息
      const hasPushed = reorderedMessages.some(
        (m) => !!message.tool_call_id && m.tool_call_id === message.tool_call_id,
      );

      if (hasPushed) return;

      reorderedMessages.push(message);

      // 如果是 assistant 消息且有 tool_calls，添加对应的 tool 消息
      if (message.role === 'assistant' && message.tool_calls) {
        message.tool_calls.forEach((toolCall) => {
          const correspondingToolMessage = toolMessages[toolCall.id];
          if (correspondingToolMessage) {
            reorderedMessages.push(correspondingToolMessage);
            delete toolMessages[toolCall.id];
          }
        });
      }
    });

    return reorderedMessages;
  };
}

export const chatService = new ChatService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/ragEval.ts
================================================================================

import { lambdaClient } from '@/libs/trpc/client';
import { uploadService } from '@/services/upload';
import {
  CreateNewEvalDatasets,
  CreateNewEvalEvaluation,
  EvalDatasetRecord,
  RAGEvalDataSetItem,
  RAGEvalEvaluationItem,
  insertEvalDatasetsSchema,
} from '@/types/eval';

class RAGEvalService {
  // Dataset
  createDataset = async (params: CreateNewEvalDatasets): Promise<number | undefined> => {
    return lambdaClient.ragEval.createDataset.mutate(params);
  };

  getDatasets = async (knowledgeBaseId: string): Promise<RAGEvalDataSetItem[]> => {
    return lambdaClient.ragEval.getDatasets.query({ knowledgeBaseId });
  };

  removeDataset = async (id: number): Promise<void> => {
    await lambdaClient.ragEval.removeDataset.mutate({ id });
  };

  updateDataset = async (
    id: number,
    value: Partial<typeof insertEvalDatasetsSchema>,
  ): Promise<void> => {
    await lambdaClient.ragEval.updateDataset.mutate({ id, value });
  };

  // Dataset Records
  getDatasetRecords = async (datasetId: number): Promise<EvalDatasetRecord[]> => {
    return lambdaClient.ragEval.getDatasetRecords.query({ datasetId });
  };

  removeDatasetRecord = async (id: number): Promise<void> => {
    await lambdaClient.ragEval.removeDatasetRecords.mutate({ id });
  };

  importDatasetRecords = async (datasetId: number, file: File): Promise<void> => {
    const { path } = await uploadService.uploadWithProgress(file, { directory: 'ragEval' });

    await lambdaClient.ragEval.importDatasetRecords.mutate({ datasetId, pathname: path });
  };

  // Evaluation
  createEvaluation = async (params: CreateNewEvalEvaluation): Promise<number | undefined> => {
    return lambdaClient.ragEval.createEvaluation.mutate(params);
  };

  getEvaluationList = async (knowledgeBaseId: string): Promise<RAGEvalEvaluationItem[]> => {
    return lambdaClient.ragEval.getEvaluationList.query({ knowledgeBaseId });
  };

  startEvaluationTask = async (id: number) => {
    return lambdaClient.ragEval.startEvaluationTask.mutate({ id });
  };

  removeEvaluation = async (id: number): Promise<void> => {
    await lambdaClient.ragEval.removeEvaluation.mutate({ id });
  };

  checkEvaluationStatus = async (id: number): Promise<{ success: boolean }> => {
    return lambdaClient.ragEval.checkEvaluationStatus.query({ id });
  };
}

export const ragEvalService = new RAGEvalService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/_url.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix */
import { transform } from 'lodash-es';

import { withBasePath } from '@/utils/basePath';

const mapWithBasePath = <T extends object>(apis: T): T => {
  return transform(apis, (result, value, key) => {
    if (typeof value === 'string') {
      // @ts-ignore
      result[key] = withBasePath(value);
    } else {
      result[key] = value;
    }
  });
};

export const API_ENDPOINTS = mapWithBasePath({
  oauth: '/api/auth',

  proxy: '/webapi/proxy',

  // assistant
  assistantStore: '/webapi/assistant/store',
  assistant: (identifier: string) => withBasePath(`/webapi/assistant/${identifier}`),

  // plugins
  gateway: '/webapi/plugin/gateway',
  pluginStore: '/webapi/plugin/store',

  // trace
  trace: '/webapi/trace',

  // chat
  chat: (provider: string) => withBasePath(`/webapi/chat/${provider}`),
  chatModels: (provider: string) => withBasePath(`/webapi/chat/models/${provider}`),

  // image
  images: (provider: string) => `/webapi/text-to-image/${provider}`,

  // STT
  stt: '/webapi/stt/openai',

  // TTS
  tts: '/webapi/tts/openai',
  edge: '/webapi/tts/edge',
  microsoft: '/webapi/tts/microsoft',
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/upload.ts
================================================================================

import { fileEnv } from '@/config/file';
import { edgeClient } from '@/libs/trpc/client';
import { API_ENDPOINTS } from '@/services/_url';
import { clientS3Storage } from '@/services/file/ClientS3';
import { FileMetadata } from '@/types/files';
import { FileUploadState, FileUploadStatus } from '@/types/files/upload';
import { uuid } from '@/utils/uuid';

export const UPLOAD_NETWORK_ERROR = 'NetWorkError';

class UploadService {
  uploadWithProgress = async (
    file: File,
    {
      onProgress,
      directory,
    }: {
      directory?: string;
      onProgress?: (status: FileUploadStatus, state: FileUploadState) => void;
    },
  ): Promise<FileMetadata> => {
    const xhr = new XMLHttpRequest();

    const { preSignUrl, ...result } = await this.getSignedUploadUrl(file, directory);
    let startTime = Date.now();
    xhr.upload.addEventListener('progress', (event) => {
      if (event.lengthComputable) {
        const progress = Number(((event.loaded / event.total) * 100).toFixed(1));

        const speedInByte = event.loaded / ((Date.now() - startTime) / 1000);

        onProgress?.('uploading', {
          // if the progress is 100, it means the file is uploaded
          // but the server is still processing it
          // so make it as 99.9 and let users think it's still uploading
          progress: progress === 100 ? 99.9 : progress,
          restTime: (event.total - event.loaded) / speedInByte,
          speed: speedInByte,
        });
      }
    });

    xhr.open('PUT', preSignUrl);
    xhr.setRequestHeader('Content-Type', file.type);
    const data = await file.arrayBuffer();

    await new Promise((resolve, reject) => {
      xhr.addEventListener('load', () => {
        if (xhr.status >= 200 && xhr.status < 300) {
          onProgress?.('success', {
            progress: 100,
            restTime: 0,
            speed: file.size / ((Date.now() - startTime) / 1000),
          });
          resolve(xhr.response);
        } else {
          reject(xhr.statusText);
        }
      });
      xhr.addEventListener('error', () => {
        if (xhr.status === 0) reject(UPLOAD_NETWORK_ERROR);
        else reject(xhr.statusText);
      });
      xhr.send(data);
    });

    return result;
  };

  uploadToClientS3 = async (hash: string, file: File): Promise<FileMetadata> => {
    await clientS3Storage.putObject(hash, file);

    return {
      date: (Date.now() / 1000 / 60 / 60).toFixed(0),
      dirname: '',
      filename: file.name,
      path: `client-s3://${hash}`,
    };
  };

  /**
   * get image File item with cors image URL
   * @param url
   * @param filename
   * @param fileType
   */
  getImageFileByUrlWithCORS = async (url: string, filename: string, fileType = 'image/png') => {
    const res = await fetch(API_ENDPOINTS.proxy, { body: url, method: 'POST' });
    const data = await res.arrayBuffer();

    return new File([data], filename, { lastModified: Date.now(), type: fileType });
  };

  private getSignedUploadUrl = async (
    file: File,
    directory?: string,
  ): Promise<
    FileMetadata & {
      preSignUrl: string;
    }
  > => {
    const filename = `${uuid()}.${file.name.split('.').at(-1)}`;

    // 精确到以 h 为单位的 path
    const date = (Date.now() / 1000 / 60 / 60).toFixed(0);
    const dirname = `${directory || fileEnv.NEXT_PUBLIC_S3_FILE_PATH}/${date}`;
    const pathname = `${dirname}/${filename}`;

    const preSignUrl = await edgeClient.upload.createS3PreSignedUrl.mutate({ pathname });

    return {
      date,
      dirname,
      filename,
      path: pathname,
      preSignUrl,
    };
  };
}

export const uploadService = new UploadService();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/file/client.ts
================================================================================

import { clientDB } from '@/database/client/db';
import { FileModel } from '@/database/server/models/file';
import { BaseClientService } from '@/services/baseClientService';
import { clientS3Storage } from '@/services/file/ClientS3';

import { IFileService } from './type';

export class ClientService extends BaseClientService implements IFileService {
  private get fileModel(): FileModel {
    return new FileModel(clientDB as any, this.userId);
  }

  createFile: IFileService['createFile'] = async (file) => {
    // save to local storage
    // we may want to save to a remote server later
    const res = await this.fileModel.create(
      {
        fileHash: file.hash,
        fileType: file.fileType,
        knowledgeBaseId: file.knowledgeBaseId,
        metadata: file.metadata,
        name: file.name,
        size: file.size,
        url: file.url!,
      },
      true,
    );

    // get file to base64 url
    const base64 = await this.getBase64ByFileHash(file.hash!);

    return {
      id: res.id,
      url: `data:${file.fileType};base64,${base64}`,
    };
  };

  getFile: IFileService['getFile'] = async (id) => {
    const item = await this.fileModel.findById(id);
    if (!item) {
      throw new Error('file not found');
    }

    // arrayBuffer to url
    const fileItem = await clientS3Storage.getObject(item.fileHash!);
    if (!fileItem) throw new Error('file not found');

    const url = URL.createObjectURL(fileItem);

    return {
      createdAt: new Date(item.createdAt),
      id,
      name: item.name,
      size: item.size,
      type: item.fileType,
      updatedAt: new Date(item.updatedAt),
      url,
    };
  };

  removeFile: IFileService['removeFile'] = async (id) => {
    await this.fileModel.delete(id, false);
  };

  removeFiles: IFileService['removeFiles'] = async (ids) => {
    await this.fileModel.deleteMany(ids, false);
  };

  removeAllFiles: IFileService['removeAllFiles'] = async () => {
    return this.fileModel.clear();
  };

  checkFileHash: IFileService['checkFileHash'] = async (hash) => {
    return this.fileModel.checkHash(hash);
  };

  private getBase64ByFileHash = async (hash: string) => {
    const fileItem = await clientS3Storage.getObject(hash);
    if (!fileItem) throw new Error('file not found');

    return Buffer.from(await fileItem.arrayBuffer()).toString('base64');
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/file/_deprecated.ts
================================================================================

import { FileModel } from '@/database/_deprecated/models/file';
import { clientS3Storage } from '@/services/file/ClientS3';
import { FileItem, UploadFileParams } from '@/types/files';

import { IFileService } from './type';

export class ClientService implements IFileService {
  async createFile(file: UploadFileParams) {
    // save to local storage
    // we may want to save to a remote server later
    const res = await FileModel.create({
      createdAt: Date.now(),
      data: undefined,
      fileHash: file.hash,
      fileType: file.fileType,
      metadata: file.metadata,
      name: file.name,
      saveMode: 'url',
      size: file.size,
      url: file.url,
    } as any);

    // get file to base64 url
    const base64 = await this.getBase64ByFileHash(file.hash!);

    return {
      id: res.id,
      url: `data:${file.fileType};base64,${base64}`,
    };
  }

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  async checkFileHash(_hash: string) {
    return { isExist: false, metadata: {} };
  }

  async getFile(id: string): Promise<FileItem> {
    const item = await FileModel.findById(id);
    if (!item) {
      throw new Error('file not found');
    }

    // arrayBuffer to blob or base64 to blob
    const blob = !!item.data
      ? new Blob([item.data!], { type: item.fileType })
      : // @ts-ignore
        new Blob([Buffer.from(item.base64!, 'base64')], { type: item.fileType });

    const url = URL.createObjectURL(blob);

    return {
      createdAt: new Date(item.createdAt),
      id,
      name: item.name,
      size: item.size,
      type: item.fileType,
      updatedAt: new Date(item.updatedAt),
      url,
    };
  }

  async removeFile(id: string) {
    return FileModel.delete(id);
  }

  async removeFiles(ids: string[]) {
    await Promise.all(ids.map((id) => FileModel.delete(id)));
  }

  async removeAllFiles() {
    return FileModel.clear();
  }

  private async getBase64ByFileHash(hash: string) {
    const fileItem = await clientS3Storage.getObject(hash);
    if (!fileItem) throw new Error('file not found');

    return Buffer.from(await fileItem.arrayBuffer()).toString('base64');
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/file/server.ts
================================================================================

import { lambdaClient } from '@/libs/trpc/client';
import { QueryFileListParams, QueryFileListSchemaType, UploadFileParams } from '@/types/files';

import { IFileService } from './type';

interface CreateFileParams extends Omit<UploadFileParams, 'url'> {
  knowledgeBaseId?: string;
  url: string;
}

export class ServerService implements IFileService {
  createFile: IFileService['createFile'] = async (params, knowledgeBaseId) => {
    return lambdaClient.file.createFile.mutate({ ...params, knowledgeBaseId } as CreateFileParams);
  };

  getFile: IFileService['getFile'] = async (id) => {
    const item = await lambdaClient.file.findById.query({ id });

    if (!item) {
      throw new Error('file not found');
    }

    return { ...item, type: item.fileType };
  };

  removeFile: IFileService['removeFile'] = async (id) => {
    await lambdaClient.file.removeFile.mutate({ id });
  };

  removeFiles: IFileService['removeFiles'] = async (ids) => {
    await lambdaClient.file.removeFiles.mutate({ ids });
  };

  removeAllFiles: IFileService['removeAllFiles'] = async () => {
    await lambdaClient.file.removeAllFiles.mutate();
  };

  getFiles = async (params: QueryFileListParams) => {
    return lambdaClient.file.getFiles.query(params as QueryFileListSchemaType);
  };

  getFileItem = async (id: string) => {
    return lambdaClient.file.getFileItemById.query({ id });
  };

  checkFileHash: IFileService['checkFileHash'] = async (hash) => {
    return lambdaClient.file.checkFileHash.mutate({ hash });
  };

  removeFileAsyncTask = async (id: string, type: 'embedding' | 'chunk') => {
    return lambdaClient.file.removeFileAsyncTask.mutate({ id, type });
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/file/ClientS3/index.ts
================================================================================

import { createStore, del, get, set } from 'idb-keyval';

const BROWSER_S3_DB_NAME = 'lobechat-local-s3';

export class BrowserS3Storage {
  private store;

  constructor() {
    // skip server-side rendering
    if (typeof window === 'undefined') return;

    this.store = createStore(BROWSER_S3_DB_NAME, 'objects');
  }

  /**
   * 上传文件
   * @param key 文件 hash
   * @param file File 对象
   */
  putObject = async (key: string, file: File): Promise<void> => {
    try {
      const data = await file.arrayBuffer();
      await set(key, { data, name: file.name, type: file.type }, this.store);
    } catch (e) {
      throw new Error(`Failed to put file ${file.name}: ${(e as Error).message}`);
    }
  };

  /**
   * 获取文件
   * @param key 文件 hash
   * @returns File 对象
   */
  getObject = async (key: string): Promise<File | undefined> => {
    try {
      const res = await get<{ data: ArrayBuffer; name: string; type: string }>(key, this.store);
      if (!res) return;

      return new File([res.data], res!.name, { type: res?.type });
    } catch (e) {
      throw new Error(`Failed to get object (key=${key}): ${(e as Error).message}`);
    }
  };

  /**
   * 删除文件
   * @param key 文件 hash
   */
  deleteObject = async (key: string): Promise<void> => {
    try {
      await del(key, this.store);
    } catch (e) {
      throw new Error(`Failed to delete object (key=${key}): ${(e as Error).message}`);
    }
  };
}

export const clientS3Storage = new BrowserS3Storage();


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/message/client.ts
================================================================================

import dayjs from 'dayjs';

import { INBOX_SESSION_ID } from '@/const/session';
import { clientDB } from '@/database/client/db';
import { MessageModel } from '@/database/server/models/message';
import { BaseClientService } from '@/services/baseClientService';
import { clientS3Storage } from '@/services/file/ClientS3';
import { ChatMessage } from '@/types/message';

import { IMessageService } from './type';

export class ClientService extends BaseClientService implements IMessageService {
  private get messageModel(): MessageModel {
    return new MessageModel(clientDB as any, this.userId);
  }

  createMessage: IMessageService['createMessage'] = async ({ sessionId, ...params }) => {
    const { id } = await this.messageModel.create({
      ...params,
      sessionId: this.toDbSessionId(sessionId) as string,
    });

    return id;
  };

  batchCreateMessages: IMessageService['batchCreateMessages'] = async (messages) => {
    return this.messageModel.batchCreate(messages);
  };

  getMessages: IMessageService['getMessages'] = async (sessionId, topicId) => {
    const data = await this.messageModel.query(
      {
        sessionId: this.toDbSessionId(sessionId),
        topicId,
      },
      {
        postProcessUrl: async (url, file) => {
          const hash = (url as string).replace('client-s3://', '');
          const base64 = await this.getBase64ByFileHash(hash);

          return `data:${file.fileType};base64,${base64}`;
        },
      },
    );

    return data as unknown as ChatMessage[];
  };

  getAllMessages: IMessageService['getAllMessages'] = async () => {
    const data = await this.messageModel.queryAll();

    return data as unknown as ChatMessage[];
  };

  countMessages: IMessageService['countMessages'] = async () => {
    return this.messageModel.count();
  };

  countTodayMessages: IMessageService['countTodayMessages'] = async () => {
    const topics = await this.messageModel.queryAll();
    return topics.filter(
      (item) => dayjs(item.createdAt).format('YYYY-MM-DD') === dayjs().format('YYYY-MM-DD'),
    ).length;
  };

  getAllMessagesInSession: IMessageService['getAllMessagesInSession'] = async (sessionId) => {
    const data = this.messageModel.queryBySessionId(this.toDbSessionId(sessionId));

    return data as unknown as ChatMessage[];
  };

  updateMessageError: IMessageService['updateMessageError'] = async (id, error) => {
    return this.messageModel.update(id, { error });
  };

  updateMessage: IMessageService['updateMessage'] = async (id, message) => {
    return this.messageModel.update(id, message);
  };

  updateMessageTTS: IMessageService['updateMessageTTS'] = async (id, tts) => {
    return this.messageModel.updateTTS(id, tts as any);
  };

  updateMessageTranslate: IMessageService['updateMessageTranslate'] = async (id, translate) => {
    return this.messageModel.updateTranslate(id, translate as any);
  };

  updateMessagePluginState: IMessageService['updateMessagePluginState'] = async (id, value) => {
    return this.messageModel.updatePluginState(id, value);
  };

  updateMessagePluginArguments: IMessageService['updateMessagePluginArguments'] = async (
    id,
    value,
  ) => {
    const args = typeof value === 'string' ? value : JSON.stringify(value);

    return this.messageModel.updateMessagePlugin(id, { arguments: args });
  };

  removeMessage: IMessageService['removeMessage'] = async (id) => {
    return this.messageModel.deleteMessage(id);
  };

  removeMessages: IMessageService['removeMessages'] = async (ids) => {
    return this.messageModel.deleteMessages(ids);
  };

  removeMessagesByAssistant: IMessageService['removeMessagesByAssistant'] = async (
    sessionId,
    topicId,
  ) => {
    return this.messageModel.deleteMessagesBySession(this.toDbSessionId(sessionId), topicId);
  };

  removeAllMessages: IMessageService['removeAllMessages'] = async () => {
    return this.messageModel.deleteAllMessages();
  };

  hasMessages: IMessageService['hasMessages'] = async () => {
    const number = await this.countMessages();
    return number > 0;
  };

  messageCountToCheckTrace: IMessageService['messageCountToCheckTrace'] = async () => {
    const number = await this.countMessages();
    return number >= 4;
  };

  private toDbSessionId = (sessionId: string | undefined) => {
    return sessionId === INBOX_SESSION_ID ? undefined : sessionId;
  };

  private getBase64ByFileHash = async (hash: string) => {
    const fileItem = await clientS3Storage.getObject(hash);
    if (!fileItem) throw new Error('file not found');

    return Buffer.from(await fileItem.arrayBuffer()).toString('base64');
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/user/client.ts
================================================================================

import { clientDB } from '@/database/client/db';
import { users } from '@/database/schemas';
import { MessageModel } from '@/database/server/models/message';
import { SessionModel } from '@/database/server/models/session';
import { UserModel } from '@/database/server/models/user';
import { BaseClientService } from '@/services/baseClientService';
import { UserPreference } from '@/types/user';
import { AsyncLocalStorage } from '@/utils/localStorage';

import { IUserService } from './type';

export class ClientService extends BaseClientService implements IUserService {
  private preferenceStorage: AsyncLocalStorage<UserPreference>;

  private get userModel(): UserModel {
    return new UserModel(clientDB as any, this.userId);
  }
  private get messageModel(): MessageModel {
    return new MessageModel(clientDB as any, this.userId);
  }
  private get sessionModel(): SessionModel {
    return new SessionModel(clientDB as any, this.userId);
  }

  constructor(userId?: string) {
    super(userId);
    this.preferenceStorage = new AsyncLocalStorage('LOBE_PREFERENCE');
  }

  getUserState: IUserService['getUserState'] = async () => {
    // if user not exist in the db, create one to make sure the user exist
    await this.makeSureUserExist();

    const state = await this.userModel.getUserState((encryptKeyVaultsStr) =>
      encryptKeyVaultsStr ? JSON.parse(encryptKeyVaultsStr) : {},
    );

    const user = await UserModel.findById(clientDB as any, this.userId);
    const messageCount = await this.messageModel.count();
    const sessionCount = await this.sessionModel.count();

    return {
      ...state,
      avatar: user?.avatar as string,
      canEnablePWAGuide: messageCount >= 4,
      canEnableTrace: messageCount >= 4,
      hasConversation: messageCount > 0 || sessionCount > 0,
      isOnboard: true,
      preference: await this.preferenceStorage.getFromLocalStorage(),
    };
  };

  updateUserSettings: IUserService['updateUserSettings'] = async (value) => {
    const { keyVaults, ...res } = value;

    return this.userModel.updateSetting({ ...res, keyVaults: JSON.stringify(keyVaults) });
  };

  resetUserSettings: IUserService['resetUserSettings'] = async () => {
    return this.userModel.deleteSetting();
  };

  updateAvatar = async (avatar: string) => {
    await this.userModel.updateUser({ avatar });
  };

  updatePreference: IUserService['updatePreference'] = async (preference) => {
    await this.preferenceStorage.saveToLocalStorage(preference);
  };

  updateGuide: IUserService['updateGuide'] = async () => {
    throw new Error('Method not implemented.');
  };

  makeSureUserExist = async () => {
    const existUsers = await clientDB.query.users.findMany();

    let user: { id: string };
    if (existUsers.length === 0) {
      const result = await clientDB.insert(users).values({ id: this.userId }).returning();
      user = result[0];
    } else {
      user = existUsers[0];
    }

    return user;
  };
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/services/user/_deprecated.ts
================================================================================

import { DeepPartial } from 'utility-types';

import { MessageModel } from '@/database/_deprecated/models/message';
import { SessionModel } from '@/database/_deprecated/models/session';
import { UserModel } from '@/database/_deprecated/models/user';
import { UserGuide, UserInitializationState, UserPreference } from '@/types/user';
import { UserSettings } from '@/types/user/settings';
import { AsyncLocalStorage } from '@/utils/localStorage';

import { IUserService } from './type';

export class ClientService implements IUserService {
  private preferenceStorage: AsyncLocalStorage<UserPreference>;

  constructor() {
    this.preferenceStorage = new AsyncLocalStorage('LOBE_PREFERENCE');
  }

  async getUserState(): Promise<UserInitializationState> {
    const user = await UserModel.getUser();
    const messageCount = await MessageModel.count();
    const sessionCount = await SessionModel.count();

    return {
      avatar: user.avatar,
      canEnablePWAGuide: messageCount >= 4,
      canEnableTrace: messageCount >= 4,
      hasConversation: messageCount > 0 || sessionCount > 0,
      isOnboard: true,
      preference: await this.preferenceStorage.getFromLocalStorage(),
      settings: user.settings as UserSettings,
      userId: user.uuid,
    };
  }

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  updateUserSettings = async (patch: DeepPartial<UserSettings>, _?: any) => {
    return UserModel.updateSettings(patch);
  };

  resetUserSettings = async () => {
    return UserModel.resetSettings();
  };

  async updateAvatar(avatar: string) {
    await UserModel.updateAvatar(avatar);
  }

  async updatePreference(preference: Partial<UserPreference>) {
    await this.preferenceStorage.saveToLocalStorage(preference);
  }

  // eslint-disable-next-line @typescript-eslint/no-unused-vars,unused-imports/no-unused-vars
  async updateGuide(guide: Partial<UserGuide>) {
    throw new Error('Method not implemented.');
  }
}


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/indexedDB.ts
================================================================================

import { createStore, delMany, getMany, setMany } from 'idb-keyval';
import { StorageValue } from 'zustand/middleware';

export const createIndexedDB = <State extends any>(dbName: string = 'indexedDB') => ({
  getItem: async <T extends State>(name: string): Promise<StorageValue<T> | undefined> => {
    const [version, state] = await getMany(['version', 'state'], createStore(dbName, name));

    if (!state) return undefined;

    return { state, version };
  },
  removeItem: async (name: string) => {
    await delMany(['version', 'state'], createStore(dbName, name));
  },
  setItem: async (name: string, state: any, version: number | undefined) => {
    const store = createStore(dbName, name);

    await setMany(
      [
        ['version', version],
        ['state', state],
      ],
      store,
    );
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/localStorage.ts
================================================================================

import { StorageValue } from 'zustand/middleware';

export const createLocalStorage = <State extends any>() => ({
  getItem: <T extends State>(name: string): StorageValue<T> | undefined => {
    if (!global.localStorage) return undefined;
    const string = localStorage.getItem(name);

    if (string) return JSON.parse(string) as StorageValue<T>;

    return undefined;
  },
  removeItem: (name: string) => {
    if (global.localStorage) localStorage.removeItem(name);
  },
  setItem: <T extends State>(name: string, state: T, version: number | undefined) => {
    if (global.localStorage) localStorage.setItem(name, JSON.stringify({ state, version }));
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/keyMapper.ts
================================================================================

import { HyperStorageOptionsObj } from './type';

export const createKeyMapper = (options: HyperStorageOptionsObj) => {
  const mapStateKeyToStorageKey = (
    key: string,
    mode: keyof HyperStorageOptionsObj = 'localStorage',
  ) => {
    const media = options[mode];
    if (media === false) return key;

    const selectors = media?.selectors;
    if (!selectors) return key;

    let storageKey: string | undefined;

    for (const selector of selectors) {
      if (typeof selector === 'string') {
        if (selector === key) storageKey = key;
      } else {
        if (selector[key]) storageKey = selector[key];
      }
    }

    return storageKey;
  };

  const getStateKeyFromStorageKey = (
    key: string,
    mode: keyof HyperStorageOptionsObj = 'localStorage',
  ) => {
    const media = options[mode];
    if (media === false) return key;

    const selectors = media?.selectors;
    if (!selectors) return key;

    let stateKey: string | undefined;

    for (const item of selectors) {
      // 对象如果是 字符串，直接返回该 item key
      if (typeof item === 'string') {
        if (item === key) stateKey = key;
      } else {
        for (const [k, v] of Object.entries(item)) {
          if (v === key) stateKey = k;
        }
      }
    }

    return stateKey;
  };

  return {
    getStateKeyFromStorageKey,
    mapStateKeyToStorageKey,
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/urlStorage.ts
================================================================================

import { isEmpty } from 'lodash-es';
import { StorageValue } from 'zustand/middleware';

interface UrlSearchHelper {
  getUrlSearch: () => string;
  setUrlSearch: (params: URLSearchParams) => void;
}

const createUrlSearch = (mode: 'search' | 'hash' = 'hash'): UrlSearchHelper => {
  if (mode === 'hash')
    return {
      getUrlSearch: () => location.hash.slice(1),
      setUrlSearch: (params: URLSearchParams) => (location.hash = params.toString()),
    };

  return {
    getUrlSearch: () => location.search.slice(1),
    setUrlSearch: (params: URLSearchParams) => {
      if (params.size === 0) return;

      history.replaceState({}, '', '?' + params.toString());
    },
  };
};

export const creatUrlStorage = <State extends object>(mode: 'hash' | 'search' = 'hash') => {
  const { setUrlSearch, getUrlSearch } = createUrlSearch(mode);

  return {
    getItem: <T extends State>(): StorageValue<T> | undefined => {
      const searchParameters = new URLSearchParams(getUrlSearch());

      if (searchParameters.size === 0) return undefined;

      const state = Object.fromEntries(searchParameters.entries()) as T;

      return { state };
    },
    removeItem: (key?: string) => {
      const searchParameters = new URLSearchParams(getUrlSearch());
      if (key) searchParameters.delete(key);

      setUrlSearch(searchParameters);
    },
    setItem: <T extends State>(name: string, state: T) => {
      const searchParameters = new URLSearchParams(getUrlSearch());

      for (const [urlKey, v] of Object.entries(state)) {
        switch (typeof v) {
          case 'boolean': {
            searchParameters.set(urlKey, (v ? 1 : 0).toString());
            break;
          }

          case 'bigint':
          case 'number': {
            searchParameters.set(urlKey, v.toString());
            break;
          }

          case 'string': {
            searchParameters.set(urlKey, v);
            break;
          }

          case 'object': {
            if (isEmpty(v)) {
              searchParameters.delete(urlKey);
              continue;
            }

            searchParameters.set(urlKey, JSON.stringify(v));
            break;
          }
        }
      }

      setUrlSearch(searchParameters);
    },
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/type.ts
================================================================================

export type StorageSelector = string | Record<string, string>;

export interface LocalStorageOptions {
  dbName?: string;
  /**
   * @default 'localStorage'
   */
  mode?: 'indexedDB' | 'localStorage';
  selectors: StorageSelector[];
}

export type HyperStorageOptionsObj = {
  localStorage?: LocalStorageOptions | false;
  url?: {
    /**
     * @default 'hash'
     */
    mode?: 'hash' | 'search';
    selectors: StorageSelector[];
  };
};

export type HyperStorageOptionsFn = () => HyperStorageOptionsObj;

export type HyperStorageOptions = HyperStorageOptionsObj | HyperStorageOptionsFn;


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/middleware/createHyperStorage/index.ts
================================================================================

import { PersistStorage, StorageValue } from 'zustand/middleware';

import { createIndexedDB } from './indexedDB';
import { createKeyMapper } from './keyMapper';
import { createLocalStorage } from './localStorage';
import { HyperStorageOptions } from './type';
import { creatUrlStorage } from './urlStorage';

export const createHyperStorage = <T extends object>(
  options: HyperStorageOptions,
): PersistStorage<T> => {
  const optionsObj = typeof options === 'function' ? options() : options;

  const getLocalStorageConfig = () => {
    if (optionsObj.localStorage === false) {
      return { dbName: '', skipLocalStorage: true, useIndexedDB: false };
    }

    const useIndexedDB = optionsObj.localStorage?.mode === 'indexedDB';
    const dbName = optionsObj.localStorage?.dbName || 'indexedDB';

    return { dbName, skipLocalStorage: false, useIndexedDB };
  };

  const hasUrl = !!optionsObj.url;

  const { skipLocalStorage, useIndexedDB, dbName } = getLocalStorageConfig();

  const { mapStateKeyToStorageKey, getStateKeyFromStorageKey } = createKeyMapper(optionsObj);

  const indexedDB = createIndexedDB(dbName);
  const localStorage = createLocalStorage();
  const urlStorage = creatUrlStorage(optionsObj.url?.mode);
  return {
    getItem: async (name): Promise<StorageValue<T>> => {
      const state: any = {};
      let version: number | undefined;

      // ============== 处理 Local Storage  ============== //
      if (!skipLocalStorage) {
        let localState: StorageValue<T> | undefined;

        // 如果使用 indexedDB，优先从 indexedDB 中获取
        if (useIndexedDB) {
          localState = await indexedDB.getItem(name);
        }
        // 如果 indexedDB 中不存在，则再试试 localStorage
        if (!localState) localState = localStorage.getItem(name);

        if (localState) {
          version = localState.version;
          for (const [k, v] of Object.entries(localState.state)) {
            const key = getStateKeyFromStorageKey(k, 'localStorage');
            if (key) state[key] = v;
          }
        }
      }

      // ============== 处理 URL Storage  ============== //
      // 不从 URL 中获取 version，由于 url 状态是临时态 不作为版本控制的依据
      if (hasUrl) {
        const urlState = urlStorage.getItem();

        if (urlState) {
          for (const [k, v] of Object.entries(urlState.state)) {
            const key = getStateKeyFromStorageKey(k, 'url');
            // 当存在 UrlSelector 逻辑，且 key 有效时，才将状态加入终态 state
            if (hasUrl && key) {
              state[key] = v;
            }
          }
        }
      }

      return { state, version };
    },
    removeItem: async (key) => {
      // ============== 处理 Local Storage  ============== //
      if (!skipLocalStorage) {
        if (useIndexedDB) {
          await indexedDB.removeItem(key);
        } else {
          localStorage.removeItem(key);
        }
      }

      // ============== 处理 URL Storage  ============== //
      if (hasUrl) {
        const storageKey = getStateKeyFromStorageKey(key, 'url');

        urlStorage.removeItem(storageKey);
      }
    },

    setItem: async (name, newValue) => {
      // ============== 处理 Local Storage  ============== //
      const localState: Record<string, any> = {};
      for (const [k, v] of Object.entries(newValue.state)) {
        const localKey = mapStateKeyToStorageKey(k, 'localStorage');
        if (localKey) localState[localKey] = v;
      }

      if (!skipLocalStorage) {
        if (useIndexedDB) {
          await indexedDB.setItem(name, localState, newValue.version);
        } else {
          localStorage.setItem(name, localState, newValue.version);
        }
      }

      // ============== 处理 URL Storage  ============== //
      if (hasUrl) {
        // 转换 key 为目标 key
        const finalEntries = Object.entries(newValue.state)
          .map(([k, v]) => {
            const urlKey = mapStateKeyToStorageKey(k, 'url');
            if (!urlKey) return undefined;
            return [urlKey, v];
          })
          .filter(Boolean) as [string, any][];

        urlStorage.setItem(name, Object.fromEntries(finalEntries));
      }
    },
  };
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/helpers.ts
================================================================================

import { LobeAgentChatConfig } from '@/types/agent';
import { ChatMessage } from '@/types/message';
import { OpenAIChatMessage } from '@/types/openai/chat';
import { encodeAsync } from '@/utils/tokenizer';

export const getMessagesTokenCount = async (messages: OpenAIChatMessage[]) =>
  encodeAsync(messages.map((m) => m.content).join(''));

export const getMessageById = (messages: ChatMessage[], id: string) =>
  messages.find((m) => m.id === id);

const getSlicedMessagesWithConfig = (
  messages: ChatMessage[],
  config: LobeAgentChatConfig,
  includeNewUserMessage?: boolean,
): ChatMessage[] => {
  // if historyCount is not enabled or set to 0, return all messages
  if (!config.enableHistoryCount || !config.historyCount) return messages;

  // if user send message, history will include this message so the total length should +1
  const messagesCount = !!includeNewUserMessage ? config.historyCount + 1 : config.historyCount;

  // if historyCount is negative, return empty array
  if (messagesCount <= 0) return [];

  // if historyCount is positive, return last N messages
  return messages.slice(-messagesCount);
};

export const chatHelpers = {
  getMessageById,
  getMessagesTokenCount,
  getSlicedMessagesWithConfig,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/portal/initialState.ts
================================================================================

import { PortalArtifact } from '@/types/artifact';

export interface PortalFile {
  chunkId?: string;
  chunkText?: string;
  fileId: string;
}

export interface ChatPortalState {
  portalArtifact?: PortalArtifact;
  portalArtifactDisplayMode?: 'code' | 'preview';
  portalFile?: PortalFile;
  portalMessageDetail?: string;
  portalThreadId?: string;
  portalToolMessage?: { id: string; identifier: string };
  showPortal: boolean;
}

export const initialChatPortalState: ChatPortalState = {
  portalArtifactDisplayMode: 'preview',
  showPortal: false,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/portal/selectors.ts
================================================================================

import { ARTIFACT_TAG_CLOSED_REGEX, ARTIFACT_TAG_REGEX } from '@/const/plugin';
import type { ChatStoreState } from '@/store/chat';

import { chatSelectors } from '../message/selectors';

const showPortal = (s: ChatStoreState) => s.showPortal;

const showMessageDetail = (s: ChatStoreState) => !!s.portalMessageDetail;
const messageDetailId = (s: ChatStoreState) => s.portalMessageDetail;

const showPluginUI = (s: ChatStoreState) => !!s.portalToolMessage;

const toolMessageId = (s: ChatStoreState) => s.portalToolMessage?.id;
const isPluginUIOpen = (id: string) => (s: ChatStoreState) =>
  toolMessageId(s) === id && showPortal(s);
const toolUIIdentifier = (s: ChatStoreState) => s.portalToolMessage?.identifier;

const showFilePreview = (s: ChatStoreState) => !!s.portalFile;
const previewFileId = (s: ChatStoreState) => s.portalFile?.fileId;
const chunkText = (s: ChatStoreState) => s.portalFile?.chunkText;

const showArtifactUI = (s: ChatStoreState) => !!s.portalArtifact;
const artifactTitle = (s: ChatStoreState) => s.portalArtifact?.title;
const artifactIdentifier = (s: ChatStoreState) => s.portalArtifact?.identifier || '';
const artifactMessageId = (s: ChatStoreState) => s.portalArtifact?.id;
const artifactType = (s: ChatStoreState) => s.portalArtifact?.type;
const artifactCodeLanguage = (s: ChatStoreState) => s.portalArtifact?.language;

const artifactMessageContent = (id: string) => (s: ChatStoreState) => {
  const message = chatSelectors.getMessageById(id)(s);
  return message?.content || '';
};

const artifactCode = (id: string) => (s: ChatStoreState) => {
  const messageContent = artifactMessageContent(id)(s);
  const result = messageContent.match(ARTIFACT_TAG_REGEX);

  return result?.groups?.content || '';
};

const isArtifactTagClosed = (id: string) => (s: ChatStoreState) => {
  const content = artifactMessageContent(id)(s);

  return ARTIFACT_TAG_CLOSED_REGEX.test(content || '');
};

/* eslint-disable sort-keys-fix/sort-keys-fix, typescript-sort-keys/interface */
export const chatPortalSelectors = {
  isPluginUIOpen,

  previewFileId,
  showFilePreview,
  chunkText,

  messageDetailId,
  showMessageDetail,

  showPluginUI,
  showPortal,

  toolMessageId,
  toolUIIdentifier,

  showArtifactUI,
  artifactTitle,
  artifactIdentifier,
  artifactMessageId,
  artifactType,
  artifactCode,
  artifactMessageContent,
  artifactCodeLanguage,
  isArtifactTagClosed,
};

export * from './selectors/thread';


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/translate/action.ts
================================================================================

import { produce } from 'immer';
import { StateCreator } from 'zustand/vanilla';

import { chainLangDetect } from '@/chains/langDetect';
import { chainTranslate } from '@/chains/translate';
import { TraceNameMap, TracePayload } from '@/const/trace';
import { supportLocales } from '@/locales/resources';
import { chatService } from '@/services/chat';
import { messageService } from '@/services/message';
import { chatSelectors } from '@/store/chat/selectors';
import { ChatStore } from '@/store/chat/store';
import { useUserStore } from '@/store/user';
import { systemAgentSelectors } from '@/store/user/selectors';
import { ChatTranslate } from '@/types/message';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

const n = setNamespace('enhance');

/**
 * chat translate
 */
export interface ChatTranslateAction {
  clearTranslate: (id: string) => Promise<void>;
  getCurrentTracePayload: (data: Partial<TracePayload>) => TracePayload;
  translateMessage: (id: string, targetLang: string) => Promise<void>;
  updateMessageTranslate: (id: string, data: Partial<ChatTranslate> | false) => Promise<void>;
}

export const chatTranslate: StateCreator<
  ChatStore,
  [['zustand/devtools', never]],
  [],
  ChatTranslateAction
> = (set, get) => ({
  clearTranslate: async (id) => {
    await get().updateMessageTranslate(id, false);
  },
  getCurrentTracePayload: (data) => ({
    sessionId: get().activeId,
    topicId: get().activeTopicId,
    ...data,
  }),

  translateMessage: async (id, targetLang) => {
    const { internal_toggleChatLoading, updateMessageTranslate, internal_dispatchMessage } = get();

    const message = chatSelectors.getMessageById(id)(get());
    if (!message) return;

    // Get current agent for translation
    const translationSetting = systemAgentSelectors.translation(useUserStore.getState());

    // create translate extra
    await updateMessageTranslate(id, { content: '', from: '', to: targetLang });

    internal_toggleChatLoading(true, id, n('translateMessage(start)', { id }) as string);

    let content = '';
    let from = '';

    // detect from language
    chatService.fetchPresetTaskResult({
      onFinish: async (data) => {
        if (data && supportLocales.includes(data)) from = data;

        await updateMessageTranslate(id, { content, from, to: targetLang });
      },
      params: merge(translationSetting, chainLangDetect(message.content)),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.LanguageDetect }),
    });

    // translate to target language
    await chatService.fetchPresetTaskResult({
      onFinish: async (content) => {
        await updateMessageTranslate(id, { content, from, to: targetLang });
        internal_toggleChatLoading(false, id);
      },
      onMessageHandle: (chunk) => {
        switch (chunk.type) {
          case 'text': {
            internal_dispatchMessage({
              id,
              key: 'translate',
              type: 'updateMessageExtra',
              value: produce({ content: '', from, to: targetLang }, (draft) => {
                content += chunk.text;
                draft.content += content;
              }),
            });
            break;
          }
        }
      },
      params: merge(translationSetting, chainTranslate(message.content, targetLang)),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.Translator }),
    });
  },

  updateMessageTranslate: async (id, data) => {
    await messageService.updateMessageTranslate(id, data);

    await get().refreshMessages();
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/message/selectors.ts
================================================================================

import { DEFAULT_USER_AVATAR } from '@/const/meta';
import { INBOX_SESSION_ID } from '@/const/session';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/selectors';
import { messageMapKey } from '@/store/chat/utils/messageMapKey';
import { useSessionStore } from '@/store/session';
import { sessionMetaSelectors } from '@/store/session/selectors';
import { useUserStore } from '@/store/user';
import { userProfileSelectors } from '@/store/user/selectors';
import { ChatFileItem, ChatMessage } from '@/types/message';

import { chatHelpers } from '../../helpers';
import type { ChatStoreState } from '../../initialState';

const getMeta = (message: ChatMessage) => {
  switch (message.role) {
    case 'user': {
      return {
        avatar: userProfileSelectors.userAvatar(useUserStore.getState()) || DEFAULT_USER_AVATAR,
      };
    }

    case 'system': {
      return message.meta;
    }

    default: {
      return sessionMetaSelectors.currentAgentMeta(useSessionStore.getState());
    }
  }
};

const currentChatKey = (s: ChatStoreState) => messageMapKey(s.activeId, s.activeTopicId);

/**
 * Current active raw message list, include thread messages
 */
const activeBaseChats = (s: ChatStoreState): ChatMessage[] => {
  if (!s.activeId) return [];

  const messages = s.messagesMap[currentChatKey(s)] || [];

  return messages.map((i) => ({ ...i, meta: getMeta(i) }));
};

/**
 * 排除掉所有 tool 消息，在展示时需要使用
 */
const activeBaseChatsWithoutTool = (s: ChatStoreState) => {
  const messages = activeBaseChats(s);

  return messages.filter((m) => m.role !== 'tool');
};

const getChatsWithThread = (s: ChatStoreState, messages: ChatMessage[]) => {
  // 如果没有 activeThreadId，则返回所有的主消息
  if (!s.activeThreadId) return messages.filter((m) => !m.threadId);

  const thread = s.threadMaps[s.activeTopicId!]?.find((t) => t.id === s.activeThreadId);

  if (!thread) return messages.filter((m) => !m.threadId);

  const sourceIndex = messages.findIndex((m) => m.id === thread.sourceMessageId);
  const sliced = messages.slice(0, sourceIndex + 1);

  return [...sliced, ...messages.filter((m) => m.threadId === s.activeThreadId)];
};

// ============= Main Display Chats ========== //
// =========================================== //
const mainDisplayChats = (s: ChatStoreState): ChatMessage[] => {
  const displayChats = activeBaseChatsWithoutTool(s);

  return getChatsWithThread(s, displayChats);
};

const mainDisplayChatIDs = (s: ChatStoreState) => mainDisplayChats(s).map((s) => s.id);

const mainAIChats = (s: ChatStoreState): ChatMessage[] => {
  const messages = activeBaseChats(s);

  return getChatsWithThread(s, messages);
};

const mainAIChatsWithHistoryConfig = (s: ChatStoreState): ChatMessage[] => {
  const chats = mainAIChats(s);
  const config = agentSelectors.currentAgentChatConfig(useAgentStore.getState());

  return chatHelpers.getSlicedMessagesWithConfig(chats, config);
};

const mainAIChatsMessageString = (s: ChatStoreState): string => {
  const chats = mainAIChatsWithHistoryConfig(s);
  return chats.map((m) => m.content).join('');
};

const currentToolMessages = (s: ChatStoreState) => {
  const messages = activeBaseChats(s);

  return messages.filter((m) => m.role === 'tool');
};

const currentUserMessages = (s: ChatStoreState) => {
  const messages = activeBaseChats(s);

  return messages.filter((m) => m.role === 'user');
};

const currentUserFiles = (s: ChatStoreState) => {
  const userMessages = currentUserMessages(s);

  return userMessages
    .filter((m) => m.fileList && m.fileList?.length > 0)
    .flatMap((m) => m.fileList)
    .filter(Boolean) as ChatFileItem[];
};

const showInboxWelcome = (s: ChatStoreState): boolean => {
  const isInbox = s.activeId === INBOX_SESSION_ID;
  if (!isInbox) return false;

  const data = activeBaseChats(s);
  return data.length === 0;
};

const getMessageById = (id: string) => (s: ChatStoreState) =>
  chatHelpers.getMessageById(activeBaseChats(s), id);

const countMessagesByThreadId = (id: string) => (s: ChatStoreState) => {
  const messages = activeBaseChats(s).filter((m) => m.threadId === id);

  return messages.length;
};

const getMessageByToolCallId = (id: string) => (s: ChatStoreState) => {
  const messages = activeBaseChats(s);
  return messages.find((m) => m.tool_call_id === id);
};
const getTraceIdByMessageId = (id: string) => (s: ChatStoreState) => getMessageById(id)(s)?.traceId;

const latestMessage = (s: ChatStoreState) => activeBaseChats(s).at(-1);

const currentChatLoadingState = (s: ChatStoreState) => !s.messagesInit;

const isCurrentChatLoaded = (s: ChatStoreState) => !!s.messagesMap[currentChatKey(s)];

const isMessageEditing = (id: string) => (s: ChatStoreState) => s.messageEditingIds.includes(id);
const isMessageLoading = (id: string) => (s: ChatStoreState) => s.messageLoadingIds.includes(id);

const isMessageGenerating = (id: string) => (s: ChatStoreState) => s.chatLoadingIds.includes(id);
const isMessageInRAGFlow = (id: string) => (s: ChatStoreState) =>
  s.messageRAGLoadingIds.includes(id);
const isPluginApiInvoking = (id: string) => (s: ChatStoreState) =>
  s.pluginApiLoadingIds.includes(id);

const isToolCallStreaming = (id: string, index: number) => (s: ChatStoreState) => {
  const isLoading = s.toolCallingStreamIds[id];

  if (!isLoading) return false;

  return isLoading[index];
};

const isAIGenerating = (s: ChatStoreState) =>
  s.chatLoadingIds.some((id) => mainDisplayChatIDs(s).includes(id));
const isInRAGFlow = (s: ChatStoreState) =>
  s.messageRAGLoadingIds.some((id) => mainDisplayChatIDs(s).includes(id));

const isCreatingMessage = (s: ChatStoreState) => s.isCreatingMessage;

const isHasMessageLoading = (s: ChatStoreState) =>
  s.messageLoadingIds.some((id) => mainDisplayChatIDs(s).includes(id));

/**
 * this function is used to determine whether the send button should be disabled
 */
const isSendButtonDisabledByMessage = (s: ChatStoreState) =>
  // 1. when there is message loading
  isHasMessageLoading(s) ||
  // 2. when is creating the topic
  s.creatingTopic ||
  // 3. when is creating the message
  isCreatingMessage(s) ||
  // 4. when the message is in RAG flow
  isInRAGFlow(s);

export const chatSelectors = {
  activeBaseChats,
  activeBaseChatsWithoutTool,
  countMessagesByThreadId,
  currentChatKey,
  currentChatLoadingState,
  currentToolMessages,
  currentUserFiles,
  getMessageById,
  getMessageByToolCallId,
  getTraceIdByMessageId,
  isAIGenerating,
  isCreatingMessage,
  isCurrentChatLoaded,
  isHasMessageLoading,
  isMessageEditing,
  isMessageGenerating,
  isMessageInRAGFlow,
  isMessageLoading,
  isPluginApiInvoking,
  isSendButtonDisabledByMessage,
  isToolCallStreaming,
  latestMessage,
  mainAIChats,
  mainAIChatsMessageString,
  mainAIChatsWithHistoryConfig,
  mainDisplayChatIDs,
  mainDisplayChats,
  showInboxWelcome,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/topic/action.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix, typescript-sort-keys/interface */
// Note: To make the code more logic and readable, we just disable the auto sort key eslint rule
// DON'T REMOVE THE FIRST LINE
import isEqual from 'fast-deep-equal';
import { t } from 'i18next';
import useSWR, { SWRResponse, mutate } from 'swr';
import { StateCreator } from 'zustand/vanilla';

import { chainSummaryTitle } from '@/chains/summaryTitle';
import { message } from '@/components/AntdStaticMethods';
import { LOADING_FLAT } from '@/const/message';
import { TraceNameMap } from '@/const/trace';
import { useClientDataSWR } from '@/libs/swr';
import { chatService } from '@/services/chat';
import { messageService } from '@/services/message';
import { topicService } from '@/services/topic';
import { CreateTopicParams } from '@/services/topic/type';
import type { ChatStore } from '@/store/chat';
import { useUserStore } from '@/store/user';
import { systemAgentSelectors } from '@/store/user/selectors';
import { ChatMessage } from '@/types/message';
import { ChatTopic } from '@/types/topic';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

import { chatSelectors } from '../message/selectors';
import { ChatTopicDispatch, topicReducer } from './reducer';
import { topicSelectors } from './selectors';

const n = setNamespace('t');

const SWR_USE_FETCH_TOPIC = 'SWR_USE_FETCH_TOPIC';
const SWR_USE_SEARCH_TOPIC = 'SWR_USE_SEARCH_TOPIC';

export interface ChatTopicAction {
  favoriteTopic: (id: string, favState: boolean) => Promise<void>;
  openNewTopicOrSaveTopic: () => Promise<void>;
  refreshTopic: () => Promise<void>;
  removeAllTopics: () => Promise<void>;
  removeSessionTopics: () => Promise<void>;
  removeTopic: (id: string) => Promise<void>;
  removeUnstarredTopic: () => Promise<void>;
  saveToTopic: () => Promise<string | undefined>;
  createTopic: () => Promise<string | undefined>;

  autoRenameTopicTitle: (id: string) => Promise<void>;
  duplicateTopic: (id: string) => Promise<void>;
  summaryTopicTitle: (topicId: string, messages: ChatMessage[]) => Promise<void>;
  switchTopic: (id?: string, skipRefreshMessage?: boolean) => Promise<void>;
  updateTopicTitle: (id: string, title: string) => Promise<void>;
  useFetchTopics: (enable: boolean, sessionId: string) => SWRResponse<ChatTopic[]>;
  useSearchTopics: (keywords?: string, sessionId?: string) => SWRResponse<ChatTopic[]>;

  internal_updateTopicTitleInSummary: (id: string, title: string) => void;
  internal_updateTopicLoading: (id: string, loading: boolean) => void;
  internal_createTopic: (params: CreateTopicParams) => Promise<string>;
  internal_updateTopic: (id: string, data: Partial<ChatTopic>) => Promise<void>;
  internal_dispatchTopic: (payload: ChatTopicDispatch, action?: any) => void;
}

export const chatTopic: StateCreator<
  ChatStore,
  [['zustand/devtools', never]],
  [],
  ChatTopicAction
> = (set, get) => ({
  // create
  openNewTopicOrSaveTopic: async () => {
    const { switchTopic, saveToTopic, refreshMessages, activeTopicId } = get();
    const hasTopic = !!activeTopicId;

    if (hasTopic) switchTopic();
    else {
      await saveToTopic();
      refreshMessages();
    }
  },

  createTopic: async () => {
    const { activeId, internal_createTopic } = get();

    const messages = chatSelectors.activeBaseChats(get());

    set({ creatingTopic: true }, false, n('creatingTopic/start'));
    const topicId = await internal_createTopic({
      sessionId: activeId,
      title: t('defaultTitle', { ns: 'topic' }),
      messages: messages.map((m) => m.id),
    });
    set({ creatingTopic: false }, false, n('creatingTopic/end'));

    return topicId;
  },

  saveToTopic: async () => {
    // if there is no message, stop
    const messages = chatSelectors.activeBaseChats(get());
    if (messages.length === 0) return;

    const { activeId, summaryTopicTitle, internal_createTopic } = get();

    // 1. create topic and bind these messages
    const topicId = await internal_createTopic({
      sessionId: activeId,
      title: t('defaultTitle', { ns: 'topic' }),
      messages: messages.map((m) => m.id),
    });

    get().internal_updateTopicLoading(topicId, true);
    // 2. auto summary topic Title
    // we don't need to wait for summary, just let it run async
    summaryTopicTitle(topicId, messages);

    return topicId;
  },
  duplicateTopic: async (id) => {
    const { refreshTopic, switchTopic } = get();

    const topic = topicSelectors.getTopicById(id)(get());
    if (!topic) return;

    const newTitle = t('duplicateTitle', { ns: 'chat', title: topic?.title });

    message.loading({
      content: t('duplicateLoading', { ns: 'topic' }),
      key: 'duplicateTopic',
      duration: 0,
    });

    const newTopicId = await topicService.cloneTopic(id, newTitle);
    await refreshTopic();
    message.destroy('duplicateTopic');
    message.success(t('duplicateSuccess', { ns: 'topic' }));

    await switchTopic(newTopicId);
  },
  // update
  summaryTopicTitle: async (topicId, messages) => {
    const { internal_updateTopicTitleInSummary, internal_updateTopicLoading } = get();
    const topic = topicSelectors.getTopicById(topicId)(get());
    if (!topic) return;

    internal_updateTopicTitleInSummary(topicId, LOADING_FLAT);

    let output = '';

    // Get current agent for topic
    const topicConfig = systemAgentSelectors.topic(useUserStore.getState());

    // Automatically summarize the topic title
    await chatService.fetchPresetTaskResult({
      onError: () => {
        internal_updateTopicTitleInSummary(topicId, topic.title);
      },
      onFinish: async (text) => {
        await get().internal_updateTopic(topicId, { title: text });
      },
      onLoadingChange: (loading) => {
        internal_updateTopicLoading(topicId, loading);
      },
      onMessageHandle: (chunk) => {
        switch (chunk.type) {
          case 'text': {
            output += chunk.text;
          }
        }

        internal_updateTopicTitleInSummary(topicId, output);
      },
      params: merge(topicConfig, chainSummaryTitle(messages)),
      trace: get().getCurrentTracePayload({ traceName: TraceNameMap.SummaryTopicTitle, topicId }),
    });
  },
  favoriteTopic: async (id, favorite) => {
    await get().internal_updateTopic(id, { favorite });
  },

  updateTopicTitle: async (id, title) => {
    await get().internal_updateTopic(id, { title });
  },

  autoRenameTopicTitle: async (id) => {
    const { activeId: sessionId, summaryTopicTitle, internal_updateTopicLoading } = get();

    internal_updateTopicLoading(id, true);
    const messages = await messageService.getMessages(sessionId, id);

    await summaryTopicTitle(id, messages);
    internal_updateTopicLoading(id, false);
  },

  // query
  useFetchTopics: (enable, sessionId) =>
    useClientDataSWR<ChatTopic[]>(
      enable ? [SWR_USE_FETCH_TOPIC, sessionId] : null,
      async ([, sessionId]: [string, string]) => topicService.getTopics({ sessionId }),
      {
        suspense: true,
        fallbackData: [],
        onSuccess: (topics) => {
          const nextMap = { ...get().topicMaps, [sessionId]: topics };

          // no need to update map if the topics have been init and the map is the same
          if (get().topicsInit && isEqual(nextMap, get().topicMaps)) return;

          set(
            { topicMaps: nextMap, topicsInit: true },
            false,
            n('useFetchTopics(success)', { sessionId }),
          );
        },
      },
    ),
  useSearchTopics: (keywords, sessionId) =>
    useSWR<ChatTopic[]>(
      [SWR_USE_SEARCH_TOPIC, keywords, sessionId],
      ([, keywords, sessionId]: [string, string, string]) =>
        topicService.searchTopics(keywords, sessionId),
      {
        onSuccess: (data) => {
          set({ searchTopics: data }, false, n('useSearchTopics(success)', { keywords }));
        },
      },
    ),
  switchTopic: async (id, skipRefreshMessage) => {
    set(
      { activeTopicId: !id ? (null as any) : id, activeThreadId: undefined },
      false,
      n('toggleTopic'),
    );

    if (skipRefreshMessage) return;
    await get().refreshMessages();
  },
  // delete
  removeSessionTopics: async () => {
    const { switchTopic, activeId, refreshTopic } = get();

    await topicService.removeTopics(activeId);
    await refreshTopic();

    // switch to default topic
    switchTopic();
  },
  removeAllTopics: async () => {
    const { refreshTopic } = get();

    await topicService.removeAllTopic();
    await refreshTopic();
  },
  removeTopic: async (id) => {
    const { activeId, activeTopicId, switchTopic, refreshTopic } = get();

    // remove messages in the topic
    // TODO: Need to remove because server service don't need to call it
    await messageService.removeMessagesByAssistant(activeId, id);

    // remove topic
    await topicService.removeTopic(id);
    await refreshTopic();

    // switch bach to default topic
    if (activeTopicId === id) switchTopic();
  },
  removeUnstarredTopic: async () => {
    const { refreshTopic, switchTopic } = get();
    const topics = topicSelectors.currentUnFavTopics(get());

    await topicService.batchRemoveTopics(topics.map((t) => t.id));
    await refreshTopic();

    // 切换到默认 topic
    switchTopic();
  },

  // Internal process method of the topics
  internal_updateTopicTitleInSummary: (id, title) => {
    get().internal_dispatchTopic(
      { type: 'updateTopic', id, value: { title } },
      'updateTopicTitleInSummary',
    );
  },
  refreshTopic: async () => {
    return mutate([SWR_USE_FETCH_TOPIC, get().activeId]);
  },

  internal_updateTopicLoading: (id, loading) => {
    set(
      (state) => {
        if (loading) return { topicLoadingIds: [...state.topicLoadingIds, id] };

        return { topicLoadingIds: state.topicLoadingIds.filter((i) => i !== id) };
      },
      false,
      n('updateTopicLoading'),
    );
  },

  internal_updateTopic: async (id, data) => {
    get().internal_dispatchTopic({ type: 'updateTopic', id, value: data });

    get().internal_updateTopicLoading(id, true);
    await topicService.updateTopic(id, data);
    await get().refreshTopic();
    get().internal_updateTopicLoading(id, false);
  },
  internal_createTopic: async (params) => {
    const tmpId = Date.now().toString();
    get().internal_dispatchTopic(
      { type: 'addTopic', value: { ...params, id: tmpId } },
      'internal_createTopic',
    );

    get().internal_updateTopicLoading(tmpId, true);
    const topicId = await topicService.createTopic(params);
    get().internal_updateTopicLoading(tmpId, false);

    get().internal_updateTopicLoading(topicId, true);
    await get().refreshTopic();
    get().internal_updateTopicLoading(topicId, false);

    return topicId;
  },

  internal_dispatchTopic: (payload, action) => {
    const nextTopics = topicReducer(topicSelectors.currentTopics(get()), payload);
    const nextMap = { ...get().topicMaps, [get().activeId]: nextTopics };

    // no need to update map if is the same
    if (isEqual(nextMap, get().topicMaps)) return;

    set({ topicMaps: nextMap }, false, action ?? n(`dispatchTopic/${payload.type}`));
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/aiChat/initialState.ts
================================================================================

export interface ChatAIChatState {
  abortController?: AbortController;
  /**
   * is the AI message is generating
   */
  chatLoadingIds: string[];
  inputFiles: File[];
  inputMessage: string;
  /**
   * is the message is in RAG flow
   */
  messageRAGLoadingIds: string[];
  pluginApiLoadingIds: string[];
  /**
   * the tool calling stream ids
   */
  toolCallingStreamIds: Record<string, boolean[]>;
}

export const initialAiChatState: ChatAIChatState = {
  chatLoadingIds: [],
  inputFiles: [],
  inputMessage: '',
  messageRAGLoadingIds: [],
  pluginApiLoadingIds: [],
  toolCallingStreamIds: {},
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/aiChat/actions/generateAIChat.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix, typescript-sort-keys/interface */
// Disable the auto sort key eslint rule to make the code more logic and readable
import { produce } from 'immer';
import { template } from 'lodash-es';
import { StateCreator } from 'zustand/vanilla';

import { LOADING_FLAT, MESSAGE_CANCEL_FLAT } from '@/const/message';
import { DEFAULT_AGENT_CHAT_CONFIG } from '@/const/settings';
import { TraceEventType, TraceNameMap } from '@/const/trace';
import { isServerMode } from '@/const/version';
import { knowledgeBaseQAPrompts } from '@/prompts/knowledgeBaseQA';
import { chatService } from '@/services/chat';
import { messageService } from '@/services/message';
import { useAgentStore } from '@/store/agent';
import { chatHelpers } from '@/store/chat/helpers';
import { ChatStore } from '@/store/chat/store';
import { messageMapKey } from '@/store/chat/utils/messageMapKey';
import { useSessionStore } from '@/store/session';
import { ChatMessage, CreateMessageParams, SendMessageParams } from '@/types/message';
import { MessageSemanticSearchChunk } from '@/types/rag';
import { setNamespace } from '@/utils/storeDebug';

import { chatSelectors, topicSelectors } from '../../../selectors';
import { getAgentChatConfig, getAgentConfig, getAgentKnowledge } from './helpers';

const n = setNamespace('ai');

interface ProcessMessageParams {
  traceId?: string;
  isWelcomeQuestion?: boolean;
  /**
   * the RAG query content, should be embedding and used in the semantic search
   */
  ragQuery?: string;
  threadId?: string;
  inPortalThread?: boolean;
}

export interface AIGenerateAction {
  /**
   * Sends a new message to the AI chat system
   */
  sendMessage: (params: SendMessageParams) => Promise<void>;
  /**
   * Regenerates a specific message in the chat
   */
  regenerateMessage: (id: string) => Promise<void>;
  /**
   * Deletes an existing message and generates a new one in its place
   */
  delAndRegenerateMessage: (id: string) => Promise<void>;
  /**
   * Interrupts the ongoing ai message generation process
   */
  stopGenerateMessage: () => void;

  // =========  ↓ Internal Method ↓  ========== //
  // ========================================== //
  // ========================================== //

  /**
   * Executes the core processing logic for AI messages
   * including preprocessing and postprocessing steps
   */
  internal_coreProcessMessage: (
    messages: ChatMessage[],
    parentId: string,
    params?: ProcessMessageParams,
  ) => Promise<void>;
  /**
   * Retrieves an AI-generated chat message from the backend service
   */
  internal_fetchAIChatMessage: (
    messages: ChatMessage[],
    assistantMessageId: string,
    params?: ProcessMessageParams,
  ) => Promise<{
    isFunctionCall: boolean;
    traceId?: string;
  }>;
  /**
   * Resends a specific message, optionally using a trace ID for tracking
   */
  internal_resendMessage: (
    id: string,
    params?: {
      traceId?: string;
      messages?: ChatMessage[];
      threadId?: string;
      inPortalThread?: boolean;
    },
  ) => Promise<void>;
  /**
   * Toggles the loading state for AI message generation, managing the UI feedback
   */
  internal_toggleChatLoading: (
    loading: boolean,
    id?: string,
    action?: string,
  ) => AbortController | undefined;
  /**
   * Controls the streaming state of tool calling processes, updating the UI accordingly
   */
  internal_toggleToolCallingStreaming: (id: string, streaming: boolean[] | undefined) => void;
}

export const generateAIChat: StateCreator<
  ChatStore,
  [['zustand/devtools', never]],
  [],
  AIGenerateAction
> = (set, get) => ({
  delAndRegenerateMessage: async (id) => {
    const traceId = chatSelectors.getTraceIdByMessageId(id)(get());
    get().internal_resendMessage(id, { traceId });
    get().deleteMessage(id);

    // trace the delete and regenerate message
    get().internal_traceMessage(id, { eventType: TraceEventType.DeleteAndRegenerateMessage });
  },
  regenerateMessage: async (id) => {
    const traceId = chatSelectors.getTraceIdByMessageId(id)(get());
    await get().internal_resendMessage(id, { traceId });

    // trace the delete and regenerate message
    get().internal_traceMessage(id, { eventType: TraceEventType.RegenerateMessage });
  },

  sendMessage: async ({ message, files, onlyAddUserMessage, isWelcomeQuestion }) => {
    const { internal_coreProcessMessage, activeTopicId, activeId, activeThreadId } = get();
    if (!activeId) return;

    const fileIdList = files?.map((f) => f.id);

    const hasFile = !!fileIdList && fileIdList.length > 0;

    // if message is empty or no files, then stop
    if (!message && !hasFile) return;

    set({ isCreatingMessage: true }, false, n('creatingMessage/start'));

    const newMessage: CreateMessageParams = {
      content: message,
      // if message has attached with files, then add files to message and the agent
      files: fileIdList,
      role: 'user',
      sessionId: activeId,
      // if there is activeTopicId，then add topicId to message
      topicId: activeTopicId,
      threadId: activeThreadId,
    };

    const agentConfig = getAgentChatConfig();

    let tempMessageId: string | undefined = undefined;
    let newTopicId: string | undefined = undefined;

    // it should be the default topic, then
    // if autoCreateTopic is enabled, check to whether we need to create a topic
    if (!onlyAddUserMessage && !activeTopicId && agentConfig.enableAutoCreateTopic) {
      // check activeTopic and then auto create topic
      const chats = chatSelectors.activeBaseChats(get());

      // we will add two messages (user and assistant), so the finial length should +2
      const featureLength = chats.length + 2;

      // if there is no activeTopicId and the feature length is greater than the threshold
      // then create a new topic and active it
      if (!get().activeTopicId && featureLength >= agentConfig.autoCreateTopicThreshold) {
        // we need to create a temp message for optimistic update
        tempMessageId = get().internal_createTmpMessage(newMessage);
        get().internal_toggleMessageLoading(true, tempMessageId);

        const topicId = await get().createTopic();

        if (topicId) {
          newTopicId = topicId;
          newMessage.topicId = topicId;

          // we need to copy the messages to the new topic or the message will disappear
          const mapKey = chatSelectors.currentChatKey(get());
          const newMaps = {
            ...get().messagesMap,
            [messageMapKey(activeId, topicId)]: get().messagesMap[mapKey],
          };
          set({ messagesMap: newMaps }, false, n('moveMessagesToNewTopic'));

          // make the topic loading
          get().internal_updateTopicLoading(topicId, true);
        }
      }
    }
    //  update assistant update to make it rerank
    useSessionStore.getState().triggerSessionUpdate(get().activeId);

    const id = await get().internal_createMessage(newMessage, {
      tempMessageId,
      skipRefresh: !onlyAddUserMessage && newMessage.fileList?.length === 0,
    });

    if (tempMessageId) get().internal_toggleMessageLoading(false, tempMessageId);

    // switch to the new topic if create the new topic
    if (!!newTopicId) {
      await get().switchTopic(newTopicId, true);
      await get().internal_fetchMessages();

      // delete previous messages
      // remove the temp message map
      const newMaps = { ...get().messagesMap, [messageMapKey(activeId, null)]: [] };
      set({ messagesMap: newMaps }, false, 'internal_copyMessages');
    }

    // if only add user message, then stop
    if (onlyAddUserMessage) {
      set({ isCreatingMessage: false }, false, 'creatingMessage/start');
      return;
    }

    // Get the current messages to generate AI response
    const messages = chatSelectors.activeBaseChats(get());
    const userFiles = chatSelectors.currentUserFiles(get()).map((f) => f.id);

    await internal_coreProcessMessage(messages, id, {
      isWelcomeQuestion,
      ragQuery: get().internal_shouldUseRAG() ? message : undefined,
      threadId: activeThreadId,
    });

    set({ isCreatingMessage: false }, false, n('creatingMessage/stop'));

    const summaryTitle = async () => {
      // if autoCreateTopic is false, then stop
      if (!agentConfig.enableAutoCreateTopic) return;

      // check activeTopic and then auto update topic title
      if (newTopicId) {
        const chats = chatSelectors.activeBaseChats(get());
        await get().summaryTopicTitle(newTopicId, chats);
        return;
      }

      const topic = topicSelectors.currentActiveTopic(get());

      if (topic && !topic.title) {
        const chats = chatSelectors.activeBaseChats(get());
        await get().summaryTopicTitle(topic.id, chats);
      }
    };

    // if there is relative files, then add files to agent
    // only available in server mode
    const addFilesToAgent = async () => {
      if (userFiles.length === 0 || !isServerMode) return;

      await useAgentStore.getState().addFilesToAgent(userFiles, false);
    };

    await Promise.all([summaryTitle(), addFilesToAgent()]);
  },
  stopGenerateMessage: () => {
    const { abortController, internal_toggleChatLoading } = get();
    if (!abortController) return;

    abortController.abort(MESSAGE_CANCEL_FLAT);

    internal_toggleChatLoading(false, undefined, n('stopGenerateMessage') as string);
  },

  // the internal process method of the AI message
  internal_coreProcessMessage: async (originalMessages, userMessageId, params) => {
    const { internal_fetchAIChatMessage, triggerToolCalls, refreshMessages, activeTopicId } = get();

    // create a new array to avoid the original messages array change
    const messages = [...originalMessages];

    const { model, provider, chatConfig } = getAgentConfig();

    let fileChunks: MessageSemanticSearchChunk[] | undefined;
    let ragQueryId;

    // go into RAG flow if there is ragQuery flag
    if (params?.ragQuery) {
      // 1. get the relative chunks from semantic search
      const { chunks, queryId, rewriteQuery } = await get().internal_retrieveChunks(
        userMessageId,
        params?.ragQuery,
        // should skip the last content
        messages.map((m) => m.content).slice(0, messages.length - 1),
      );

      ragQueryId = queryId;

      const lastMsg = messages.pop() as ChatMessage;

      // 2. build the retrieve context messages
      const knowledgeBaseQAContext = knowledgeBaseQAPrompts({
        chunks,
        userQuery: lastMsg.content,
        rewriteQuery,
        knowledge: getAgentKnowledge(),
      });

      // 3. add the retrieve context messages to the messages history
      messages.push({
        ...lastMsg,
        content: (lastMsg.content + '\n\n' + knowledgeBaseQAContext).trim(),
      });

      fileChunks = chunks.map((c) => ({ id: c.id, similarity: c.similarity }));
    }

    // 2. Add an empty message to place the AI response
    const assistantMessage: CreateMessageParams = {
      role: 'assistant',
      content: LOADING_FLAT,
      fromModel: model,
      fromProvider: provider,

      parentId: userMessageId,
      sessionId: get().activeId,
      topicId: activeTopicId, // if there is activeTopicId，then add it to topicId
      threadId: params?.threadId,
      fileChunks,
      ragQueryId,
    };

    const assistantId = await get().internal_createMessage(assistantMessage);

    // 3. fetch the AI response
    const { isFunctionCall } = await internal_fetchAIChatMessage(messages, assistantId, params);

    // 4. if it's the function call message, trigger the function method
    if (isFunctionCall) {
      await refreshMessages();
      await triggerToolCalls(assistantId, {
        threadId: params?.threadId,
        inPortalThread: params?.inPortalThread,
      });
    }

    // 5. summary history if context messages is larger than historyCount
    const historyCount =
      chatConfig.historyCount || (DEFAULT_AGENT_CHAT_CONFIG.historyCount as number);

    if (
      chatConfig.enableHistoryCount &&
      chatConfig.enableCompressHistory &&
      originalMessages.length > historyCount
    ) {
      // after generation: [u1,a1,u2,a2,u3,a3]
      // but the `originalMessages` is still: [u1,a1,u2,a2,u3]
      // So if historyCount=2, we need to summary [u1,a1,u2,a2]
      // because user find UI is [u1,a1,u2,a2 | u3,a3]
      const historyMessages = originalMessages.slice(0, -historyCount + 1);

      await get().internal_summaryHistory(historyMessages);
    }
  },
  internal_fetchAIChatMessage: async (messages, assistantId, params) => {
    const {
      internal_toggleChatLoading,
      refreshMessages,
      internal_updateMessageContent,
      internal_dispatchMessage,
      internal_toggleToolCallingStreaming,
    } = get();

    const abortController = internal_toggleChatLoading(
      true,
      assistantId,
      n('generateMessage(start)', { assistantId, messages }) as string,
    );

    const agentConfig = getAgentConfig();
    const chatConfig = agentConfig.chatConfig;

    const compiler = template(chatConfig.inputTemplate, { interpolate: /{{([\S\s]+?)}}/g });

    // ================================== //
    //   messages uniformly preprocess    //
    // ================================== //

    // 1. slice messages with config
    let preprocessMsgs = chatHelpers.getSlicedMessagesWithConfig(messages, chatConfig, true);

    // 2. replace inputMessage template
    preprocessMsgs = !chatConfig.inputTemplate
      ? preprocessMsgs
      : preprocessMsgs.map((m) => {
          if (m.role === 'user') {
            try {
              return { ...m, content: compiler({ text: m.content }) };
            } catch (error) {
              console.error(error);

              return m;
            }
          }

          return m;
        });

    // 3. add systemRole
    if (agentConfig.systemRole) {
      preprocessMsgs.unshift({ content: agentConfig.systemRole, role: 'system' } as ChatMessage);
    }

    // 4. handle max_tokens
    agentConfig.params.max_tokens = chatConfig.enableMaxTokens
      ? agentConfig.params.max_tokens
      : undefined;

    let isFunctionCall = false;
    let msgTraceId: string | undefined;
    let output = '';

    const historySummary = topicSelectors.currentActiveTopicSummary(get());
    await chatService.createAssistantMessageStream({
      abortController,
      params: {
        messages: preprocessMsgs,
        model: agentConfig.model,
        provider: agentConfig.provider,
        ...agentConfig.params,
        plugins: agentConfig.plugins,
      },
      historySummary: historySummary?.content,
      trace: {
        traceId: params?.traceId,
        sessionId: get().activeId,
        topicId: get().activeTopicId,
        traceName: TraceNameMap.Conversation,
      },
      isWelcomeQuestion: params?.isWelcomeQuestion,
      onErrorHandle: async (error) => {
        await messageService.updateMessageError(assistantId, error);
        await refreshMessages();
      },
      onFinish: async (content, { traceId, observationId, toolCalls }) => {
        // if there is traceId, update it
        if (traceId) {
          msgTraceId = traceId;
          await messageService.updateMessage(assistantId, {
            traceId,
            observationId: observationId ?? undefined,
          });
        }

        if (toolCalls && toolCalls.length > 0) {
          internal_toggleToolCallingStreaming(assistantId, undefined);
        }

        // update the content after fetch result
        await internal_updateMessageContent(assistantId, content, toolCalls);
      },
      onMessageHandle: async (chunk) => {
        switch (chunk.type) {
          case 'text': {
            output += chunk.text;
            internal_dispatchMessage({
              id: assistantId,
              type: 'updateMessage',
              value: { content: output },
            });
            break;
          }

          // is this message is just a tool call
          case 'tool_calls': {
            internal_toggleToolCallingStreaming(assistantId, chunk.isAnimationActives);
            internal_dispatchMessage({
              id: assistantId,
              type: 'updateMessage',
              value: { tools: get().internal_transformToolCalls(chunk.tool_calls) },
            });
            isFunctionCall = true;
          }
        }
      },
    });

    internal_toggleChatLoading(false, assistantId, n('generateMessage(end)') as string);

    return {
      isFunctionCall,
      traceId: msgTraceId,
    };
  },

  internal_resendMessage: async (
    messageId,
    { traceId, messages: outChats, threadId: outThreadId, inPortalThread } = {},
  ) => {
    // 1. 构造所有相关的历史记录
    const chats = outChats ?? chatSelectors.mainAIChats(get());

    const currentIndex = chats.findIndex((c) => c.id === messageId);
    if (currentIndex < 0) return;

    const currentMessage = chats[currentIndex];

    let contextMessages: ChatMessage[] = [];

    switch (currentMessage.role) {
      case 'tool':
      case 'user': {
        contextMessages = chats.slice(0, currentIndex + 1);
        break;
      }
      case 'assistant': {
        // 消息是 AI 发出的因此需要找到它的 user 消息
        const userId = currentMessage.parentId;
        const userIndex = chats.findIndex((c) => c.id === userId);
        // 如果消息没有 parentId，那么同 user/function 模式
        contextMessages = chats.slice(0, userIndex < 0 ? currentIndex + 1 : userIndex + 1);
        break;
      }
    }

    if (contextMessages.length <= 0) return;

    const { internal_coreProcessMessage, activeThreadId } = get();

    const latestMsg = contextMessages.findLast((s) => s.role === 'user');

    if (!latestMsg) return;

    const threadId = outThreadId ?? activeThreadId;

    await internal_coreProcessMessage(contextMessages, latestMsg.id, {
      traceId,
      ragQuery: get().internal_shouldUseRAG() ? latestMsg.content : undefined,
      threadId,
      inPortalThread,
    });
  },

  // ----- Loading ------- //
  internal_toggleChatLoading: (loading, id, action) => {
    return get().internal_toggleLoadingArrays('chatLoadingIds', loading, id, action);
  },
  internal_toggleToolCallingStreaming: (id, streaming) => {
    set(
      {
        toolCallingStreamIds: produce(get().toolCallingStreamIds, (draft) => {
          if (!!streaming) {
            draft[id] = streaming;
          } else {
            delete draft[id];
          }
        }),
      },

      false,
      'toggleToolCallingStreaming',
    );
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/aiChat/actions/rag.ts
================================================================================

import { StateCreator } from 'zustand/vanilla';

import { chainRewriteQuery } from '@/chains/rewriteQuery';
import { chatService } from '@/services/chat';
import { ragService } from '@/services/rag';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/selectors';
import { ChatStore } from '@/store/chat';
import { chatSelectors } from '@/store/chat/selectors';
import { toggleBooleanList } from '@/store/chat/utils';
import { useUserStore } from '@/store/user';
import { systemAgentSelectors } from '@/store/user/selectors';
import { ChatSemanticSearchChunk } from '@/types/chunk';

export interface ChatRAGAction {
  deleteUserMessageRagQuery: (id: string) => Promise<void>;
  /**
   * Retrieve chunks from semantic search
   */
  internal_retrieveChunks: (
    id: string,
    userQuery: string,
    messages: string[],
  ) => Promise<{ chunks: ChatSemanticSearchChunk[]; queryId: string; rewriteQuery?: string }>;
  /**
   * Rewrite user content to better RAG query
   */
  internal_rewriteQuery: (id: string, content: string, messages: string[]) => Promise<string>;

  /**
   * Check if we should use RAG
   */
  internal_shouldUseRAG: () => boolean;
  internal_toggleMessageRAGLoading: (loading: boolean, id: string) => void;
  rewriteQuery: (id: string) => Promise<void>;
}

const knowledgeIds = () => agentSelectors.currentKnowledgeIds(useAgentStore.getState());
const hasEnabledKnowledge = () => agentSelectors.hasEnabledKnowledge(useAgentStore.getState());

export const chatRag: StateCreator<ChatStore, [['zustand/devtools', never]], [], ChatRAGAction> = (
  set,
  get,
) => ({
  deleteUserMessageRagQuery: async (id) => {
    const message = chatSelectors.getMessageById(id)(get());

    if (!message || !message.ragQueryId) return;

    // optimistic update the message's ragQuery
    get().internal_dispatchMessage({
      id,
      type: 'updateMessage',
      value: { ragQuery: null },
    });

    await ragService.deleteMessageRagQuery(message.ragQueryId);
    await get().refreshMessages();
  },

  internal_retrieveChunks: async (id, userQuery, messages) => {
    get().internal_toggleMessageRAGLoading(true, id);

    const message = chatSelectors.getMessageById(id)(get());

    // 1. get the rewrite query
    let rewriteQuery = message?.ragQuery as string | undefined;

    // if there is no ragQuery and there is a chat history
    // we need to rewrite the user message to get better results
    if (!message?.ragQuery && messages.length > 0) {
      rewriteQuery = await get().internal_rewriteQuery(id, userQuery, messages);
    }

    // 2. retrieve chunks from semantic search
    const files = chatSelectors.currentUserFiles(get()).map((f) => f.id);
    const { chunks, queryId } = await ragService.semanticSearchForChat({
      fileIds: knowledgeIds().fileIds.concat(files),
      knowledgeIds: knowledgeIds().knowledgeBaseIds,
      messageId: id,
      rewriteQuery: rewriteQuery || userQuery,
      userQuery,
    });

    get().internal_toggleMessageRAGLoading(false, id);

    return { chunks, queryId, rewriteQuery };
  },
  internal_rewriteQuery: async (id, content, messages) => {
    let rewriteQuery = content;

    const queryRewriteConfig = systemAgentSelectors.queryRewrite(useUserStore.getState());
    if (!queryRewriteConfig.enabled) return content;

    const rewriteQueryParams = {
      model: queryRewriteConfig.model,
      provider: queryRewriteConfig.provider,
      ...chainRewriteQuery(
        content,
        messages,
        !!queryRewriteConfig.customPrompt ? queryRewriteConfig.customPrompt : undefined,
      ),
    };

    let ragQuery = '';
    await chatService.fetchPresetTaskResult({
      onFinish: async (text) => {
        rewriteQuery = text;
      },

      onMessageHandle: (chunk) => {
        if (chunk.type !== 'text') return;
        ragQuery += chunk.text;

        get().internal_dispatchMessage({
          id,
          type: 'updateMessage',
          value: { ragQuery },
        });
      },
      params: rewriteQueryParams,
    });

    return rewriteQuery;
  },
  internal_shouldUseRAG: () => {
    const userFiles = chatSelectors.currentUserFiles(get()).map((f) => f.id);
    //  if there is relative files or enabled knowledge, try with ragQuery
    return hasEnabledKnowledge() || userFiles.length > 0;
  },

  internal_toggleMessageRAGLoading: (loading, id) => {
    set(
      {
        messageRAGLoadingIds: toggleBooleanList(get().messageRAGLoadingIds, id, loading),
      },
      false,
      'internal_toggleMessageLoading',
    );
  },

  rewriteQuery: async (id) => {
    const message = chatSelectors.getMessageById(id)(get());
    if (!message) return;

    // delete the current ragQuery
    await get().deleteUserMessageRagQuery(id);

    const chats = chatSelectors.mainAIChatsWithHistoryConfig(get());

    await get().internal_rewriteQuery(
      id,
      message.content,
      chats.map((m) => m.content),
    );
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/aiChat/actions/index.ts
================================================================================

import { StateCreator } from 'zustand/vanilla';

import { ChatStore } from '@/store/chat/store';

import { AIGenerateAction, generateAIChat } from './generateAIChat';
import { ChatMemoryAction, chatMemory } from './memory';
import { ChatRAGAction, chatRag } from './rag';

export interface ChatAIChatAction extends ChatRAGAction, ChatMemoryAction, AIGenerateAction {
  /**/
}

export const chatAiChat: StateCreator<
  ChatStore,
  [['zustand/devtools', never]],
  [],
  ChatAIChatAction
> = (...params) => ({
  ...chatRag(...params),
  ...generateAIChat(...params),
  ...chatMemory(...params),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/thread/action.ts
================================================================================

/* eslint-disable sort-keys-fix/sort-keys-fix, typescript-sort-keys/interface */
// Disable the auto sort key eslint rule to make the code more logic and readable
import isEqual from 'fast-deep-equal';
import { SWRResponse, mutate } from 'swr';
import { StateCreator } from 'zustand/vanilla';

import { chainSummaryTitle } from '@/chains/summaryTitle';
import { LOADING_FLAT, THREAD_DRAFT_ID } from '@/const/message';
import { isDeprecatedEdition } from '@/const/version';
import { useClientDataSWR } from '@/libs/swr';
import { chatService } from '@/services/chat';
import { threadService } from '@/services/thread';
import { threadSelectors } from '@/store/chat/selectors';
import { ChatStore } from '@/store/chat/store';
import { useSessionStore } from '@/store/session';
import { useUserStore } from '@/store/user';
import { systemAgentSelectors } from '@/store/user/selectors';
import { ChatMessage, CreateMessageParams, SendThreadMessageParams } from '@/types/message';
import { ThreadItem, ThreadType } from '@/types/topic';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

import { ThreadDispatch, threadReducer } from './reducer';

const n = setNamespace('thd');
const SWR_USE_FETCH_THREADS = 'SWR_USE_FETCH_THREADS';

export interface ChatThreadAction {
  // update
  updateThreadInputMessage: (message: string) => void;
  refreshThreads: () => Promise<void>;
  /**
   * Sends a new thread message to the AI chat system
   */
  sendThreadMessage: (params: SendThreadMessageParams) => Promise<void>;
  resendThreadMessage: (messageId: string) => Promise<void>;
  delAndResendThreadMessage: (messageId: string) => Promise<void>;
  createThread: (params: {
    message: CreateMessageParams;
    sourceMessageId: string;
    topicId: string;
    type: ThreadType;
  }) => Promise<{ threadId: string; messageId: string }>;
  openThreadCreator: (messageId: string) => void;
  openThreadInPortal: (threadId: string, sourceMessageId: string) => void;
  closeThreadPortal: () => void;
  useFetchThreads: (enable: boolean, topicId?: string) => SWRResponse<ThreadItem[]>;
  summaryThreadTitle: (threadId: string, messages: ChatMessage[]) => Promise<void>;
  updateThreadTitle: (id: string, title: string) => Promise<void>;
  removeThread: (id: string) => Promise<void>;
  switchThread: (id: string) => void;

  internal_updateThreadTitleInSummary: (id: string, title: string) => void;
  internal_updateThreadLoading: (id: string, loading: boolean) => void;
  internal_updateThread: (id: string, data: Partial<ThreadItem>) => Promise<void>;
  internal_dispatchThread: (payload: ThreadDispatch, action?: any) => void;
}

export const chatThreadMessage: StateCreator<
  ChatStore,
  [['zustand/devtools', never]],
  [],
  ChatThreadAction
> = (set, get) => ({
  updateThreadInputMessage: (message) => {
    if (isEqual(message, get().threadInputMessage)) return;

    set({ threadInputMessage: message }, false, n(`updateThreadInputMessage`, message));
  },

  openThreadCreator: (messageId) => {
    set(
      { threadStartMessageId: messageId, portalThreadId: undefined, startToForkThread: true },
      false,
      'openThreadCreator',
    );
    get().togglePortal(true);
  },
  openThreadInPortal: (threadId, sourceMessageId) => {
    set(
      { portalThreadId: threadId, threadStartMessageId: sourceMessageId, startToForkThread: false },
      false,
      'openThreadInPortal',
    );
    get().togglePortal(true);
  },

  closeThreadPortal: () => {
    set(
      { threadStartMessageId: undefined, portalThreadId: undefined, startToForkThread: undefined },
      false,
      'closeThreadPortal',
    );
    get().togglePortal(false);
  },
  sendThreadMessage: async ({ message }) => {
    const {
      internal_coreProcessMessage,
      activeTopicId,
      activeId,
      threadStartMessageId,
      newThreadMode,
      portalThreadId,
    } = get();
    if (!activeId || !activeTopicId) return;

    // if message is empty or no files, then stop
    if (!message) return;

    set({ isCreatingThreadMessage: true }, false, n('creatingThreadMessage/start'));

    const newMessage: CreateMessageParams = {
      content: message,
      // if message has attached with files, then add files to message and the agent
      // files: fileIdList,
      role: 'user',
      sessionId: activeId,
      // if there is activeTopicId，then add topicId to message
      topicId: activeTopicId,
      threadId: portalThreadId,
    };

    let parentMessageId: string | undefined = undefined;
    let tempMessageId: string | undefined = undefined;

    // if there is no portalThreadId, then create a thread and then append message
    if (!portalThreadId) {
      if (!threadStartMessageId) return;
      // we need to create a temp message for optimistic update
      tempMessageId = get().internal_createTmpMessage({
        ...newMessage,
        threadId: THREAD_DRAFT_ID,
      });
      get().internal_toggleMessageLoading(true, tempMessageId);

      const { threadId, messageId } = await get().createThread({
        message: newMessage,
        sourceMessageId: threadStartMessageId,
        topicId: activeTopicId,
        type: newThreadMode,
      });

      parentMessageId = messageId;

      // mark the portal in thread mode
      await get().refreshThreads();
      await get().refreshMessages();

      get().openThreadInPortal(threadId, threadStartMessageId);
    } else {
      // if there is a thread, just append message
      // we need to create a temp message for optimistic update
      tempMessageId = get().internal_createTmpMessage(newMessage);
      get().internal_toggleMessageLoading(true, tempMessageId);

      parentMessageId = await get().internal_createMessage(newMessage, { tempMessageId });
    }

    get().internal_toggleMessageLoading(false, tempMessageId);

    //  update assistant update to make it rerank
    useSessionStore.getState().triggerSessionUpdate(get().activeId);

    // Get the current messages to generate AI response
    const messages = threadSelectors.portalAIChats(get());

    await internal_coreProcessMessage(messages, parentMessageId, {
      ragQuery: get().internal_shouldUseRAG() ? message : undefined,
      threadId: get().portalThreadId,
      inPortalThread: true,
    });

    set({ isCreatingThreadMessage: false }, false, n('creatingThreadMessage/stop'));

    // 说明是在新建 thread，需要自动总结标题
    if (!portalThreadId) {
      const portalThread = threadSelectors.currentPortalThread(get());

      if (!portalThread) return;

      const chats = threadSelectors.portalAIChats(get());
      await get().summaryThreadTitle(portalThread.id, chats);
    }
  },
  resendThreadMessage: async (messageId) => {
    const chats = threadSelectors.portalAIChats(get());

    await get().internal_resendMessage(messageId, {
      messages: chats,
      threadId: get().portalThreadId,
      inPortalThread: true,
    });
  },
  delAndResendThreadMessage: async (id) => {
    get().resendThreadMessage(id);
    get().deleteMessage(id);
  },
  createThread: async ({ message, sourceMessageId, topicId, type }) => {
    set({ isCreatingThread: true }, false, n('creatingThread/start'));

    const data = await threadService.createThreadWithMessage({
      topicId,
      sourceMessageId,
      type,
      message,
    });
    set({ isCreatingThread: false }, false, n('creatingThread/end'));

    return data;
  },

  useFetchThreads: (enable, topicId) =>
    useClientDataSWR<ThreadItem[]>(
      enable && !!topicId && !isDeprecatedEdition ? [SWR_USE_FETCH_THREADS, topicId] : null,
      async ([, topicId]: [string, string]) => threadService.getThreads(topicId),
      {
        suspense: true,
        fallbackData: [],
        onSuccess: (threads) => {
          const nextMap = { ...get().threadMaps, [topicId!]: threads };

          // no need to update map if the topics have been init and the map is the same
          if (get().topicsInit && isEqual(nextMap, get().topicMaps)) return;

          set(
            { threadMaps: nextMap, threadsInit: true },
            false,
            n('useFetchThreads(success)', { topicId }),
          );
        },
      },
    ),

  refreshThreads: async () => {
    const topicId = get().activeTopicId;
    if (!topicId) return;

    return mutate([SWR_USE_FETCH_THREADS, topicId]);
  },
  removeThread: async (id) => {
    await threadService.removeThread(id);
    await get().refreshThreads();

    if (get().activeThreadId === id) {
      set({ activeThreadId: undefined });
    }
  },
  switchThread: async (id) => {
    set({ activeThreadId: id }, false, n('toggleTopic'));
  },
  updateThreadTitle: async (id, title) => {
    await get().internal_updateThread(id, { title });
  },

  summaryThreadTitle: async (threadId, messages) => {
    const { internal_updateThreadTitleInSummary, internal_updateThreadLoading } = get();
    const portalThread = threadSelectors.currentPortalThread(get());
    if (!portalThread) return;

    internal_updateThreadTitleInSummary(threadId, LOADING_FLAT);

    let output = '';
    const threadConfig = systemAgentSelectors.thread(useUserStore.getState());

    await chatService.fetchPresetTaskResult({
      onError: () => {
        internal_updateThreadTitleInSummary(threadId, portalThread.title);
      },
      onFinish: async (text) => {
        await get().internal_updateThread(threadId, { title: text });
      },
      onLoadingChange: (loading) => {
        internal_updateThreadLoading(threadId, loading);
      },
      onMessageHandle: (chunk) => {
        switch (chunk.type) {
          case 'text': {
            output += chunk.text;
          }
        }

        internal_updateThreadTitleInSummary(threadId, output);
      },
      params: merge(threadConfig, chainSummaryTitle(messages)),
    });
  },

  // Internal process method of the topics
  internal_updateThreadTitleInSummary: (id, title) => {
    get().internal_dispatchThread(
      { type: 'updateThread', id, value: { title } },
      'updateThreadTitleInSummary',
    );
  },

  internal_updateThreadLoading: (id, loading) => {
    set(
      (state) => {
        if (loading) return { threadLoadingIds: [...state.threadLoadingIds, id] };

        return { threadLoadingIds: state.threadLoadingIds.filter((i) => i !== id) };
      },
      false,
      n('updateThreadLoading'),
    );
  },

  internal_updateThread: async (id, data) => {
    get().internal_dispatchThread({ type: 'updateThread', id, value: data });

    get().internal_updateThreadLoading(id, true);
    await threadService.updateThread(id, data);
    await get().refreshThreads();
    get().internal_updateThreadLoading(id, false);
  },

  internal_dispatchThread: (payload, action) => {
    const nextThreads = threadReducer(threadSelectors.currentTopicThreads(get()), payload);
    const nextMap = { ...get().threadMaps, [get().activeTopicId!]: nextThreads };

    // no need to update map if is the same
    if (isEqual(nextMap, get().threadMaps)) return;

    set({ threadMaps: nextMap }, false, action ?? n(`dispatchThread/${payload.type}`));
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/chat/slices/thread/selectors/index.ts
================================================================================

import { THREAD_DRAFT_ID } from '@/const/message';
import { useAgentStore } from '@/store/agent';
import { agentSelectors } from '@/store/agent/selectors';
import type { ChatStoreState } from '@/store/chat';
import { chatHelpers } from '@/store/chat/helpers';
import { ChatMessage } from '@/types/message';
import { ThreadItem } from '@/types/topic';

import { chatSelectors } from '../../message/selectors';
import { genMessage } from './util';

const currentTopicThreads = (s: ChatStoreState) => {
  if (!s.activeTopicId) return [];

  return s.threadMaps[s.activeTopicId] || [];
};

const currentPortalThread = (s: ChatStoreState): ThreadItem | undefined => {
  if (!s.portalThreadId) return undefined;

  const threads = currentTopicThreads(s);

  return threads.find((t) => t.id === s.portalThreadId);
};

const threadStartMessageId = (s: ChatStoreState) => s.threadStartMessageId;

const threadSourceMessageId = (s: ChatStoreState) => {
  if (s.startToForkThread) return threadStartMessageId(s);

  const portalThread = currentPortalThread(s);
  return portalThread?.sourceMessageId;
};

const getTheadParentMessages = (s: ChatStoreState, data: ChatMessage[]) => {
  if (s.startToForkThread) {
    const startMessageId = threadStartMessageId(s)!;

    // 存在 threadId 的消息是子消息，在创建付消息时需要忽略
    const messages = data.filter((m) => !m.threadId);
    return genMessage(messages, startMessageId, s.newThreadMode);
  }

  const portalThread = currentPortalThread(s);
  return genMessage(data, portalThread?.sourceMessageId, portalThread?.type);
};

// ======= Portal Thread Display Chats ======= //
// =========================================== //

/**
 * 获取当前 thread 的父级消息
 */
const portalDisplayParentMessages = (s: ChatStoreState): ChatMessage[] => {
  const data = chatSelectors.activeBaseChatsWithoutTool(s);

  return getTheadParentMessages(s, data);
};

/**
 * these messages are the messages that are in the thread
 *
 */
const portalDisplayChildChatsByThreadId =
  (id?: string) =>
  (s: ChatStoreState): ChatMessage[] => {
    // skip tool message
    const data = chatSelectors.activeBaseChatsWithoutTool(s);

    return data.filter((m) => !!id && m.threadId === id);
  };

const portalDisplayChats = (s: ChatStoreState) => {
  const parentMessages = portalDisplayParentMessages(s);
  const afterMessages = portalDisplayChildChatsByThreadId(s.portalThreadId)(s);
  // use for optimistic update
  const draftMessage = chatSelectors.activeBaseChats(s).find((m) => m.threadId === THREAD_DRAFT_ID);

  return [...parentMessages, draftMessage, ...afterMessages].filter(Boolean) as ChatMessage[];
};

const portalDisplayChatsLength = (s: ChatStoreState) => {
  // history length include a thread divider
  return portalDisplayChats(s).length;
};
const portalDisplayChatsString = (s: ChatStoreState) => {
  const messages = portalDisplayChats(s);

  return messages.map((m) => m.content).join('');
};

const portalDisplayChatIDs = (s: ChatStoreState): string[] =>
  portalDisplayChats(s).map((i) => i.id);

// ========= Portal Thread AI Chats ========= //
// ========================================== //

const portalAIParentMessages = (s: ChatStoreState): ChatMessage[] => {
  const data = chatSelectors.activeBaseChats(s);

  return getTheadParentMessages(s, data);
};

const portalAIChildChatsByThreadId =
  (id?: string) =>
  (s: ChatStoreState): ChatMessage[] => {
    // skip tool message
    const data = chatSelectors.activeBaseChats(s);

    return data.filter((m) => !!id && m.threadId === id);
  };

const portalAIChats = (s: ChatStoreState) => {
  const parentMessages = portalAIParentMessages(s);
  const afterMessages = portalAIChildChatsByThreadId(s.portalThreadId)(s);

  return [...parentMessages, ...afterMessages].filter(Boolean) as ChatMessage[];
};

const portalAIChatsWithHistoryConfig = (s: ChatStoreState) => {
  const parentMessages = portalAIParentMessages(s);
  const afterMessages = portalAIChildChatsByThreadId(s.portalThreadId)(s);

  const messages = [...parentMessages, ...afterMessages].filter(Boolean) as ChatMessage[];

  const config = agentSelectors.currentAgentChatConfig(useAgentStore.getState());

  return chatHelpers.getSlicedMessagesWithConfig(messages, config);
};

const threadSourceMessageIndex = (s: ChatStoreState) => {
  const theadMessageId = threadSourceMessageId(s);
  const data = portalDisplayChats(s);

  return !theadMessageId ? -1 : data.findIndex((d) => d.id === theadMessageId);
};
const getThreadsByTopic = (topicId?: string) => (s: ChatStoreState) => {
  if (!topicId) return;

  return s.threadMaps[topicId];
};

const getFirstThreadBySourceMsgId = (id: string) => (s: ChatStoreState) => {
  const threads = currentTopicThreads(s);

  return threads.find((t) => t.sourceMessageId === id);
};

const getThreadsBySourceMsgId = (id: string) => (s: ChatStoreState) => {
  const threads = currentTopicThreads(s);

  return threads.filter((t) => t.sourceMessageId === id);
};

const hasThreadBySourceMsgId = (id: string) => (s: ChatStoreState) => {
  const threads = currentTopicThreads(s);

  return threads.some((t) => t.sourceMessageId === id);
};

const isThreadAIGenerating = (s: ChatStoreState) =>
  s.chatLoadingIds.some((id) => portalDisplayChatIDs(s).includes(id));

const isInRAGFlow = (s: ChatStoreState) =>
  s.messageRAGLoadingIds.some((id) => portalDisplayChatIDs(s).includes(id));
const isCreatingMessage = (s: ChatStoreState) => s.isCreatingThreadMessage;
const isHasMessageLoading = (s: ChatStoreState) =>
  s.messageLoadingIds.some((id) => portalDisplayChatIDs(s).includes(id));

/**
 * this function is used to determine whether the send button should be disabled
 */
const isSendButtonDisabledByMessage = (s: ChatStoreState) =>
  // 1. when there is message loading
  isHasMessageLoading(s) ||
  // 2. when is creating the topic
  s.isCreatingThread ||
  // 3. when is creating the message
  isCreatingMessage(s) ||
  // 4. when the message is in RAG flow
  isInRAGFlow(s);

export const threadSelectors = {
  currentPortalThread,
  currentTopicThreads,
  getFirstThreadBySourceMsgId,
  getThreadsBySourceMsgId,
  getThreadsByTopic,
  hasThreadBySourceMsgId,
  isSendButtonDisabledByMessage,
  isThreadAIGenerating,
  portalAIChats,
  portalAIChatsWithHistoryConfig,
  portalDisplayChatIDs,
  portalDisplayChats,
  portalDisplayChatsLength,
  portalDisplayChatsString,
  portalDisplayChildChatsByThreadId,
  threadSourceMessageId,
  threadSourceMessageIndex,
  threadStartMessageId,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/initialState.ts
================================================================================

import { ImageFileState, initialImageFileState } from './slices/chat';
import { FileChunkState, initialFileChunkState } from './slices/chunk';
import { FileManagerState, initialFileManagerState } from './slices/fileManager';

export type FilesStoreState = ImageFileState & FileManagerState & FileChunkState;

export const initialState: FilesStoreState = {
  ...initialImageFileState,
  ...initialFileManagerState,
  ...initialFileChunkState,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/store.ts
================================================================================

import { shallow } from 'zustand/shallow';
import { createWithEqualityFn } from 'zustand/traditional';
import { StateCreator } from 'zustand/vanilla';

import { createDevtools } from '../middleware/createDevtools';
import { FilesStoreState, initialState } from './initialState';
import { FileAction, createFileSlice } from './slices/chat';
import { FileChunkAction, createFileChunkSlice } from './slices/chunk';
import { FileManageAction, createFileManageSlice } from './slices/fileManager';
import { TTSFileAction, createTTSFileSlice } from './slices/tts';
import { FileUploadAction, createFileUploadSlice } from './slices/upload/action';

//  ===============  聚合 createStoreFn ============ //

export type FileStore = FilesStoreState &
  FileAction &
  TTSFileAction &
  FileManageAction &
  FileChunkAction &
  FileUploadAction;

const createStore: StateCreator<FileStore, [['zustand/devtools', never]]> = (...parameters) => ({
  ...initialState,
  ...createFileSlice(...parameters),
  ...createFileManageSlice(...parameters),
  ...createTTSFileSlice(...parameters),
  ...createFileChunkSlice(...parameters),
  ...createFileUploadSlice(...parameters),
});

//  ===============  实装 useStore ============ //
const devtools = createDevtools('file');

export const useFileStore = createWithEqualityFn<FileStore>()(devtools(createStore), shallow);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/chunk/initialState.ts
================================================================================

import { SemanticSearchChunk } from '@/types/chunk';

export interface FileChunkState {
  chunkDetailId: string | null;
  highlightChunkIds: string[];
  isSimilaritySearch?: boolean;
  isSimilaritySearching?: boolean;
  similaritySearchChunks?: SemanticSearchChunk[];
}

export const initialFileChunkState: FileChunkState = {
  chunkDetailId: null,
  highlightChunkIds: [],
  similaritySearchChunks: [],
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/chunk/action.ts
================================================================================

import { StateCreator } from 'zustand/vanilla';

import { ragService } from '@/services/rag';

import { FileStore } from '../../store';

export interface FileChunkAction {
  closeChunkDrawer: () => void;
  highlightChunks: (ids: string[]) => void;

  openChunkDrawer: (id: string) => void;
  semanticSearch: (text: string, fileId: string) => Promise<void>;
}

export const createFileChunkSlice: StateCreator<
  FileStore,
  [['zustand/devtools', never]],
  [],
  FileChunkAction
> = (set) => ({
  closeChunkDrawer: () => {
    set({ chunkDetailId: null, isSimilaritySearch: false, similaritySearchChunks: [] });
  },
  highlightChunks: (ids) => {
    set({ highlightChunkIds: ids });
  },
  openChunkDrawer: (id) => {
    set({ chunkDetailId: id });
  },

  semanticSearch: async (text, fileId) => {
    set({ isSimilaritySearching: true });
    const data = await ragService.semanticSearch(text, [fileId]);
    set({ isSimilaritySearching: false, similaritySearchChunks: data });
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/chunk/selectors.ts
================================================================================

// import { FileStore } from '../../store';
import { FilesStoreState } from '@/store/file/initialState';

const showSimilaritySearchResult = (s: FilesStoreState) => s.isSimilaritySearch;
const enabledChunkFileId = (s: FilesStoreState) => s.chunkDetailId;

export const fileChunkSelectors = {
  enabledChunkFileId,
  showSimilaritySearchResult,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/chat/action.ts
================================================================================

import { t } from 'i18next';
import { StateCreator } from 'zustand/vanilla';

import { notification } from '@/components/AntdStaticMethods';
import { FILE_UPLOAD_BLACKLIST } from '@/const/file';
import { fileService } from '@/services/file';
import { ServerService } from '@/services/file/server';
import { ragService } from '@/services/rag';
import { UPLOAD_NETWORK_ERROR } from '@/services/upload';
import { userService } from '@/services/user';
import { useAgentStore } from '@/store/agent';
import {
  UploadFileListDispatch,
  uploadFileListReducer,
} from '@/store/file/reducers/uploadFileList';
import { useUserStore } from '@/store/user';
import { preferenceSelectors } from '@/store/user/selectors';
import { FileListItem } from '@/types/files';
import { UploadFileItem } from '@/types/files/upload';
import { isChunkingUnsupported } from '@/utils/isChunkingUnsupported';
import { sleep } from '@/utils/sleep';
import { setNamespace } from '@/utils/storeDebug';

import { FileStore } from '../../store';

const n = setNamespace('chat');

const serverFileService = new ServerService();

export interface FileAction {
  clearChatUploadFileList: () => void;
  dispatchChatUploadFileList: (payload: UploadFileListDispatch) => void;

  removeChatUploadFile: (id: string) => Promise<void>;
  startAsyncTask: (
    fileId: string,
    runner: (id: string) => Promise<string>,
    onFileItemChange: (fileItem: FileListItem) => void,
  ) => Promise<void>;

  uploadChatFiles: (files: File[]) => Promise<void>;
}

export const createFileSlice: StateCreator<
  FileStore,
  [['zustand/devtools', never]],
  [],
  FileAction
> = (set, get) => ({
  clearChatUploadFileList: () => {
    set({ chatUploadFileList: [] }, false, n('clearChatUploadFileList'));
  },
  dispatchChatUploadFileList: (payload) => {
    const nextValue = uploadFileListReducer(get().chatUploadFileList, payload);
    if (nextValue === get().chatUploadFileList) return;

    set({ chatUploadFileList: nextValue }, false, `dispatchChatFileList/${payload.type}`);
  },
  removeChatUploadFile: async (id) => {
    const { dispatchChatUploadFileList } = get();

    dispatchChatUploadFileList({ id, type: 'removeFile' });
    await fileService.removeFile(id);
  },

  startAsyncTask: async (id, runner, onFileItemUpdate) => {
    await runner(id);

    let isFinished = false;

    while (!isFinished) {
      // 每间隔 2s 查询一次任务状态
      await sleep(2000);

      let fileItem: FileListItem | undefined = undefined;

      try {
        fileItem = await serverFileService.getFileItem(id);
      } catch (e) {
        console.error('getFileItem Error:', e);
        continue;
      }

      if (!fileItem) return;

      onFileItemUpdate(fileItem);

      if (fileItem.finishEmbedding) {
        isFinished = true;
      }

      // if error, also break
      else if (fileItem.chunkingStatus === 'error' || fileItem.embeddingStatus === 'error') {
        isFinished = true;
      }
    }
  },

  uploadChatFiles: async (rawFiles) => {
    const { dispatchChatUploadFileList, startAsyncTask } = get();
    // 0. skip file in blacklist
    const files = rawFiles.filter((file) => !FILE_UPLOAD_BLACKLIST.includes(file.name));
    // 1. add files with base64
    const uploadFiles: UploadFileItem[] = await Promise.all(
      files.map(async (file) => {
        let previewUrl: string | undefined = undefined;
        let base64Url: string | undefined = undefined;

        // only image and video can be previewed, we create a previewUrl and base64Url for them
        if (file.type.startsWith('image') || file.type.startsWith('video')) {
          const data = await file.arrayBuffer();

          previewUrl = URL.createObjectURL(new Blob([data!], { type: file.type }));

          const base64 = Buffer.from(data!).toString('base64');
          base64Url = `data:${file.type};base64,${base64}`;
        }

        return { base64Url, file, id: file.name, previewUrl, status: 'pending' } as UploadFileItem;
      }),
    );

    dispatchChatUploadFileList({ files: uploadFiles, type: 'addFiles' });

    // upload files and process it
    const pools = files.map(async (file) => {
      let fileResult: { id: string; url: string } | undefined;

      try {
        fileResult = await get().uploadWithProgress({
          file,
          onStatusUpdate: dispatchChatUploadFileList,
        });
      } catch (error) {
        // skip `UNAUTHORIZED` error
        if ((error as any)?.message !== 'UNAUTHORIZED')
          notification.error({
            description:
              // it may be a network error or the cors error
              error === UPLOAD_NETWORK_ERROR
                ? t('upload.networkError', { ns: 'error' })
                : // or the error from the server
                  typeof error === 'string'
                  ? error
                  : t('upload.unknownError', { ns: 'error', reason: (error as Error).message }),
            message: t('upload.uploadFailed', { ns: 'error' }),
          });

        dispatchChatUploadFileList({ id: file.name, type: 'removeFile' });
      }

      if (!fileResult) return;

      // image don't need to be chunked and embedding
      if (isChunkingUnsupported(file.type)) return;

      // 3. auto chunk and embedding
      dispatchChatUploadFileList({
        id: fileResult.id,
        type: 'updateFile',
        // make the taks empty to hint the user that the task is starting but not triggered
        value: { tasks: {} },
      });

      await startAsyncTask(
        fileResult.id,
        async (id) => {
          const data = await ragService.createParseFileTask(id);
          if (!data || !data.id) throw new Error('failed to createParseFileTask');

          // run the assignment
          useAgentStore
            .getState()
            .addFilesToAgent([id], false)
            .then(() => {
              // trigger the tip if it's the first time
              if (!preferenceSelectors.shouldTriggerFileInKnowledgeBaseTip(useUserStore.getState()))
                return;

              userService.updateGuide({ uploadFileInKnowledgeBase: true });
            });

          return data.id;
        },

        (fileItem) => {
          dispatchChatUploadFileList({
            id: fileResult.id,
            type: 'updateFile',
            value: {
              tasks: {
                chunkCount: fileItem.chunkCount,
                chunkingError: fileItem.chunkingError,
                chunkingStatus: fileItem.chunkingStatus,
                embeddingError: fileItem.embeddingError,
                embeddingStatus: fileItem.embeddingStatus,
                finishEmbedding: fileItem.finishEmbedding,
              },
            },
          });
        },
      );
    });

    await Promise.all(pools);
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/fileManager/initialState.ts
================================================================================

import { FileListItem, QueryFileListParams } from '@/types/files';
import { UploadFileItem } from '@/types/files/upload';

export interface FileManagerState {
  creatingChunkingTaskIds: string[];
  creatingEmbeddingTaskIds: string[];
  dockUploadFileList: UploadFileItem[];
  fileDetail?: FileListItem;
  fileList: FileListItem[];
  queryListParams?: QueryFileListParams;
}

export const initialFileManagerState: FileManagerState = {
  creatingChunkingTaskIds: [],
  creatingEmbeddingTaskIds: [],
  dockUploadFileList: [],
  fileList: [],
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/fileManager/action.ts
================================================================================

import { SWRResponse, mutate } from 'swr';
import { StateCreator } from 'zustand/vanilla';

import { FILE_UPLOAD_BLACKLIST } from '@/const/file';
import { useClientDataSWR } from '@/libs/swr';
import { fileService } from '@/services/file';
import { ServerService } from '@/services/file/server';
import { ragService } from '@/services/rag';
import {
  UploadFileListDispatch,
  uploadFileListReducer,
} from '@/store/file/reducers/uploadFileList';
import { FileListItem, QueryFileListParams } from '@/types/files';

import { FileStore } from '../../store';
import { fileManagerSelectors } from './selectors';

const serverFileService = new ServerService();

export interface FileManageAction {
  dispatchDockFileList: (payload: UploadFileListDispatch) => void;
  embeddingChunks: (fileIds: string[]) => Promise<void>;
  parseFilesToChunks: (ids: string[], params?: { skipExist?: boolean }) => Promise<void>;
  pushDockFileList: (files: File[], knowledgeBaseId?: string) => Promise<void>;

  reEmbeddingChunks: (id: string) => Promise<void>;
  reParseFile: (id: string) => Promise<void>;
  refreshFileList: () => Promise<void>;
  removeAllFiles: () => Promise<void>;
  removeFileItem: (id: string) => Promise<void>;
  removeFiles: (ids: string[]) => Promise<void>;

  toggleEmbeddingIds: (ids: string[], loading?: boolean) => void;
  toggleParsingIds: (ids: string[], loading?: boolean) => void;

  useFetchFileItem: (id?: string) => SWRResponse<FileListItem | undefined>;
  useFetchFileManage: (params: QueryFileListParams) => SWRResponse<FileListItem[]>;
}

const FETCH_FILE_LIST_KEY = 'useFetchFileManage';

export const createFileManageSlice: StateCreator<
  FileStore,
  [['zustand/devtools', never]],
  [],
  FileManageAction
> = (set, get) => ({
  dispatchDockFileList: (payload: UploadFileListDispatch) => {
    const nextValue = uploadFileListReducer(get().dockUploadFileList, payload);
    if (nextValue === get().dockUploadFileList) return;

    set({ dockUploadFileList: nextValue }, false, `dispatchDockFileList/${payload.type}`);
  },
  embeddingChunks: async (fileIds: string[]) => {
    // toggle file ids
    get().toggleEmbeddingIds(fileIds);

    // parse files
    const pools = fileIds.map(async (id) => {
      try {
        await ragService.createEmbeddingChunksTask(id);
      } catch (e) {
        console.error(e);
      }
    });

    await Promise.all(pools);
    await get().refreshFileList();
    get().toggleEmbeddingIds(fileIds, false);
  },
  parseFilesToChunks: async (ids: string[], params) => {
    // toggle file ids
    get().toggleParsingIds(ids);

    // parse files
    const pools = ids.map(async (id) => {
      try {
        await ragService.createParseFileTask(id, params?.skipExist);
      } catch (e) {
        console.error(e);
      }
    });

    await Promise.all(pools);
    await get().refreshFileList();
    get().toggleParsingIds(ids, false);
  },
  pushDockFileList: async (rawFiles, knowledgeBaseId) => {
    const { dispatchDockFileList } = get();

    // 0. skip file in blacklist
    const files = rawFiles.filter((file) => !FILE_UPLOAD_BLACKLIST.includes(file.name));

    // 1. add files
    dispatchDockFileList({
      atStart: true,
      files: files.map((file) => ({ file, id: file.name, status: 'pending' })),
      type: 'addFiles',
    });

    const pools = files.map(async (file) => {
      await get().uploadWithProgress({
        file,
        knowledgeBaseId,
        onStatusUpdate: dispatchDockFileList,
      });

      await get().refreshFileList();
    });

    await Promise.all(pools);
  },

  reEmbeddingChunks: async (id) => {
    if (fileManagerSelectors.isCreatingChunkEmbeddingTask(id)(get())) return;

    // toggle file ids
    get().toggleEmbeddingIds([id]);

    await serverFileService.removeFileAsyncTask(id, 'embedding');

    await get().refreshFileList();

    await ragService.createEmbeddingChunksTask(id);

    await get().refreshFileList();

    get().toggleEmbeddingIds([id], false);
  },
  reParseFile: async (id) => {
    // toggle file ids
    get().toggleParsingIds([id]);

    await ragService.retryParseFile(id);

    await get().refreshFileList();

    get().toggleParsingIds([id], false);
  },
  refreshFileList: async () => {
    await mutate([FETCH_FILE_LIST_KEY, get().queryListParams]);
  },
  removeAllFiles: async () => {
    await fileService.removeAllFiles();
  },
  removeFileItem: async (id) => {
    await fileService.removeFile(id);
    await get().refreshFileList();
  },

  removeFiles: async (ids) => {
    await fileService.removeFiles(ids);
    await get().refreshFileList();
  },
  toggleEmbeddingIds: (ids, loading) => {
    set((state) => {
      const nextValue = new Set(state.creatingEmbeddingTaskIds);

      ids.forEach((id) => {
        if (typeof loading === 'undefined') {
          if (nextValue.has(id)) nextValue.delete(id);
          else nextValue.add(id);
        } else {
          if (loading) nextValue.add(id);
          else nextValue.delete(id);
        }
      });

      return { creatingEmbeddingTaskIds: Array.from(nextValue.values()) };
    });
  },
  toggleParsingIds: (ids, loading) => {
    set((state) => {
      const nextValue = new Set(state.creatingChunkingTaskIds);

      ids.forEach((id) => {
        if (typeof loading === 'undefined') {
          if (nextValue.has(id)) nextValue.delete(id);
          else nextValue.add(id);
        } else {
          if (loading) nextValue.add(id);
          else nextValue.delete(id);
        }
      });

      return { creatingChunkingTaskIds: Array.from(nextValue.values()) };
    });
  },

  useFetchFileItem: (id) =>
    useClientDataSWR<FileListItem | undefined>(!id ? null : ['useFetchFileItem', id], () =>
      serverFileService.getFileItem(id!),
    ),

  useFetchFileManage: (params) =>
    useClientDataSWR<FileListItem[]>(
      [FETCH_FILE_LIST_KEY, params],
      () => serverFileService.getFiles(params),
      {
        onSuccess: (data) => {
          set({ fileList: data, queryListParams: params });
        },
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/file/slices/fileManager/selectors.ts
================================================================================

// import { FileStore } from '../../store';
import { FilesStoreState } from '@/store/file/initialState';
import { FileUploadStatus } from '@/types/files/upload';

const uploadStatusArray = new Set(['uploading', 'pending', 'processing']);

const dockFileList = (s: FilesStoreState) => s.dockUploadFileList;
const dockRawFileList = (s: FilesStoreState) => s.dockUploadFileList.map((item) => item.file);
const getFileById = (id?: string | null) => (s: FilesStoreState) => {
  if (!id) return;

  return s.fileList.find((item) => item.id === id);
};

const isUploadingFiles = (s: FilesStoreState) =>
  s.dockUploadFileList.some((file) => uploadStatusArray.has(file.status));

const overviewUploadingStatus = (s: FilesStoreState): FileUploadStatus => {
  if (s.dockUploadFileList.length === 0) return 'pending';
  if (s.dockUploadFileList.some((file) => uploadStatusArray.has(file.status))) {
    return 'uploading';
  }

  return 'success';
};

const overviewUploadingProgress = (s: FilesStoreState) => {
  const uploadFiles = s.dockUploadFileList.filter(
    (file) => file.status === 'uploading' || file.status === 'pending',
  );

  if (uploadFiles.length === 0) return 100;

  const totalPercent = uploadFiles.length * 100;
  const currentPercent = uploadFiles.reduce(
    (acc, file) => acc + (file.uploadState?.progress || 0),
    0,
  );

  return (currentPercent / totalPercent) * 100;
};

const isCreatingFileParseTask = (id: string) => (s: FilesStoreState) =>
  s.creatingChunkingTaskIds.includes(id);

const isCreatingChunkEmbeddingTask = (id: string) => (s: FilesStoreState) =>
  s.creatingEmbeddingTaskIds.includes(id);

export const fileManagerSelectors = {
  dockFileList,
  dockRawFileList,
  getFileById,
  isCreatingChunkEmbeddingTask,
  isCreatingFileParseTask,
  isUploadingFiles,
  overviewUploadingProgress,
  overviewUploadingStatus,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/settings/selectors/settings.ts
================================================================================

import { DEFAULT_AGENT_META } from '@/const/meta';
import {
  DEFAULT_AGENT,
  DEFAULT_AGENT_CONFIG,
  DEFAULT_SYSTEM_AGENT_CONFIG,
  DEFAULT_TTS_CONFIG,
} from '@/const/settings';
import {
  GlobalLLMProviderKey,
  ProviderConfig,
  UserModelProviderConfig,
  UserSettings,
} from '@/types/user/settings';
import { merge } from '@/utils/merge';

import { UserStore } from '../../../store';

export const currentSettings = (s: UserStore): UserSettings => merge(s.defaultSettings, s.settings);

export const currentLLMSettings = (s: UserStore): UserModelProviderConfig =>
  currentSettings(s).languageModel || {};

export const getProviderConfigById = (provider: string) => (s: UserStore) =>
  currentLLMSettings(s)[provider as GlobalLLMProviderKey] as ProviderConfig | undefined;

const currentTTS = (s: UserStore) => merge(DEFAULT_TTS_CONFIG, currentSettings(s).tts);

const defaultAgent = (s: UserStore) => merge(DEFAULT_AGENT, currentSettings(s).defaultAgent);
const defaultAgentConfig = (s: UserStore) => merge(DEFAULT_AGENT_CONFIG, defaultAgent(s).config);

const defaultAgentMeta = (s: UserStore) => merge(DEFAULT_AGENT_META, defaultAgent(s).meta);

const exportSettings = currentSettings;

const dalleConfig = (s: UserStore) => currentSettings(s).tool?.dalle || {};
const isDalleAutoGenerating = (s: UserStore) => currentSettings(s).tool?.dalle?.autoGenerate;

const currentSystemAgent = (s: UserStore) =>
  merge(DEFAULT_SYSTEM_AGENT_CONFIG, currentSettings(s).systemAgent);

export const settingsSelectors = {
  currentSettings,
  currentSystemAgent,
  currentTTS,
  dalleConfig,
  defaultAgent,
  defaultAgentConfig,
  defaultAgentMeta,
  exportSettings,
  isDalleAutoGenerating,
  providerConfig: getProviderConfigById,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/preference/initialState.ts
================================================================================

import { DEFAULT_PREFERENCE } from '@/const/user';
import { UserPreference } from '@/types/user';

export interface UserPreferenceState {
  /**
   * the user preference, which only store in local storage
   */
  preference: UserPreference;
}

export const initialPreferenceState: UserPreferenceState = {
  preference: DEFAULT_PREFERENCE,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/common/action.ts
================================================================================

import useSWR, { SWRResponse, mutate } from 'swr';
import { DeepPartial } from 'utility-types';
import type { StateCreator } from 'zustand/vanilla';

import { DEFAULT_PREFERENCE } from '@/const/user';
import { useOnlyFetchOnceSWR } from '@/libs/swr';
import { userService } from '@/services/user';
import type { UserStore } from '@/store/user';
import type { GlobalServerConfig } from '@/types/serverConfig';
import { UserInitializationState } from '@/types/user';
import type { UserSettings } from '@/types/user/settings';
import { switchLang } from '@/utils/client/switchLang';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

import { preferenceSelectors } from '../preference/selectors';
import { userGeneralSettingsSelectors } from '../settings/selectors';

const n = setNamespace('common');

const GET_USER_STATE_KEY = 'initUserState';
/**
 * 设置操作
 */
export interface CommonAction {
  refreshUserState: () => Promise<void>;

  updateAvatar: (avatar: string) => Promise<void>;
  useCheckTrace: (shouldFetch: boolean) => SWRResponse;
  useInitUserState: (
    isLogin: boolean | undefined,
    serverConfig: GlobalServerConfig,
    options?: {
      onSuccess: (data: UserInitializationState) => void;
    },
  ) => SWRResponse;
}

export const createCommonSlice: StateCreator<
  UserStore,
  [['zustand/devtools', never]],
  [],
  CommonAction
> = (set, get) => ({
  refreshUserState: async () => {
    await mutate(GET_USER_STATE_KEY);
  },
  updateAvatar: async (avatar) => {
    const { userClientService } = await import('@/services/user');

    await userClientService.updateAvatar(avatar);
    await get().refreshUserState();
  },

  useCheckTrace: (shouldFetch) =>
    useSWR<boolean>(
      shouldFetch ? 'checkTrace' : null,
      () => {
        const userAllowTrace = preferenceSelectors.userAllowTrace(get());

        // if user have set the trace, return false
        if (typeof userAllowTrace === 'boolean') return Promise.resolve(false);

        return Promise.resolve(get().isUserCanEnableTrace);
      },
      {
        revalidateOnFocus: false,
      },
    ),

  useInitUserState: (isLogin, serverConfig, options) =>
    useOnlyFetchOnceSWR<UserInitializationState>(
      !!isLogin ? GET_USER_STATE_KEY : null,
      () => userService.getUserState(),
      {
        onSuccess: (data) => {
          options?.onSuccess?.(data);

          if (data) {
            // merge settings
            const serverSettings: DeepPartial<UserSettings> = {
              defaultAgent: serverConfig.defaultAgent,
              languageModel: serverConfig.languageModel,
              systemAgent: serverConfig.systemAgent,
            };

            const defaultSettings = merge(get().defaultSettings, serverSettings);

            // merge preference
            const isEmpty = Object.keys(data.preference || {}).length === 0;
            const preference = isEmpty ? DEFAULT_PREFERENCE : data.preference;

            // if there is avatar or userId (from client DB), update it into user
            const user =
              data.avatar || data.userId
                ? merge(get().user, { avatar: data.avatar, id: data.userId })
                : get().user;

            set(
              {
                defaultSettings,
                enabledNextAuth: serverConfig.enabledOAuthSSO,
                isOnboard: data.isOnboard,
                isShowPWAGuide: data.canEnablePWAGuide,
                isUserCanEnableTrace: data.canEnableTrace,
                isUserHasConversation: data.hasConversation,
                isUserStateInit: true,
                preference,
                serverLanguageModel: serverConfig.languageModel,
                settings: data.settings || {},
                user,
              },
              false,
              n('initUserState'),
            );

            get().refreshDefaultModelProviderList({ trigger: 'fetchUserState' });

            // auto switch language
            const language = userGeneralSettingsSelectors.config(get()).language;
            if (language === 'auto') {
              switchLang('auto');
            }
          }
        },
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/modelList/initialState.ts
================================================================================

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';
import { ModelProviderCard } from '@/types/llm';
import { ServerLanguageModel } from '@/types/serverConfig';

export interface ModelListState {
  defaultModelProviderList: ModelProviderCard[];
  editingCustomCardModel?: { id: string; provider: string } | undefined;
  modelProviderList: ModelProviderCard[];
  serverLanguageModel?: ServerLanguageModel;
}

export const initialModelListState: ModelListState = {
  defaultModelProviderList: DEFAULT_MODEL_PROVIDER_LIST,
  modelProviderList: DEFAULT_MODEL_PROVIDER_LIST,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/modelList/action.ts
================================================================================

import { produce } from 'immer';
import useSWR, { SWRResponse } from 'swr';
import type { StateCreator } from 'zustand/vanilla';

import { DEFAULT_MODEL_PROVIDER_LIST } from '@/config/modelProviders';
import { ModelProvider } from '@/libs/agent-runtime';
import { UserStore } from '@/store/user';
import type { ChatModelCard, ModelProviderCard } from '@/types/llm';
import type {
  GlobalLLMProviderKey,
  UserKeyVaults,
  UserModelProviderConfig,
} from '@/types/user/settings';

import { settingsSelectors } from '../settings/selectors';
import { CustomModelCardDispatch, customModelCardsReducer } from './reducers/customModelCard';
import { modelProviderSelectors } from './selectors/modelProvider';

/**
 * 设置操作
 */
export interface ModelListAction {
  clearObtainedModels: (provider: GlobalLLMProviderKey) => Promise<void>;
  dispatchCustomModelCards: (
    provider: GlobalLLMProviderKey,
    payload: CustomModelCardDispatch,
  ) => Promise<void>;
  /**
   * make sure the default model provider list is sync to latest state
   */
  refreshDefaultModelProviderList: (params?: { trigger?: string }) => void;
  refreshModelProviderList: (params?: { trigger?: string }) => void;
  removeEnabledModels: (provider: GlobalLLMProviderKey, model: string) => Promise<void>;
  setModelProviderConfig: <T extends GlobalLLMProviderKey>(
    provider: T,
    config: Partial<UserModelProviderConfig[T]>,
  ) => Promise<void>;
  toggleEditingCustomModelCard: (params?: { id: string; provider: GlobalLLMProviderKey }) => void;

  toggleProviderEnabled: (provider: GlobalLLMProviderKey, enabled: boolean) => Promise<void>;

  updateEnabledModels: (
    provider: GlobalLLMProviderKey,
    modelKeys: string[],
    options: { label?: string; value?: string }[],
  ) => Promise<void>;

  updateKeyVaultConfig: <T extends GlobalLLMProviderKey>(
    provider: T,
    config: Partial<UserKeyVaults[T]>,
  ) => Promise<void>;

  useFetchProviderModelList: (
    provider: GlobalLLMProviderKey,
    enabledAutoFetch: boolean,
  ) => SWRResponse;
}

export const createModelListSlice: StateCreator<
  UserStore,
  [['zustand/devtools', never]],
  [],
  ModelListAction
> = (set, get) => ({
  clearObtainedModels: async (provider: GlobalLLMProviderKey) => {
    await get().setModelProviderConfig(provider, {
      remoteModelCards: [],
    });

    get().refreshDefaultModelProviderList();
  },
  dispatchCustomModelCards: async (provider, payload) => {
    const prevState = settingsSelectors.providerConfig(provider)(get());

    if (!prevState) return;

    const nextState = customModelCardsReducer(prevState.customModelCards, payload);

    await get().setModelProviderConfig(provider, { customModelCards: nextState });
  },
  refreshDefaultModelProviderList: (params) => {
    /**
     * Because we have several model cards sources, we need to merge the model cards
     * the priority is below:
     * 1 - server side model cards
     * 2 - remote model cards
     * 3 - default model cards
     */

    const mergeModels = (providerKey: GlobalLLMProviderKey, providerCard: ModelProviderCard) => {
      // if the chat model is config in the server side, use the server side model cards
      const serverChatModels = modelProviderSelectors.serverProviderModelCards(providerKey)(get());
      const remoteChatModels = providerCard.modelList?.showModelFetcher
        ? modelProviderSelectors.remoteProviderModelCards(providerKey)(get())
        : undefined;

      if (serverChatModels && serverChatModels.length > 0) {
        return serverChatModels;
      }
      if (remoteChatModels && remoteChatModels.length > 0) {
        return remoteChatModels;
      }

      return providerCard.chatModels;
    };

    const defaultModelProviderList = produce(DEFAULT_MODEL_PROVIDER_LIST, (draft) => {
      Object.values(ModelProvider).forEach((id) => {
        const provider = draft.find((d) => d.id === id);
        if (provider) provider.chatModels = mergeModels(id as any, provider);
      });
    });

    set({ defaultModelProviderList }, false, `refreshDefaultModelList - ${params?.trigger}`);

    get().refreshModelProviderList({ trigger: 'refreshDefaultModelList' });
  },
  refreshModelProviderList: (params) => {
    const modelProviderList = get().defaultModelProviderList.map((list) => ({
      ...list,
      chatModels: modelProviderSelectors
        .getModelCardsById(list.id)(get())
        ?.map((model) => {
          const models = modelProviderSelectors.getEnableModelsById(list.id)(get());

          if (!models) return model;

          return {
            ...model,
            enabled: models?.some((m) => m === model.id),
          };
        }),
      enabled: modelProviderSelectors.isProviderEnabled(list.id as any)(get()),
    }));

    set({ modelProviderList }, false, `refreshModelList - ${params?.trigger}`);
  },

  removeEnabledModels: async (provider, model) => {
    const config = settingsSelectors.providerConfig(provider)(get());

    await get().setModelProviderConfig(provider, {
      enabledModels: config?.enabledModels?.filter((s) => s !== model).filter(Boolean),
    });
  },

  setModelProviderConfig: async (provider, config) => {
    await get().setSettings({ languageModel: { [provider]: config } });
  },

  toggleEditingCustomModelCard: (params) => {
    set({ editingCustomCardModel: params }, false, 'toggleEditingCustomModelCard');
  },

  toggleProviderEnabled: async (provider, enabled) => {
    await get().setSettings({ languageModel: { [provider]: { enabled } } });
  },
  updateEnabledModels: async (provider, value, options) => {
    const { dispatchCustomModelCards, setModelProviderConfig } = get();
    const enabledModels = modelProviderSelectors.getEnableModelsById(provider)(get());

    // if there is a new model, add it to `customModelCards`
    const pools = options.map(async (option: { label?: string; value?: string }, index: number) => {
      // if is a known model, it should have value
      // if is an unknown model, the option will be {}
      if (option.value) return;

      const modelId = value[index];

      // if is in enabledModels, it means it's a removed model
      if (enabledModels?.some((m) => modelId === m)) return;

      await dispatchCustomModelCards(provider, {
        modelCard: { id: modelId },
        type: 'add',
      });
    });

    // TODO: 当前的这个 pool 方法并不是最好的实现，因为它会触发 setModelProviderConfig 的多次更新。
    // 理论上应该合并这些变更，然后最后只做一次触发
    // 因此后续的做法应该是将 dispatchCustomModelCards 改造为同步方法，并在最后做一次异步更新
    // 对应需要改造 'should add new custom model to customModelCards' 这一个单测
    await Promise.all(pools);

    await setModelProviderConfig(provider, { enabledModels: value.filter(Boolean) });
  },

  updateKeyVaultConfig: async (provider, config) => {
    await get().setSettings({ keyVaults: { [provider]: config } });
  },

  useFetchProviderModelList: (provider, enabledAutoFetch) =>
    useSWR<ChatModelCard[] | undefined>(
      [provider, enabledAutoFetch],
      async ([p]) => {
        const { modelsService } = await import('@/services/models');

        return modelsService.getChatModels(p);
      },
      {
        onSuccess: async (data) => {
          if (data) {
            await get().setModelProviderConfig(provider, {
              latestFetchTime: Date.now(),
              remoteModelCards: data,
            });

            get().refreshDefaultModelProviderList();
          }
        },
        revalidateOnFocus: false,
        revalidateOnMount: enabledAutoFetch,
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/modelList/selectors/modelConfig.ts
================================================================================

import { isProviderDisableBroswerRequest } from '@/config/modelProviders';
import { UserStore } from '@/store/user';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import { currentLLMSettings, getProviderConfigById } from '../../settings/selectors/settings';
import { keyVaultsConfigSelectors } from './keyVaults';

const isProviderEnabled = (provider: GlobalLLMProviderKey) => (s: UserStore) =>
  getProviderConfigById(provider)(s)?.enabled || false;

const providerWhitelist = new Set(['ollama']);
/**
 * @description The conditions to enable client fetch
 * 1. If no baseUrl and apikey input, force on Server.
 * 2. If only contains baseUrl, force on Client
 * 3. Follow the user settings.
 * 4. On Server, by default.
 */
const isProviderFetchOnClient = (provider: GlobalLLMProviderKey | string) => (s: UserStore) => {
  const config = getProviderConfigById(provider)(s);

  // If the provider already disable broswer request in model config, force on Server.
  if (isProviderDisableBroswerRequest(provider)) return false;

  // If the provider in the whitelist, follow the user settings
  if (providerWhitelist.has(provider) && typeof config?.fetchOnClient !== 'undefined')
    return config?.fetchOnClient;

  // 1. If no baseUrl and apikey input, force on Server.
  const isProviderEndpointNotEmpty =
    keyVaultsConfigSelectors.isProviderEndpointNotEmpty(provider)(s);
  const isProviderApiKeyNotEmpty = keyVaultsConfigSelectors.isProviderApiKeyNotEmpty(provider)(s);
  if (!isProviderEndpointNotEmpty && !isProviderApiKeyNotEmpty) return false;

  // 2. If only contains baseUrl, force on Client
  if (isProviderEndpointNotEmpty && !isProviderApiKeyNotEmpty) return true;

  // 3. Follow the user settings.
  if (typeof config?.fetchOnClient !== 'undefined') return config?.fetchOnClient;

  // 4. On Server, by default.
  return false;
};

const getCustomModelCard =
  ({ id, provider }: { id?: string; provider?: string }) =>
  (s: UserStore) => {
    if (!provider) return;

    const config = getProviderConfigById(provider)(s);

    return config?.customModelCards?.find((m) => m.id === id);
  };

const currentEditingCustomModelCard = (s: UserStore) => {
  if (!s.editingCustomCardModel) return;
  const { id, provider } = s.editingCustomCardModel;

  return getCustomModelCard({ id, provider })(s);
};

const isAutoFetchModelsEnabled = true;

const openAIConfig = (s: UserStore) => currentLLMSettings(s).openai;
const bedrockConfig = (s: UserStore) => currentLLMSettings(s).bedrock;
const ollamaConfig = (s: UserStore) => currentLLMSettings(s).ollama;
const azureConfig = (s: UserStore) => currentLLMSettings(s).azure;
const cloudflareConfig = (s: UserStore) => currentLLMSettings(s).cloudflare;

const isAzureEnabled = (s: UserStore) => currentLLMSettings(s).azure.enabled;

export const modelConfigSelectors = {
  azureConfig,
  bedrockConfig,
  cloudflareConfig,

  currentEditingCustomModelCard,
  getCustomModelCard,

  isAutoFetchModelsEnabled,
  isAzureEnabled,
  isProviderEnabled,
  isProviderFetchOnClient,

  ollamaConfig,
  openAIConfig,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/modelList/selectors/modelProvider.ts
================================================================================

import { uniqBy } from 'lodash-es';

import { filterEnabledModels } from '@/config/modelProviders';
import { ChatModelCard, ModelProviderCard } from '@/types/llm';
import { ServerModelProviderConfig } from '@/types/serverConfig';
import { GlobalLLMProviderKey } from '@/types/user/settings';

import { UserStore } from '../../../store';
import { currentSettings, getProviderConfigById } from '../../settings/selectors/settings';

/**
 * get the server side model cards
 */
const serverProviderModelCards =
  (provider: GlobalLLMProviderKey) =>
  (s: UserStore): ChatModelCard[] | undefined => {
    const config = s.serverLanguageModel?.[provider] as ServerModelProviderConfig | undefined;

    if (!config) return;

    return config.serverModelCards;
  };

const remoteProviderModelCards =
  (provider: GlobalLLMProviderKey) =>
  (s: UserStore): ChatModelCard[] | undefined => {
    const cards = currentSettings(s).languageModel?.[provider]?.remoteModelCards as
      | ChatModelCard[]
      | undefined;

    if (!cards) return;

    return cards;
  };

const isProviderEnabled = (provider: GlobalLLMProviderKey) => (s: UserStore) =>
  getProviderConfigById(provider)(s)?.enabled || false;

// Default Model Provider List

/**
 * define all the model list of providers
 */
const defaultModelProviderList = (s: UserStore): ModelProviderCard[] => s.defaultModelProviderList;

export const getDefaultModeProviderById = (provider: string) => (s: UserStore) =>
  defaultModelProviderList(s).find((s) => s.id === provider);

/**
 * get the default enabled models for a provider
 * it's a default enabled model list by Lobe Chat
 * e.g. openai is ['gpt-4o-mini','gpt-4o','gpt-4-turbo']
 */
const getDefaultEnabledModelsById = (provider: string) => (s: UserStore) => {
  const modelProvider = getDefaultModeProviderById(provider)(s);

  if (modelProvider) return filterEnabledModels(modelProvider);

  return undefined;
};

const getDefaultModelCardById = (id: string) => (s: UserStore) => {
  const list = defaultModelProviderList(s);

  return list.flatMap((i) => i.chatModels).find((m) => m.id === id);
};

// Model Provider List

const getModelCardsById =
  (provider: string) =>
  (s: UserStore): ChatModelCard[] => {
    const builtinCards = getDefaultModeProviderById(provider)(s)?.chatModels || [];

    const userCards = (getProviderConfigById(provider)(s)?.customModelCards || []).map((model) => ({
      ...model,
      isCustom: true,
    }));

    return uniqBy([...userCards, ...builtinCards], 'id');
  };

const getEnableModelsById = (provider: string) => (s: UserStore) => {
  if (!getProviderConfigById(provider)(s)?.enabledModels) return;

  return getProviderConfigById(provider)(s)?.enabledModels?.filter(Boolean);
};

const modelProviderList = (s: UserStore): ModelProviderCard[] => s.modelProviderList;

const modelProviderListForModelSelect = (s: UserStore): ModelProviderCard[] =>
  modelProviderList(s)
    .filter((s) => s.enabled)
    .map((provider) => ({
      ...provider,
      chatModels: provider.chatModels.filter((model) => model.enabled),
    }));

const getModelCardById = (id: string) => (s: UserStore) => {
  const list = modelProviderList(s);

  return list.flatMap((i) => i.chatModels).find((m) => m.id === id);
};

const isModelEnabledFunctionCall = (id: string) => (s: UserStore) =>
  getModelCardById(id)(s)?.functionCall || false;

// vision model white list, these models will change the content from string to array
// refs: https://github.com/lobehub/lobe-chat/issues/790
const isModelEnabledVision = (id: string) => (s: UserStore) =>
  getModelCardById(id)(s)?.vision || id.includes('vision');

const isModelEnabledFiles = (id: string) => (s: UserStore) => getModelCardById(id)(s)?.files;

const isModelEnabledUpload = (id: string) => (s: UserStore) =>
  isModelEnabledVision(id)(s) || isModelEnabledFiles(id)(s);

const isModelHasMaxToken = (id: string) => (s: UserStore) =>
  typeof getModelCardById(id)(s)?.contextWindowTokens !== 'undefined';

const modelMaxToken = (id: string) => (s: UserStore) =>
  getModelCardById(id)(s)?.contextWindowTokens || 0;

export const modelProviderSelectors = {
  defaultModelProviderList,
  getDefaultEnabledModelsById,
  getDefaultModelCardById,

  getEnableModelsById,
  getModelCardById,

  getModelCardsById,
  isModelEnabledFiles,
  isModelEnabledFunctionCall,
  isModelEnabledUpload,
  isModelEnabledVision,
  isModelHasMaxToken,

  isProviderEnabled,

  modelMaxToken,
  modelProviderList,

  modelProviderListForModelSelect,

  remoteProviderModelCards,
  serverProviderModelCards,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/user/slices/modelList/selectors/keyVaults.ts
================================================================================

import { UserStore } from '@/store/user';
import {
  AWSBedrockKeyVault,
  AzureOpenAIKeyVault,
  GlobalLLMProviderKey,
  OpenAICompatibleKeyVault,
  UserKeyVaults,
} from '@/types/user/settings';

import { currentSettings } from '../../settings/selectors/settings';

export const keyVaultsSettings = (s: UserStore): UserKeyVaults =>
  currentSettings(s).keyVaults || {};

const openAIConfig = (s: UserStore) => keyVaultsSettings(s).openai || {};
const bedrockConfig = (s: UserStore) => keyVaultsSettings(s).bedrock || {};
const wenxinConfig = (s: UserStore) => keyVaultsSettings(s).wenxin || {};
const ollamaConfig = (s: UserStore) => keyVaultsSettings(s).ollama || {};
const azureConfig = (s: UserStore) => keyVaultsSettings(s).azure || {};
const cloudflareConfig = (s: UserStore) => keyVaultsSettings(s).cloudflare || {};
const getVaultByProvider = (provider: GlobalLLMProviderKey) => (s: UserStore) =>
  (keyVaultsSettings(s)[provider] || {}) as OpenAICompatibleKeyVault &
    AzureOpenAIKeyVault &
    AWSBedrockKeyVault;

const isProviderEndpointNotEmpty = (provider: string) => (s: UserStore) => {
  const vault = getVaultByProvider(provider as GlobalLLMProviderKey)(s);
  return !!vault?.baseURL || !!vault?.endpoint;
};

const isProviderApiKeyNotEmpty = (provider: string) => (s: UserStore) => {
  const vault = getVaultByProvider(provider as GlobalLLMProviderKey)(s);
  return !!vault?.apiKey || !!vault?.accessKeyId || !!vault?.secretAccessKey;
};

const password = (s: UserStore) => keyVaultsSettings(s).password || '';

export const keyVaultsConfigSelectors = {
  azureConfig,
  bedrockConfig,
  cloudflareConfig,
  getVaultByProvider,
  isProviderApiKeyNotEmpty,
  isProviderEndpointNotEmpty,
  ollamaConfig,
  openAIConfig,
  password,
  wenxinConfig,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/agent/slices/chat/selectors.ts
================================================================================

import { VoiceList } from '@lobehub/tts';

import { INBOX_SESSION_ID } from '@/const/session';
import {
  DEFAULT_AGENT_CONFIG,
  DEFAULT_CHAT_MODEL,
  DEFAULT_CHAT_PROVIDER,
  DEFAUTT_AGENT_TTS_CONFIG,
} from '@/const/settings';
import { AgentStore } from '@/store/agent';
import { LobeAgentChatConfig, LobeAgentConfig, LobeAgentTTSConfig } from '@/types/agent';
import { KnowledgeItem, KnowledgeType } from '@/types/knowledgeBase';
import { merge } from '@/utils/merge';

const isInboxSession = (s: AgentStore) => s.activeId === INBOX_SESSION_ID;

// ==========   Config   ============== //

const inboxAgentConfig = (s: AgentStore) =>
  merge(DEFAULT_AGENT_CONFIG, s.agentMap[INBOX_SESSION_ID]);
const inboxAgentModel = (s: AgentStore) => inboxAgentConfig(s).model;

const getAgentConfigById =
  (id: string) =>
  (s: AgentStore): LobeAgentConfig =>
    merge(s.defaultAgentConfig, s.agentMap[id]);

const currentAgentConfig = (s: AgentStore): LobeAgentConfig => getAgentConfigById(s.activeId)(s);

const currentAgentChatConfig = (s: AgentStore): LobeAgentChatConfig =>
  currentAgentConfig(s).chatConfig || {};

const currentAgentSystemRole = (s: AgentStore) => {
  return currentAgentConfig(s).systemRole;
};

const currentAgentModel = (s: AgentStore): string => {
  const config = currentAgentConfig(s);

  return config?.model || DEFAULT_CHAT_MODEL;
};

const currentAgentModelProvider = (s: AgentStore) => {
  const config = currentAgentConfig(s);

  return config?.provider || DEFAULT_CHAT_PROVIDER;
};

const currentAgentPlugins = (s: AgentStore) => {
  const config = currentAgentConfig(s);

  return config?.plugins || [];
};

const currentAgentKnowledgeBases = (s: AgentStore) => {
  const config = currentAgentConfig(s);

  return config?.knowledgeBases || [];
};

const currentAgentFiles = (s: AgentStore) => {
  const config = currentAgentConfig(s);

  return config?.files || [];
};

const currentAgentTTS = (s: AgentStore): LobeAgentTTSConfig => {
  const config = currentAgentConfig(s);

  return config?.tts || DEFAUTT_AGENT_TTS_CONFIG;
};

const currentAgentTTSVoice =
  (lang: string) =>
  (s: AgentStore): string => {
    const { voice, ttsService } = currentAgentTTS(s);
    const voiceList = new VoiceList(lang);
    let currentVoice;
    switch (ttsService) {
      case 'openai': {
        currentVoice = voice.openai || (VoiceList.openaiVoiceOptions?.[0].value as string);
        break;
      }
      case 'edge': {
        currentVoice = voice.edge || (voiceList.edgeVoiceOptions?.[0].value as string);
        break;
      }
      case 'microsoft': {
        currentVoice = voice.microsoft || (voiceList.microsoftVoiceOptions?.[0].value as string);
        break;
      }
    }
    return currentVoice || 'alloy';
  };

const currentEnabledKnowledge = (s: AgentStore) => {
  const knowledgeBases = currentAgentKnowledgeBases(s);
  const files = currentAgentFiles(s);

  return [
    ...files
      .filter((f) => f.enabled)
      .map((f) => ({ fileType: f.type, id: f.id, name: f.name, type: KnowledgeType.File })),
    ...knowledgeBases
      .filter((k) => k.enabled)
      .map((k) => ({ id: k.id, name: k.name, type: KnowledgeType.KnowledgeBase })),
  ] as KnowledgeItem[];
};

const hasSystemRole = (s: AgentStore) => {
  const config = currentAgentConfig(s);

  return !!config.systemRole;
};

const hasKnowledgeBases = (s: AgentStore) => {
  const knowledgeBases = currentAgentKnowledgeBases(s);

  return knowledgeBases.length > 0;
};

const hasFiles = (s: AgentStore) => {
  const files = currentAgentFiles(s);

  return files.length > 0;
};

const hasKnowledge = (s: AgentStore) => hasKnowledgeBases(s) || hasFiles(s);
const hasEnabledKnowledge = (s: AgentStore) => currentEnabledKnowledge(s).length > 0;
const currentKnowledgeIds = (s: AgentStore) => {
  return {
    fileIds: currentAgentFiles(s)
      .filter((item) => item.enabled)
      .map((f) => f.id),
    knowledgeBaseIds: currentAgentKnowledgeBases(s)
      .filter((item) => item.enabled)
      .map((k) => k.id),
  };
};

export const agentSelectors = {
  currentAgentChatConfig,
  currentAgentConfig,
  currentAgentFiles,
  currentAgentKnowledgeBases,
  currentAgentModel,
  currentAgentModelProvider,
  currentAgentPlugins,
  currentAgentSystemRole,
  currentAgentTTS,
  currentAgentTTSVoice,
  currentEnabledKnowledge,
  currentKnowledgeIds,
  getAgentConfigById,
  hasEnabledKnowledge,
  hasKnowledge,
  hasSystemRole,
  inboxAgentConfig,
  inboxAgentModel,
  isInboxSession,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/initialState.ts
================================================================================

import { KnowledgeBaseState, initialKnowledgeBaseState } from '../knowledgeBase/slices/crud';
import { RAGEvalState, initialDatasetState } from '../knowledgeBase/slices/ragEval';

export type KnowledgeBaseStoreState = KnowledgeBaseState & RAGEvalState;

export const initialState: KnowledgeBaseStoreState = {
  ...initialKnowledgeBaseState,
  ...initialDatasetState,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/store.ts
================================================================================

import { shallow } from 'zustand/shallow';
import { createWithEqualityFn } from 'zustand/traditional';
import { StateCreator } from 'zustand/vanilla';

import { createDevtools } from '../middleware/createDevtools';
import { KnowledgeBaseStoreState, initialState } from './initialState';
import { KnowledgeBaseContentAction, createContentSlice } from './slices/content';
import { KnowledgeBaseCrudAction, createCrudSlice } from './slices/crud';
import { RAGEvalAction, createRagEvalSlice } from './slices/ragEval';

//  ===============  聚合 createStoreFn ============ //

export interface KnowledgeBaseStore
  extends KnowledgeBaseStoreState,
    KnowledgeBaseCrudAction,
    KnowledgeBaseContentAction,
    RAGEvalAction {
  // empty
}

const createStore: StateCreator<KnowledgeBaseStore, [['zustand/devtools', never]]> = (
  ...parameters
) => ({
  ...initialState,
  ...createCrudSlice(...parameters),
  ...createContentSlice(...parameters),
  ...createRagEvalSlice(...parameters),
});

//  ===============  实装 useStore ============ //
const devtools = createDevtools('knowledgeBase');

export const useKnowledgeBaseStore = createWithEqualityFn<KnowledgeBaseStore>()(
  devtools(createStore),
  shallow,
);


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/slices/ragEval/initialState.ts
================================================================================

export interface RAGEvalState {
  initDatasetList: boolean;
}

export const initialDatasetState: RAGEvalState = {
  initDatasetList: false,
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/slices/ragEval/actions/dataset.ts
================================================================================

import { SWRResponse, mutate } from 'swr';
import { StateCreator } from 'zustand/vanilla';

import { notification } from '@/components/AntdStaticMethods';
import { useClientDataSWR } from '@/libs/swr';
import { ragEvalService } from '@/services/ragEval';
import { KnowledgeBaseStore } from '@/store/knowledgeBase/store';
import {
  CreateNewEvalDatasets,
  EvalDatasetRecord,
  RAGEvalDataSetItem,
  insertEvalDatasetRecordSchema,
} from '@/types/eval';

const FETCH_DATASET_LIST_KEY = 'FETCH_DATASET_LIST';
const FETCH_DATASET_RECORD_KEY = 'FETCH_DATASET_RECORD_KEY';

export interface RAGEvalDatasetAction {
  createNewDataset: (params: CreateNewEvalDatasets) => Promise<void>;

  importDataset: (file: File, datasetId: number) => Promise<void>;
  refreshDatasetList: () => Promise<void>;
  removeDataset: (id: number) => Promise<void>;
  useFetchDatasetRecords: (datasetId: number | null) => SWRResponse<EvalDatasetRecord[]>;
  useFetchDatasets: (knowledgeBaseId: string) => SWRResponse<RAGEvalDataSetItem[]>;
}

export const createRagEvalDatasetSlice: StateCreator<
  KnowledgeBaseStore,
  [['zustand/devtools', never]],
  [],
  RAGEvalDatasetAction
> = (set, get) => ({
  createNewDataset: async (params) => {
    await ragEvalService.createDataset(params);
    await get().refreshDatasetList();
  },

  importDataset: async (file, datasetId) => {
    if (!datasetId) return;
    const fileType = file.name.split('.').pop();

    if (fileType === 'jsonl') {
      // jsonl 文件 需要拆分成单个条，然后逐一校验格式
      const jsonl = await file.text();
      const { default: JSONL } = await import('jsonl-parse-stringify');

      try {
        const items = JSONL.parse(jsonl);

        // check if the items are valid
        insertEvalDatasetRecordSchema.array().parse(items);

        // if valid, send to backend
        await ragEvalService.importDatasetRecords(datasetId, file);
      } catch (e) {
        notification.error({ description: (e as Error).message, message: '文件格式错误' });
      }
    }

    await get().refreshDatasetList();
  },
  refreshDatasetList: async () => {
    await mutate(FETCH_DATASET_LIST_KEY);
  },

  removeDataset: async (id) => {
    await ragEvalService.removeDataset(id);
    await get().refreshDatasetList();
  },
  useFetchDatasetRecords: (datasetId) =>
    useClientDataSWR<EvalDatasetRecord[]>(
      !!datasetId ? [FETCH_DATASET_RECORD_KEY, datasetId] : null,
      () => ragEvalService.getDatasetRecords(datasetId!),
    ),
  useFetchDatasets: (knowledgeBaseId) =>
    useClientDataSWR<RAGEvalDataSetItem[]>(
      [FETCH_DATASET_LIST_KEY, knowledgeBaseId],
      () => ragEvalService.getDatasets(knowledgeBaseId),
      {
        fallbackData: [],
        onSuccess: () => {
          if (!get().initDatasetList)
            set({ initDatasetList: true }, false, 'useFetchDatasets/init');
        },
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/slices/ragEval/actions/evaluation.ts
================================================================================

import { SWRResponse, mutate } from 'swr';
import { StateCreator } from 'zustand/vanilla';

import { useClientDataSWR } from '@/libs/swr';
import { ragEvalService } from '@/services/ragEval';
import { KnowledgeBaseStore } from '@/store/knowledgeBase/store';
import { CreateNewEvalEvaluation, RAGEvalDataSetItem } from '@/types/eval';

const FETCH_EVALUATION_LIST_KEY = 'FETCH_EVALUATION_LIST_KEY';

export interface RAGEvalEvaluationAction {
  checkEvaluationStatus: (id: number) => Promise<void>;

  createNewEvaluation: (params: CreateNewEvalEvaluation) => Promise<void>;
  refreshEvaluationList: () => Promise<void>;

  removeEvaluation: (id: number) => Promise<void>;
  runEvaluation: (id: number) => Promise<void>;

  useFetchEvaluationList: (knowledgeBaseId: string) => SWRResponse<RAGEvalDataSetItem[]>;
}

export const createRagEvalEvaluationSlice: StateCreator<
  KnowledgeBaseStore,
  [['zustand/devtools', never]],
  [],
  RAGEvalEvaluationAction
> = (set, get) => ({
  checkEvaluationStatus: async (id) => {
    await ragEvalService.checkEvaluationStatus(id);
  },

  createNewEvaluation: async (params) => {
    await ragEvalService.createEvaluation(params);
    await get().refreshEvaluationList();
  },
  refreshEvaluationList: async () => {
    await mutate(FETCH_EVALUATION_LIST_KEY);
  },

  removeEvaluation: async (id) => {
    await ragEvalService.removeEvaluation(id);
    // await get().refreshEvaluationList();
  },

  runEvaluation: async (id) => {
    await ragEvalService.startEvaluationTask(id);
  },

  useFetchEvaluationList: (knowledgeBaseId) =>
    useClientDataSWR<RAGEvalDataSetItem[]>(
      [FETCH_EVALUATION_LIST_KEY, knowledgeBaseId],
      () => ragEvalService.getEvaluationList(knowledgeBaseId),
      {
        fallbackData: [],
        onSuccess: () => {
          if (!get().initDatasetList)
            set({ initDatasetList: true }, false, 'useFetchDatasets/init');
        },
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/knowledgeBase/slices/ragEval/actions/index.ts
================================================================================

import { StateCreator } from 'zustand/vanilla';

import { KnowledgeBaseStore } from '@/store/knowledgeBase/store';

import { RAGEvalDatasetAction, createRagEvalDatasetSlice } from './dataset';
import { RAGEvalEvaluationAction, createRagEvalEvaluationSlice } from './evaluation';

export interface RAGEvalAction extends RAGEvalDatasetAction, RAGEvalEvaluationAction {
  // empty
}

export const createRagEvalSlice: StateCreator<
  KnowledgeBaseStore,
  [['zustand/devtools', never]],
  [],
  RAGEvalAction
> = (...params) => ({
  ...createRagEvalDatasetSlice(...params),
  ...createRagEvalEvaluationSlice(...params),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/global/initialState.ts
================================================================================

import { AppRouterInstance } from 'next/dist/shared/lib/app-router-context.shared-runtime';

import { DatabaseLoadingState } from '@/types/clientDB';
import { SessionDefaultGroup } from '@/types/session';
import { AsyncLocalStorage } from '@/utils/localStorage';

export enum SidebarTabKey {
  Chat = 'chat',
  Discover = 'discover',
  Files = 'files',
  Me = 'me',
  Setting = 'settings',
}

export enum ChatSettingsTabs {
  Chat = 'chat',
  Meta = 'meta',
  Modal = 'modal',
  Plugin = 'plugin',
  Prompt = 'prompt',
  TTS = 'tts',
}

export enum SettingsTabs {
  About = 'about',
  Agent = 'agent',
  Common = 'common',
  LLM = 'llm',
  Sync = 'sync',
  SystemAgent = 'system-agent',
  TTS = 'tts',
}

export interface SystemStatus {
  // which sessionGroup should expand
  expandSessionGroupKeys: string[];
  filePanelWidth: number;
  hidePWAInstaller?: boolean;
  hideThreadLimitAlert?: boolean;
  inputHeight: number;
  /**
   * 应用初始化时不启用 PGLite，只有当用户手动开启时才启用
   */
  isEnablePglite?: boolean;
  latestChangelogId?: string;
  mobileShowPortal?: boolean;
  mobileShowTopic?: boolean;
  sessionsWidth: number;
  showChatSideBar?: boolean;
  showFilePanel?: boolean;
  showSessionPanel?: boolean;
  showSystemRole?: boolean;
  threadInputHeight: number;
  zenMode?: boolean;
}

export interface GlobalState {
  hasNewVersion?: boolean;
  initClientDBError?: Error;
  initClientDBProcess?: { costTime?: number; phase: 'wasm' | 'dependencies'; progress: number };
  /**
   * 客户端数据库初始化状态
   * 启动时为 Idle，完成为 Ready，报错为 Error
   */
  initClientDBStage: DatabaseLoadingState;
  isMobile?: boolean;
  isStatusInit?: boolean;
  latestVersion?: string;
  router?: AppRouterInstance;
  sidebarKey: SidebarTabKey;
  status: SystemStatus;
  statusStorage: AsyncLocalStorage<SystemStatus>;
}

export const INITIAL_STATUS = {
  expandSessionGroupKeys: [SessionDefaultGroup.Pinned, SessionDefaultGroup.Default],
  filePanelWidth: 320,
  hidePWAInstaller: false,
  hideThreadLimitAlert: false,
  inputHeight: 200,
  mobileShowTopic: false,
  sessionsWidth: 320,
  showChatSideBar: true,
  showFilePanel: true,
  showSessionPanel: true,
  showSystemRole: false,
  threadInputHeight: 200,
  zenMode: false,
} satisfies SystemStatus;

export const initialState: GlobalState = {
  initClientDBStage: DatabaseLoadingState.Idle,
  isMobile: false,
  isStatusInit: false,
  sidebarKey: SidebarTabKey.Chat,
  status: INITIAL_STATUS,
  statusStorage: new AsyncLocalStorage('LOBE_SYSTEM_STATUS'),
};


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/global/action.ts
================================================================================

import isEqual from 'fast-deep-equal';
import { produce } from 'immer';
import { gt, parse, valid } from 'semver';
import { SWRResponse } from 'swr';
import type { StateCreator } from 'zustand/vanilla';

import { INBOX_SESSION_ID } from '@/const/session';
import { SESSION_CHAT_URL } from '@/const/url';
import { CURRENT_VERSION } from '@/const/version';
import { useOnlyFetchOnceSWR } from '@/libs/swr';
import { globalService } from '@/services/global';
import type { GlobalStore } from '@/store/global/index';
import { merge } from '@/utils/merge';
import { setNamespace } from '@/utils/storeDebug';

import type { SystemStatus } from './initialState';

const n = setNamespace('g');

/**
 * 设置操作
 */
export interface GlobalStoreAction {
  switchBackToChat: (sessionId?: string) => void;
  toggleChatSideBar: (visible?: boolean) => void;
  toggleExpandSessionGroup: (id: string, expand: boolean) => void;
  toggleMobilePortal: (visible?: boolean) => void;
  toggleMobileTopic: (visible?: boolean) => void;
  toggleSystemRole: (visible?: boolean) => void;
  toggleZenMode: () => void;
  updateSystemStatus: (status: Partial<SystemStatus>, action?: any) => void;
  useCheckLatestVersion: (enabledCheck?: boolean) => SWRResponse<string>;
  useInitSystemStatus: () => SWRResponse;
}

export const globalActionSlice: StateCreator<
  GlobalStore,
  [['zustand/devtools', never]],
  [],
  GlobalStoreAction
> = (set, get) => ({
  switchBackToChat: (sessionId) => {
    get().router?.push(SESSION_CHAT_URL(sessionId || INBOX_SESSION_ID, get().isMobile));
  },

  toggleChatSideBar: (newValue) => {
    const showChatSideBar =
      typeof newValue === 'boolean' ? newValue : !get().status.showChatSideBar;

    get().updateSystemStatus({ showChatSideBar }, n('toggleAgentPanel', newValue));
  },
  toggleExpandSessionGroup: (id, expand) => {
    const { status } = get();
    const nextExpandSessionGroup = produce(status.expandSessionGroupKeys, (draft: string[]) => {
      if (expand) {
        if (draft.includes(id)) return;
        draft.push(id);
      } else {
        const index = draft.indexOf(id);
        if (index !== -1) draft.splice(index, 1);
      }
    });
    get().updateSystemStatus({ expandSessionGroupKeys: nextExpandSessionGroup });
  },
  toggleMobilePortal: (newValue) => {
    const mobileShowPortal =
      typeof newValue === 'boolean' ? newValue : !get().status.mobileShowPortal;

    get().updateSystemStatus({ mobileShowPortal }, n('toggleMobilePortal', newValue));
  },
  toggleMobileTopic: (newValue) => {
    const mobileShowTopic =
      typeof newValue === 'boolean' ? newValue : !get().status.mobileShowTopic;

    get().updateSystemStatus({ mobileShowTopic }, n('toggleMobileTopic', newValue));
  },
  toggleSystemRole: (newValue) => {
    const showSystemRole = typeof newValue === 'boolean' ? newValue : !get().status.mobileShowTopic;

    get().updateSystemStatus({ showSystemRole }, n('toggleMobileTopic', newValue));
  },
  toggleZenMode: () => {
    const { status } = get();
    const nextZenMode = !status.zenMode;

    get().updateSystemStatus({ zenMode: nextZenMode }, n('toggleZenMode'));
  },
  updateSystemStatus: (status, action) => {
    // Status cannot be modified when it is not initialized
    if (!get().isStatusInit) return;

    const nextStatus = merge(get().status, status);
    if (isEqual(get().status, nextStatus)) return;

    set({ status: nextStatus }, false, action || n('updateSystemStatus'));

    get().statusStorage.saveToLocalStorage(nextStatus);
  },

  useCheckLatestVersion: (enabledCheck = true) =>
    useOnlyFetchOnceSWR(
      enabledCheck ? 'checkLatestVersion' : null,
      async () => globalService.getLatestVersion(),
      {
        // check latest version every 30 minutes
        focusThrottleInterval: 1000 * 60 * 30,
        onSuccess: (data: string) => {
          if (!valid(CURRENT_VERSION) || !valid(data)) return;

          // Parse versions to ensure we're working with valid SemVer objects
          const currentVersion = parse(CURRENT_VERSION);
          const latestVersion = parse(data);

          if (!currentVersion || !latestVersion) return;

          // only compare major and minor versions
          // solve the problem of frequent patch updates
          const currentMajorMinor = `${currentVersion.major}.${currentVersion.minor}.0`;
          const latestMajorMinor = `${latestVersion.major}.${latestVersion.minor}.0`;

          if (gt(latestMajorMinor, currentMajorMinor)) {
            set({ hasNewVersion: true, latestVersion: data }, false, n('checkLatestVersion'));
          }
        },
      },
    ),

  useInitSystemStatus: () =>
    useOnlyFetchOnceSWR<SystemStatus>(
      'initSystemStatus',
      () => get().statusStorage.getFromLocalStorage(),
      {
        onSuccess: (status) => {
          set({ isStatusInit: true }, false, 'setStatusInit');

          get().updateSystemStatus(status, 'initSystemStatus');
        },
      },
    ),
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/tool/slices/builtin/action.ts
================================================================================

import { StateCreator } from 'zustand/vanilla';

import { OpenAIImagePayload } from '@/types/openai/image';
import { DallEImageItem } from '@/types/tool/dalle';
import { setNamespace } from '@/utils/storeDebug';

import { ToolStore } from '../../store';

const n = setNamespace('builtinTool');

interface Text2ImageParams extends Pick<OpenAIImagePayload, 'quality' | 'style' | 'size'> {
  prompts: string[];
}

/**
 * 代理行为接口
 */
export interface BuiltinToolAction {
  text2image: (params: Text2ImageParams) => DallEImageItem[];
  toggleBuiltinToolLoading: (key: string, value: boolean) => void;
  transformApiArgumentsToAiState: (key: string, params: any) => Promise<string | undefined>;
}

export const createBuiltinToolSlice: StateCreator<
  ToolStore,
  [['zustand/devtools', never]],
  [],
  BuiltinToolAction
> = (set, get) => ({
  text2image: ({ prompts, size = '1024x1024' as const, quality = 'standard', style = 'vivid' }) =>
    prompts.map((p) => ({ prompt: p, quality, size, style })),
  toggleBuiltinToolLoading: (key, value) => {
    set({ builtinToolLoading: { [key]: value } }, false, n('toggleBuiltinToolLoading'));
  },

  transformApiArgumentsToAiState: async (key, params) => {
    const { builtinToolLoading, toggleBuiltinToolLoading } = get();
    if (builtinToolLoading[key]) return;

    const { [key as keyof BuiltinToolAction]: action } = get();

    if (!action) return JSON.stringify(params);

    toggleBuiltinToolLoading(key, true);

    try {
      // @ts-ignore
      const result = await action(params);

      toggleBuiltinToolLoading(key, false);

      return JSON.stringify(result);
    } catch (e) {
      toggleBuiltinToolLoading(key, false);
      throw e;
    }
  },
});


================================================================================
File: /Users/geoffpike/Code/lobe-chat/src/store/tool/selectors/tool.ts
================================================================================

import { LobeChatPluginManifest } from '@lobehub/chat-plugin-sdk';
import { uniqBy } from 'lodash-es';

import { pluginPrompts } from '@/prompts/plugin';
import { MetaData } from '@/types/meta';
import { ChatCompletionTool } from '@/types/openai/chat';
import { LobeToolMeta } from '@/types/tool/tool';
import { genToolCallingName } from '@/utils/toolCall';

import { pluginHelpers } from '../helpers';
import { ToolStoreState } from '../initialState';
import { builtinToolSelectors } from '../slices/builtin/selectors';
import { pluginSelectors } from '../slices/plugin/selectors';

const enabledSchema =
  (tools: string[] = []) =>
  (s: ToolStoreState): ChatCompletionTool[] => {
    const list = pluginSelectors
      .installedPluginManifestList(s)
      .concat(s.builtinTools.map((b) => b.manifest as LobeChatPluginManifest))
      // 如果存在 enabledPlugins，那么只启用 enabledPlugins 中的插件
      .filter((m) => tools.includes(m?.identifier))
      .flatMap((manifest) =>
        manifest.api.map((m) => ({
          description: m.description,
          name: genToolCallingName(manifest.identifier, m.name, manifest.type),
          parameters: m.parameters,
        })),
      );

    return uniqBy(list, 'name').map((i) => ({ function: i, type: 'function' }));
  };

const enabledSystemRoles =
  (tools: string[] = []) =>
  (s: ToolStoreState) => {
    const toolsSystemRole = pluginSelectors
      .installedPluginManifestList(s)
      .concat(s.builtinTools.map((b) => b.manifest as LobeChatPluginManifest))
      // 如果存在 enabledPlugins，那么只启用 enabledPlugins 中的插件
      .filter((m) => m && tools.includes(m.identifier))
      .map((manifest) => {
        const meta = manifest.meta || {};

        const title = pluginHelpers.getPluginTitle(meta) || manifest.identifier;
        const systemRole = manifest.systemRole || pluginHelpers.getPluginDesc(meta);

        return {
          apis: manifest.api.map((m) => ({
            desc: m.description,
            name: genToolCallingName(manifest.identifier, m.name, manifest.type),
          })),
          identifier: manifest.identifier,
          name: title,
          systemRole,
        };
      });

    if (toolsSystemRole.length > 0) {
      return pluginPrompts({ tools: toolsSystemRole });
    }

    return '';
  };

const metaList =
  (showDalle?: boolean) =>
  (s: ToolStoreState): LobeToolMeta[] => {
    const pluginList = pluginSelectors.installedPluginMetaList(s) as LobeToolMeta[];

    return builtinToolSelectors.metaList(showDalle)(s).concat(pluginList);
  };

const getMetaById =
  (id: string, showDalle: boolean = true) =>
  (s: ToolStoreState): MetaData | undefined =>
    metaList(showDalle)(s).find((m) => m.identifier === id)?.meta;

const getManifestById =
  (id: string) =>
  (s: ToolStoreState): LobeChatPluginManifest | undefined =>
    pluginSelectors
      .installedPluginManifestList(s)
      .concat(s.builtinTools.map((b) => b.manifest as LobeChatPluginManifest))
      .find((i) => i.identifier === id);

// 获取插件 manifest 加载状态
const getManifestLoadingStatus = (id: string) => (s: ToolStoreState) => {
  const manifest = getManifestById(id)(s);

  if (s.pluginInstallLoading[id]) return 'loading';

  if (!manifest) return 'error';

  if (!!manifest) return 'success';
};

const isToolHasUI = (id: string) => (s: ToolStoreState) => {
  const manifest = getManifestById(id)(s);
  if (!manifest) return false;
  const builtinTool = s.builtinTools.find((tool) => tool.identifier === id);

  if (builtinTool && builtinTool.type === 'builtin') {
    return true;
  }

  return !!manifest.ui;
};

export const toolSelectors = {
  enabledSchema,
  enabledSystemRoles,
  getManifestById,
  getManifestLoadingStatus,
  getMetaById,
  isToolHasUI,
  metaList,
};

